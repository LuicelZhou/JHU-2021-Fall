{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr/>\n",
    "\n",
    "# Introduction to Data Science - Fall 2021\n",
    "**Jinchao Feng and Christian Kuemmerle** - introdsfall2021@jh.edu <br/>\n",
    "\n",
    "\n",
    "- Principal Component Analysis: Motivation & Formulation\n",
    "\n",
    "- Linear Algebra Recap\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><font color=\"darkblue\">Principal Component Analysis</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: Statistical Learning\n",
    "\n",
    ">|                | Supervised     |         Unsupervised     |\n",
    " |:---------------|:--------------:|:------------------------:|\n",
    " | **Discrete**   | Classification | Clustering               |   \n",
    " | **Continuous** | Regression     | Dimensionality Reduction |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspects of Principal Component Analysis (PCA)\n",
    "\n",
    "<!--<img src=\"files/800px-GaussianScatterPCA.png\" width=300 align=left>-->\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/GaussianScatterPCA.svg/440px-GaussianScatterPCA.svg.png\" width=300 align=topleft>\n",
    "\n",
    "- Find low-dimensional representation of high-dimensional data.\n",
    "   - Interpretability\n",
    "   - Compression\n",
    "- Linear data transformation\n",
    "- Directions of maximal variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example from Genetics\n",
    "\n",
    "Identify relationship between geography and genetics from single-nucleotide polymorphism (SNP) data. <br>\n",
    "See\n",
    "  - [Paschou, Peristera, et al. \"PCA-correlated SNPs for structure identification in worldwide human populations.\" PLoS genetics 3.9 (2007): e160.](https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.0030160)\n",
    "  - [Slides](https://livejohnshopkins-my.sharepoint.com/:b:/g/personal/ckuemme1_jh_edu/EdhsuknNn5pAqnEMTQipojIBh12Xk3Ot9CRwGthikI3ToQ?e=gN5u8o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of Linear Algebra\n",
    "\n",
    "Assume $n$ data points $\\{x_1,\\ldots,x_n\\} \\subset \\mathbb{R}^D$ are given.\n",
    "\n",
    "  - Define **data matrix** $X$:\n",
    "  > $X = \\begin{bmatrix} x_1 ,\\dots, x_n \\end{bmatrix} \\in \\mathbb{R}^{D \\times n}$.\n",
    "  \n",
    "  - Let $\\overline{X}:= \\frac{1}{n}X \\mathbf{1}\\mathbf{1}^T = (\\frac{1}{n} \\sum_{i=1}^n (x_i)_j)_{j,\\ell}$ be the matrix with row averages of $X$. <br>\n",
    "   **Centered data matrix** $\\widetilde{X}$:\n",
    "  > $\\widetilde{X} = X -\\overline{X} \\in \\mathbb{R}^{D \\times n}$.\n",
    "  - **Sample covariance matrix** $C$:\n",
    "  >$\\displaystyle C =  \\frac{1}{n\\!-\\!1}\\widetilde{X}\\widetilde{X}^T =  \\frac{1}{n\\!-\\!1}\\ (X- \\overline{X})(X- \\overline{X})^T = \\frac{1}{n\\!-\\!1}\\  \\sum_{i=1}^n (x_i- \\overline{x}) (x_i- \\overline{x})^T \\in \\mathbb{R}^{D \\times D}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues & Eigenvectors\n",
    "\n",
    "- $\\lambda \\in \\mathbb{R}$ is an **eigenvalue** of $C \\in \\mathbb{R}^{D \\times D}$ and $e \\in \\mathbb{R}^{D} \\setminus \\{0\\}$ is **eigenvector** of $C$ if \n",
    "> $$ C e = \\lambda e. $$\n",
    "\n",
    "- If $\\{e_1,e_2,\\ldots,e_{D}\\}$ are a basis of $\\mathbb{R}^D$ (in particular, if they are linear independent), then there exists an **eigendecomposition**. <br>\n",
    "In this case, if $\\lambda_1\\geq\\lambda_2\\geq\\dots\\geq\\lambda_D\\geq{}0$ are the eigenvalues of $C$ and ${e}_1,\\dots,{e}_D$ the corresponding eigenvectors, the eigendecomposition can be written as\n",
    "> $\\displaystyle  C  = \\sum_{i=1}^D\\ \\lambda_i\\left({e}_i\\,{e}_i^T\\right) = \\begin{bmatrix} | & & | \\\\e_1,& \\dots,& e_D \\\\ |& & | \\end{bmatrix} \\begin{bmatrix} \\lambda_1 & 0 & 0 \\\\ \n",
    "0& \\ddots &0 \\\\ 0 & 0& \\lambda_{D} \\end{bmatrix} \\begin{bmatrix} - & e_1^T& -  \\\\ & \\dots & \\\\ - & e_D^T  & - \\end{bmatrix} = E\\ \\Lambda\\ E^T.$  <br>\n",
    "> $\\Lambda$ is diagonal.\n",
    ">\n",
    "\n",
    "- The matrix $E \\in \\mathbb{R}^{D \\times D}$ with eigenvectors in its columns is **orthogonal**, i.e.,\n",
    "> $$E^T E = E E^T =  I$$ (or $E^{-1} = E^T$).\n",
    "\n",
    "> With diagonal $\\Lambda$ matrix of the eigenvalues and an $E$ matrix of $[{e}_1, \\dots, {e}_N]$\n",
    "><br/><br/>\n",
    "> $\\displaystyle  C = E\\ \\Lambda\\ E^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition (SVD)\n",
    "\n",
    "One advantage: Is useable for non-square matrices such as the data matrices $X$ or $\\widetilde{X}$.\n",
    "\n",
    "- For each matrix $X \\in \\mathbb{R}^{D \\times n}$, there is a **singular value decomposition** such that (without loss of generality, $D < n$)\n",
    "> $\\displaystyle  X  = \\sum_{i=1}^{D} \\ \\sigma_i\\left({u}_i\\,{v}_i^T\\right) = \\begin{bmatrix} | & & | \\\\u_1,& \\dots,& u_D \\\\ |& & | \\end{bmatrix} \\begin{bmatrix} \\sigma_1 & 0 & 0 &0  \\\\ \n",
    "0& \\ddots &0 &0 \\\\ 0 & 0& \\sigma_{D} & 0 \\end{bmatrix} \\begin{bmatrix} - & v_1^T& -  \\\\ & \\dots & \\\\ - & v_n^T  & - \\end{bmatrix} = U \\Sigma V^T$ <br><br>\n",
    "> where $\\Sigma$ is diagonal, $V^T V=I$ and $U^T U = I$, and $\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_D \\geq 0$.\n",
    "  - Columns $u_i \\in \\mathbb{R}^{D}$ of $U$ are called **left singular vectors** of $X$, <br>\n",
    "  - Columns $v_i \\in \\mathbb{R}^{n}$ of $V$ are called **right singular vectors** of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear algebra in action\n",
    "\n",
    "- We generate $n$ points in $D=2$ sampled as i.i.d. standard Gaussians. Their $x$-coordinates are streched by a factor $4$, and their location is subsequently rotated by $45Â°$, after which a shift by $(1,3)$ is applied. This gives rise to the data matrix $X \\in \\mathbb{R}^{2 \\times n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "n = 50\n",
    "D = 2\n",
    "# generate multiple 2-D (column) vectors\n",
    "N = norm.rvs(0,1,(D,n))\n",
    "\n",
    "figure(figsize=(4,4)); xlim(-10,10); ylim(-10,10); title('Gaussian data points')\n",
    "scatter(N[0,:],N[1,:], marker='o',color='b', s=50, alpha=0.3, edgecolor='none')\n",
    "plt = gca()\n",
    "plt.set_aspect('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy and scale the data set\n",
    "S = np.copy(N)\n",
    "S[0,:] *= 4  # scale axis 0 (\"x-axis\")\n",
    "figure(figsize=(4,4)); xlim(-10,10); ylim(-10,10);title('scaled')\n",
    "scatter(S[0,:],S[1,:], marker='o',color='b', s=50, alpha=0.3, edgecolor='none');\n",
    "plt = gca()\n",
    "plt.set_aspect('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotate the data set by 45 degrees\n",
    "f = +pi/4    \n",
    "R = array([[cos(f), -sin(f)],\n",
    "           [sin(f),  cos(f)]]) # rotation matrix\n",
    "X = R@S  # matrix multiplication with rotation matrix\n",
    "X += np.array([[1],[3]]) # shift\n",
    "figure(figsize=(4,4)); xlim(-15,15); ylim(-15,15);\n",
    "scatter(X[0,:],X[1,:],marker='o',color='b',alpha=0.3, edgecolor='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract sample mean\n",
    "avg = mean(X, axis=1).reshape(X[:,1].size,1)\n",
    "Xtilde = X - avg\n",
    "# sample covariance matrix\n",
    "C = Xtilde.dot(Xtilde.T) / (n-1) \n",
    "print (\"Sample averages\\n\", avg)\n",
    "print (\"Sample covariance matrix\\n\",C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # compare with numpy function\n",
    "(np.cov(X),C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigen decomposition of sample covariance matrix\n",
    "L, E = np.linalg.eig(C)\n",
    "print(\"Eigenvalues:\\n\",L)\n",
    "print(\"Eigenvector matrix:\\n\",E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute singular value decomposition\n",
    "U, W, V = np.linalg.svd(C)\n",
    "print(\"Singular values:\\n\",W)\n",
    "print(\"Left singular vector matrix:\\n\",U)\n",
    "print(\"Right singular vector matrix:\\n\",V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to potential reordering of columns, SVD and eigendecomposition of $C$ the same (since $C$ is symmetric and positive definite)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check orthogonality properties\n",
    "print(E.dot(E.T))\n",
    "print(U.dot(U.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose( E.T, np.linalg.inv(E) ) # alternative way to do this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $X = U \\Sigma V^T$ is a singular value decomposition of a _centered_ data matrix $X$ and \n",
    " $C = \\frac{1}{n\\!-\\!1}X X^T$ its sample covariance matrix, we can very closely relate the singular value decomposition of $X$ with the eigendecomposition of $C$:\n",
    "\n",
    ">$\\displaystyle E\\Lambda E^T = C = \\frac{1}{n\\!-\\!1} X X^T =  \\frac{1}{n\\!-\\!1} U \\Sigma V^T V \\Sigma^T U  = \\frac{1}{n\\!-\\!1} U \\Sigma \\Sigma^T U  = \\frac{1}{n\\!-\\!1} U \\Sigma^2 U.$ <br><br>\n",
    "> Due to uniqueness of the SVD and orthogonality of $U$ and $E$, we have \n",
    ">$$\\displaystyle \\displaystyle \\Lambda = \\frac{1}{n\\!-\\!1}\\  \\Sigma^2 $$\n",
    "and \n",
    ">$$ U = E $$\n",
    ">(up to reordering of columns / eigenvalues, so that eigenvalues of $C$ in $\\Lambda$ are in non-increasing order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UX, W, VX = np.linalg.svd(Xtilde) # take here Xtilde as X, since Xtilde was centered\n",
    "UX, W**2 / (X[0,:].size-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(U,UX) # the two ways of how to compute U indeed coincide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "<img src=files/PCA_2D_illustration.png align=left width=800>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA as Directions of Maximal Variance\n",
    "\n",
    "Assume now we are given a data matrix $X = \\begin{bmatrix} x_1 ,\\dots, x_n \\end{bmatrix} \\in \\mathbb{R}^{D \\times n}$ as above, which is centered, i.e., $\\overline{X} = 0$. If $X'$ is the empirical distribution of the data matrix, the **first principal direction** $u_1 \\in \\mathbb{R}^D$ can be computed as the **direction of maximal variance** of $X'$.\n",
    "- Since $X$ is centered, $\\mathbb{E}\\left[X'\\right]=0$, and furthermore for any $u \\in \\mathbb{R}^D$\n",
    "\n",
    "> $\\displaystyle \\mathbb{Var}[u^T X'] = \\mathbb{E}\\left[(u^T X')(X'^T u)\\right] = u^T\\mathbb{E}\\left[X'X'^T\\right]\\,u = \\frac{1}{n} u^T \\sum_{i=1}^n x_i x_i^T u = \\frac{1}{n} u^T X X^T u  = \\frac{n-1}{n}u^T C u $\n",
    "><br/><br/>\n",
    "\n",
    "- Therefore, in order to maximize $\\mathbb{Var}[u^T X']$, we need to maximize $\\frac{n-1}{n}u^T C u $ (or equivalently, $u^T C u $) over $u$.\n",
    "\n",
    "- We add the constraint $u^2\\!=\\!1$ for normalization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constrained Optimization\n",
    "\n",
    "- **Lagrange multiplier**: extra term with new parameter $\\lambda$\n",
    "\n",
    "> $\\displaystyle  \\hat{u} = \\arg\\max_{u\\in{}\\mathbb{R}^N} \\left[u^T C\\,u - \\lambda\\,(u^2\\!-\\!1)\\right]$\n",
    "\n",
    "- Partial derivatives vanish at optimum\n",
    "\n",
    "> $\\displaystyle \\frac{\\partial}{\\partial\\lambda} \\rightarrow\\ \\  \\hat{u}^2\\!-\\!1 = 0\\ \\ $  (duh!)\n",
    "><br/><br/>\n",
    "> $\\displaystyle \\frac{\\partial}{\\partial u_k} \\rightarrow\\ \\  $?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With indices\n",
    "\n",
    "\n",
    "> $\\displaystyle \\max_{u\\in{}\\mathbb{R}^N}  \\left[ \\sum_{i,j} u_i C_{ij} u_j - \\lambda\\,\\left(\\sum_i u_i^2 - 1\\right) \\right]$\n",
    "\n",
    "- Partial derivatives $\\partial \\big/ \\partial u_k$ vanish at optimum\n",
    "\n",
    "> $\\displaystyle \\sum_{i,j} \\frac{\\partial u_i}{\\partial u_k} C_{ij} u_j + \\sum_{i,j} u_i C_{ij} \\frac{\\partial u_j}{\\partial u_k} - 2\\lambda\\,\\left(\\sum_i u_i \\frac{\\partial u_i}{\\partial u_k}\\right)  $ \n",
    "> $=\\displaystyle \\sum_{i,j} \\delta_{ik} C_{ij} u_j + \\sum_{i,j} u_i C_{ij} \\delta_{jk} - 2\\lambda\\,\\left(\\sum_i u_i \\delta_{ik}\\right)  $ \n",
    "> $=\\displaystyle \\sum_{j} C_{kj} u_j + \\sum_{i} u_i C_{ik}  - 2\\lambda\\,u_k $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And back again...\n",
    "\n",
    "- With vectors and matrices\n",
    "\n",
    "> $\\displaystyle  C \\hat{u} + C^T\\hat{u} - 2\\lambda \\hat{u} = 0$\n",
    "><br/><br/>\n",
    "> but $C$ is symmetric \n",
    "><br/><br/>\n",
    "> $\\displaystyle  C\\,\\hat{u} = \\lambda\\,\\hat{u} $\n",
    "\n",
    "- We have now recovered an eigenvalue problem! See eigendecomposition above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "- The value of maximum variance is\n",
    "\n",
    "> $\\displaystyle  \\hat{u}^TC\\,\\hat{u} = \\hat{u}^T \\lambda\\,\\hat{u} = \\lambda\\, \\hat{u}^T\\hat{u} = \\lambda$\n",
    "><br/><br/>\n",
    "> the largest eigenvalue $\\lambda_1$\n",
    "\n",
    "- The direction of maximum variance is the corresponding eigenvector $u_1$\n",
    "\n",
    "> $\\displaystyle  Cu_1 = \\lambda_1 u_1 $\n",
    "\n",
    "- The vector $u_1 \\in \\mathbb{R}^D$  is the **1st principal component (or 1st principal direction)**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis: The Method\n",
    "\n",
    "To perform PCA, we fix an integer $k \\leq D$, the number of principal directions we are looking for.\n",
    "\n",
    "The principal directions $u_1,u_2,\\ldots,u_k \\in \\mathbb{R}^{D}$ are designed to maximize the variance of the data set, but also to be orthogonal to each other, i.e. \n",
    "<br><br>\n",
    "$$u_i^T u_j = 1 \\text{ if and only if }i = j, \\text{ and else }=0.$$ <br>.\n",
    "Thus, we maximize \n",
    "> $\\displaystyle \\mathbb{Var}[U_k^T X']$\n",
    "with $U_k = \\begin{bmatrix}u_1, & \\ldots,& u_k \\end{bmatrix}$ under the constraint that <br><br>\n",
    "$$ U_k^T U_k = I.$$\n",
    "\n",
    "This correspond to just finding the largest $k$ eigenvectors of the covariance matrix $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA \n",
    "\n",
    "\n",
    "\n",
    "- The eigenvectors of largest eigenvalues capture the most variance\n",
    "\n",
    "> If keeping only $k<D$ eigenvectors, the best approximation is taking the first $k$ PCs\n",
    "><br/><br/>\n",
    "> $\\displaystyle  C \\approx \\sum_{i=1}^k\\ \\lambda_i\\left({e}_i\\,{e}_i^T\\right) =  E_k\\Lambda_k E_k^T$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Coordiante System\n",
    "\n",
    "- The $E$ matrix of eigenvectors is a rotation, $E\\,E^T = I$\n",
    "\n",
    "> $\\displaystyle  Z = E^T\\, X $\n",
    "\n",
    "\n",
    "- A truncated set of eigenvectors $E_K$ defines a projection\n",
    "\n",
    "> $\\displaystyle  Z_k = E_k^T\\, X $\n",
    "><br/><br/>\n",
    "> and\n",
    "><br/><br/>\n",
    "> $\\displaystyle  X_k = E_k Z_k = E_k E_k^T\\, X = P_k\\,X $\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detour: Projections\n",
    "\n",
    "- If the square of a matrix is equal to itself\n",
    "\n",
    "> $\\displaystyle  P^2 = P $\n",
    "\n",
    "- For example, projecting on the ${e}$ unit vector\n",
    "\n",
    "<img src=files/Y7Gx8.png align=right width=250>\n",
    "\n",
    "> Scalar times vector\n",
    "><br/><br/>\n",
    "> $\\displaystyle  r' = {e}\\left({e}^T r\\right) = {e}\\,\\beta_r$\n",
    "><br/><br/>\n",
    "> Or  projection of vector $r$\n",
    "><br/><br/>\n",
    "> $\\displaystyle  r' = \\left({e}\\,{e}^T\\right)r = P\\,r$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# principle components from sklearn\n",
    "from sklearn import decomposition\n",
    "# object-oriented interface\n",
    "pca = decomposition.PCA(n_components=X[:,0].size)\n",
    "# sklearn uses a different convention\n",
    "pca.fit(X.T) # note the transpose\n",
    "# pca.transform(X.T)\n",
    "print (pca.components_.T, pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# singular value decomposition of data matrix yields also the same\n",
    "U, W, V = np.linalg.svd(X)\n",
    "U, W**2 / (X[0,:].size-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use to yield results without bessel correction\n",
    "print (U, W**2 / X[0,:].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotation\n",
    "C = X.dot(X.T) / (X[0,:].size-1) \n",
    "E, L, E_same = np.linalg.svd(C)\n",
    "A = E.T.dot(X);\n",
    "figure(figsize=(5,5)); xlim(-10,10); ylim(-10,10);\n",
    "scatter(A[0,:],A[1,:],marker='o',s=50,alpha=0.3,edgecolor='none');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# projected points\n",
    "P = E[:,:1].dot(E[:,:1].T).dot(X)\n",
    "\n",
    "\n",
    "figure(figsize=(5,5)); xlim(-10,10); ylim(-10,10);\n",
    "scatter(X[0,:],X[1,:],marker='o',color='b', s=50, alpha=0.3, edgecolor='none');\n",
    "scatter(P[0,:],P[1,:],marker='o',color='r', s=50, alpha=0.3, edgecolor='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whitening\n",
    "- Frequently used to preprocess data, e.g., in signal processing\n",
    "\n",
    ">$ \\displaystyle Z = \\Lambda^{-1/2}\\ E^T\\ X$\n",
    "\n",
    "- **Unhomework:** Verify that its covariance matrix is identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc and plot whitened coordinates\n",
    "B = np.diag(1/np.sqrt(L)).dot(A)\n",
    "figure(figsize=(5,5)); xlim(-5,5); ylim(-5,5);\n",
    "scatter(B[0,:],B[1,:], marker='o',color='b', s=50, alpha=0.3, edgecolor='none');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cf. original normal randoms in cell [2]\n",
    "figure(figsize=(11,5)); \n",
    "subplot(1,2,1).set_aspect('equal'); xlim(-5,5); ylim(-5,5); title('whitened')\n",
    "scatter(B[0,:],B[1,:], marker='o',color='b', s=50, alpha=0.3, edgecolor='none');\n",
    "subplot(1,2,2).set_aspect('equal'); xlim(-5,5); ylim(-5,5); title('original')\n",
    "scatter(N[0,:],N[1,:], marker='o',color='b', s=50, alpha=0.3, edgecolor='none');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '3',\n",
       " '4',\n",
       " '1',\n",
       " '3',\n",
       " '4',\n",
       " '1',\n",
       " '3',\n",
       " '4',\n",
       " '1',\n",
       " '3',\n",
       " '4',\n",
       " '1',\n",
       " '3',\n",
       " '4',\n",
       " '1',\n",
       " '3',\n",
       " '4',\n",
       " '1',\n",
       " '3',\n",
       " '4',\n",
       " '1',\n",
       " '3',\n",
       " '4',\n",
       " '1',\n",
       " '3',\n",
       " '4',\n",
       " '1',\n",
       " '3',\n",
       " '4']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =['1','3','4']\n",
    "a * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
