{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr/>\n",
    "\n",
    "# Introduction to Data Science - Fall 2021\n",
    "**Jinchao Feng and Christian Kuemmerle** - introdsfall2021@jh.edu <br/>\n",
    "\n",
    "\n",
    "- Ridge Regression & Sparse Regression\n",
    "\n",
    "- Linear Algebra Recap\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: Linear Regression/Ordinary Least Squares\n",
    "\n",
    "(see written notes)\n",
    "\n",
    "# Regularization: Making Linear Regression more flexible\n",
    "\n",
    "Penalize large coefficients in $\\beta$\n",
    "\n",
    "- **Ridge regression** uses $L_2$-regularization, where $\\|\\beta\\|^2_2 = \\sum_{i=1}^N \\beta_i^2$\n",
    "\n",
    "> $\\displaystyle \\hat{\\beta} = \\arg\\!\\min_{\\beta} \\, \\| y- X\\beta \\,\\|^2_2\\ + \\alpha\\,\\|\\beta\\|^2_2$  \n",
    "><br/>\n",
    "> or even with a constant matrix $\\Gamma$\n",
    "><br/><br/>\n",
    "> $\\displaystyle \\hat{\\beta} = \\arg\\!\\min_{\\beta} \\, \\| y- X\\beta \\,\\|^2_2 + \\alpha\\,\\|\\Gamma\\beta\\|^2_2$  \n",
    "\n",
    "- **Lasso regression** uses $L_1$-regularization, where $\\|\\beta\\|_1 = \\sum_{i=1}^N |\\beta_i|$.\n",
    "\n",
    "> $\\displaystyle \\hat{\\beta} = \\arg\\!\\min_{\\beta}  \\, \\| y- X\\beta \\,\\|^2_2 + \\alpha\\,\\|\\beta\\|_1$ \n",
    "><br/><br/>\n",
    "> $L_1$ yields sparse results\n",
    "\n",
    "Different geometric meanings! See written notes for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to sample $n=\\text{'n_sample'}$ points $x_1,\\ldots,x_n$ uniformly distributed at random and define \n",
    "corresponding samples $y_i=f(x_i) + \\epsilon_i$ where \n",
    "$f(x)= \\sin(4 x) + x$ and $\\epsilon_i$ are i.i.d. standard random variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to create random samples from a wave-like signal\n",
    "def make_wave(n_samples=100):\n",
    "    rnd = np.random.RandomState(42)\n",
    "    x = rnd.uniform(-3, 3, size=n_samples)\n",
    "    x = np.sort(x)\n",
    "    y_no_noise = (np.sin(4 * x) + x)\n",
    "    y = y_no_noise + rnd.normal(size=len(x))/2\n",
    "    return x.reshape(-1, 1), y\n",
    "\n",
    "# Visualize the function f:\n",
    "fn = lambda x: np.sin(4 * x) + x\n",
    "line_xvalues = np.linspace(-3, 3, 1000).reshape(-1, 1)\n",
    "plt.figure(figsize=(14,8))\n",
    "plt.plot(line_xvalues,fn(line_xvalues))\n",
    "ax = plt.gca()\n",
    "ax.set_ylim(-6, 6)\n",
    "ax.set_xlim(-4, 4)\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a data set (consisting of $x$-coordinates in '$X$' and $y$-coordinates in '$y$') based on the above model, using a 'nr_samples' samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Create data, create training and test data split\n",
    "nr_samples = 30\n",
    "X, y = make_wave(n_samples=nr_samples)\n",
    "y = y.reshape((len(y), 1))\n",
    "dfX = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the method [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?highlight=train_test#sklearn.model_selection.train_test_split) of scikit-learn to split the available samples into to groups, a training dataset and a test dataset. The share of samples in the training data is provided by the choice of 'train_share', i.e., if $\\text{train_share} = 0.6$, then 60% of the samples are used in the training dataset.\n",
    "\n",
    "From the samples variable $X$, we create a [pandas](https://pandas.pydata.org/docs/) DataFrame as its methods are sometimes more convenient to use - for example, we can easily obtain a vector of indices corresponding to the training and test set, respectively. We define the corresponding vectors 'id_train' and 'id_test'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_share = 0.6\n",
    "#dfX = pd.DataFrame(X)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(dfX, y, train_size=train_share,random_state=25) # split data into  a share of 'train_share' in training data set and  a share of 1-'train_share' in test data set\n",
    "#id_train = X_train.index # obtain index of training set\n",
    "#X_train = X_train.to_numpy()\n",
    "#id_test = X_test.index # obtain index of test set\n",
    "#X_test= X_test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The samples can be visualized as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Plot training data\n",
    "plt.figure(figsize=(14,8))\n",
    "plt.plot(X,y,'o',c='blue')\n",
    "ax = plt.gca()\n",
    "ax.set_ylim(-6, 6)\n",
    "ax.set_xlim(-4, 4)\n",
    "ax.spines['left'].set_position('center')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['bottom'].set_position('center')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.legend([\"data points\"],loc=0,fontsize=8)\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train a linear regression model (with $\\alpha=\\text{alphaval}$) on the samples of the training set, and evaluate the mean squared errors as well as the resulting $R^2$ coefficient. We also train a linear and a ridge regression model on **polynomial features** of degree='degree_poly'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit first the linear regression model to the training data with unmodified features. This corresponds to a truly linear model in the standard coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Fit models to training data\n",
    "lr = LinearRegression().fit(X, y) # linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Run linear regression with polynomial features of degree 'degree_poly':\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "degree_poly = 24;\n",
    "poly = PolynomialFeatures(degree_poly) # define polynomial data model\n",
    "X_poly = poly.fit_transform(X) # obtain coordinates of polynomial features of training set\n",
    "polylr = LinearRegression().fit(X_poly,y) # perform linear regression with polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use ridge regression on the polynomial features for a fixed regularization parameter $\\alpha=\\text{alphaval}$. Please note the options of [linear_models](https://scikit-learn.org/stable/modules/linear_model.html#linear-model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from scipy.linalg import LinAlgWarning\n",
    "warnings.filterwarnings(\"ignore\", category=LinAlgWarning)\n",
    "# %% Run ridge regression with polynomial features\n",
    "alphaval = 1 # ridge regression parameter alpha\n",
    "polyridge = Ridge(alpha=alphaval).fit(X_poly,y) # perform ridge regression with polynomial features\n",
    "alphaval2 = 10**3 # ridge regression parameter alpha\n",
    "polyridge2 = Ridge(alpha=alphaval2).fit(X_poly,y) # perform ridge regression with polynomial features\n",
    "line = np.linspace(-8, 8, 1000).reshape(-1, 1)\n",
    "line_poly = poly.transform(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We output the $R^2$ for the three considered models, for both training and test set, as well as the _mean squared errors_ for these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Plot summary\n",
    "plt.figure(figsize=(14,8))\n",
    "plt.plot(X,y,'o',c='blue')\n",
    "#plt.plot(line, lr.predict(line))\n",
    "plt.plot(line, polylr.predict(line_poly))\n",
    "plt.plot(line, polyridge.predict(line_poly))\n",
    "plt.plot(line, polyridge2.predict(line_poly))\n",
    "ax = plt.gca()\n",
    "ax.spines['left'].set_position('center')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['bottom'].set_position('center')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.set_ylim(-6, 6)\n",
    "ax.set_xlim(-4, 4)\n",
    "ax.legend([\"data points\",\"OLS (Ridge with $alpha = 0$)\",\"Ridge with alpha=\"+str(alphaval),\"Ridge with alpha=\"+str(alphaval2)], loc=0,fontsize=10)\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that for larger regularization parameters $\\alpha$, the regression function is \"smoother\". From the perspective of the regression coefficients (cooridnates of the vector $\\widehat{\\beta}$), we see that for large $\\alpha$, the coefficients are \"shrinked\" by Ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(polylr.coef_.T, '^', label=\"OLS (Ridge with $alpha = 0$)\")\n",
    "plt.plot(polyridge.coef_.T, 'v', label=\"Ridge with alpha=\"+str(alphaval))\n",
    "plt.plot(polyridge2.coef_.T, 'x', label=\"Ridge with alpha=\"+str(alphaval2))\n",
    "plt.legend(ncol=2, loc=(0, 1.05))\n",
    "#plt.ylim(-25, 25)\n",
    "plt.xlabel(\"Coefficient index\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Bias-Variance Tradeoff\n",
    "\n",
    "- Bias: the difference between the average prediction of our model and the correct value which we are trying to predict. \n",
    "> Model with high bias pays very little attention to the training data and oversimplifies the model. It always leads to high error on training and test data (underfitting).\n",
    "\n",
    "- Variance: the variability of model prediction for a given data point or a value which tells us the spread of our data.\n",
    "> Model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before. As a result, such models perform very well on training data but has high error rates on test data (overfitting).\n",
    "\n",
    "<img src='files/BiasVarianceTradeoff.png' width=400 align=left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "\n",
    "We recall the [diabetes](https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html) dataset from last week's problem set. We use `StandardScaler` from scikit-learn to standardize the scale of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Download the diabetes file, and save it in the working directory\n",
    "# Read the text file into a pandas dataframe\n",
    "diabetes = pd.read_csv(\"https://www4.stat.ncsu.edu/~boos/var.select/diabetes.tab.txt\", sep='\\t')\n",
    "#diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "diabetes_df = diabetes.drop(columns = ['SEX'])\n",
    "diabetes_df\n",
    "scaler = StandardScaler()\n",
    "saved_cols = diabetes_df.columns\n",
    "diabetes_scaled = scaler.fit_transform(diabetes_df)\n",
    "diabetes_df_scaled = pd.DataFrame(diabetes_scaled, columns = saved_cols)\n",
    "#diabetes_df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a seperate the dataset at random into a training set that contains 80% of the samples and a test set that contains 20% of the samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(diabetes_df_scaled, test_size=0.2)\n",
    "train_X = train.drop(columns = [\"Y\"])\n",
    "train_y = train[\"Y\"]\n",
    "test_X = test.drop(columns = [\"Y\"])\n",
    "test_y = test[\"Y\"]\n",
    "#test_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html?highlight=ridge#sklearn.linear_model.Ridge) regression models for different regularization parameters $\\alpha$.\n",
    "For this, we first define a function `computeScoresMSE` that computes R2 scores and MSE values of an estimator for a range of regularization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeScoresMSE(model,alphas,train_X,train_y,test_X,test_y):\n",
    "    train_scores = np.empty_like(alphas)\n",
    "    test_scores = np.empty_like(alphas)\n",
    "    test_MSE    = np.empty_like(alphas)\n",
    "    train_MSE   = np.empty_like(alphas)\n",
    "    models = []\n",
    "    for i in range(alphas.size):\n",
    "        models.append(model(alpha=alphas[i]))\n",
    "        models[i].fit(train_X,train_y)\n",
    "        train_scores[i] = models[i].score(train_X, train_y)\n",
    "        test_scores[i]  = models[i].score(test_X, test_y)\n",
    "        test_MSE[i]     = np.mean((test_y-models[i].predict(test_X))**2)\n",
    "        train_MSE[i]    = np.mean((train_y-models[i].predict(train_X))**2)\n",
    "    index = np.argmin(test_MSE)\n",
    "    best_model = models[index]\n",
    "    return train_MSE, test_MSE, train_scores, test_scores, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "import matplotlib.pyplot as plt\n",
    "alphas = np.logspace(-2,6,num=60) # create numpy array of logarithmically interpolated values between 10^(-5) and 10^(9)\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(1,interaction_only=False)\n",
    "poly.fit(train_X)\n",
    "features_names = poly.get_feature_names(train_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_train_X = poly.transform(train_X)\n",
    "p_test_X = poly.transform(test_X)\n",
    "\n",
    "train_MSE, test_MSE, train_scores, test_scores, Ridge_opt = \\\n",
    "    computeScoresMSE(Ridge,alphas,p_train_X,train_y,p_test_X,test_y)\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.semilogx(alphas,test_MSE,'x',label=\"Test MSE\")\n",
    "plt.semilogx(alphas,train_MSE,'x',label=\"Train MSE\")\n",
    "plt.xlabel(r\"Regularization parameter $\\alpha$\")\n",
    "plt.ylabel(r\"Mean Squared Error (MSE)\")\n",
    "plt.title(\"Bias-Variance Tradeoff for Ridge Regression\")\n",
    "ax = plt.gca()\n",
    "ax.invert_xaxis()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Model with optimal regularization parameter alpha: \",Ridge_opt)\n",
    "print(\"Best test MSE: \",min(test_MSE)) # compare with np.mean((test_y-Ridge_opt.predict(test_X))**2)\n",
    "(min(test_MSE) , np.mean((test_y-Ridge_opt.predict(p_test_X))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(test_scores)\n",
    "y_test_avg = np.mean(test_y)\n",
    "Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsquared = 1 - np.linalg.norm(test_y-Ridge_opt.predict(p_test_X))**2 / np.linalg.norm(test_y-y_test_avg)**2\n",
    "(max(test_scores),rsquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "We repeat this experiment for the Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning) # this is to suppress warnings related to the optimization in the training of Lasso\n",
    "\n",
    "train_MSE, test_MSE, train_scores, test_scores, Lasso_opt = \\\n",
    "    computeScoresMSE(Lasso,alphas,p_train_X,train_y,p_test_X,test_y)\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.semilogx(alphas,test_MSE,'x',label=\"Test MSE\")\n",
    "plt.semilogx(alphas,train_MSE,'x',label=\"Train MSE\")\n",
    "plt.xlabel(r\"Regularization parameter $\\alpha$\")\n",
    "plt.ylabel(r\"Mean Squared Error (MSE)\")\n",
    "plt.title(\"Bias-Variance Tradeoff for Lasso Regression\")\n",
    "ax = plt.gca()\n",
    "ax.invert_xaxis()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Model with optimal regularization parameter alpha: \",Lasso_opt)\n",
    "print(\"Best test MSE: \",np.mean((test_y-Lasso_opt.predict(p_test_X))**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the regularization parameter $\\alpha$ resulting in the smallest test MSE is different for Ridge and Lasso regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create a plot that visualizes the differences between the coefficients returned by the two different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso_opt.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge_opt.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(Lasso_opt.coef_, '^', label=str(Lasso_opt))\n",
    "plt.plot(Ridge_opt.coef_, 'v', label=str(Ridge_opt))\n",
    "plt.plot(np.arange(np.size(Lasso_opt.coef_)),np.zeros(np.size(Lasso_opt.coef_)))\n",
    "plt.legend(ncol=2, loc=(0, 1.05))\n",
    "#plt.ylim(-25, 25)\n",
    "plt.xlabel(\"Coefficient index\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "plt.xticks(ticks = np.arange(Lasso_opt.coef_.size),labels=features_names)\n",
    "plt.show()\n",
    "#ax = plt.gca()\n",
    "#ax.set_xlabels(features_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient vector $\\widehat{\\beta}$ returned by Lasso is _sparser_ then the one returned by Ridge regression. This leads to a better interpretability of the results Lasso, as it performs _variable selection_: Ideally, we can infer from the non-zero coeffcients of the coefficient vector $\\widehat{\\beta}$ _which predictor variables_ are most relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularized Regression & Bias-Variance Tradeoff\n",
    "For both Ridge regression and Lasso regression it is true that:\n",
    "  - $\\alpha \\approx 0$ or small: Larger variance, smaller bias\n",
    "  - $\\alpha$ large: Small variance, large bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra for Data Science\n",
    "\n",
    "Matrix decompositions and linear algebra are at the core of many techniques in data science.\n",
    "Please review Chapter 4 (and, if necessary, Chapter 2) of the [book by Deisenroth, Faisal and Ong](https://mml-book.github.io/book/mml-book.pdf).\n",
    "### Data sample matrix and sample covariance matrix\n",
    "\n",
    "- We generate $n = 20$ points in 2d sampled as i.i.d. standard Gaussians. Their $x$-coordinates are streched by a factor $4$, and their location is subsequently rotated by $45°$, after which a shift by $(1,3)$ is applied. This gives rise to the data matrix $X \\in \\mathbb{R}^{2 \\times 20}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "n = 20\n",
    "# generate multiple 2-D (column) vectors\n",
    "S = norm.rvs(0,1,(2,n))\n",
    "S[0,:] *= 4  # scale axis 0\n",
    "f = +pi/4    # rotate by 45 degrees\n",
    "R = array([[cos(f), -sin(f)],\n",
    "           [sin(f),  cos(f)]]) \n",
    "X = R.dot(S)\n",
    "X += np.array([[1],[3]]) # shift\n",
    "\n",
    "figure(figsize=(5,5)); xlim(-15,15); ylim(-15,15);\n",
    "plot(X[0,:],X[1,:],'o',alpha=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\overline{X}$ be the matrix with row averages of $X$, and $\\widetilde{X} = X -\\overline{X}$.\n",
    "> Sample covariance matrix corresponds to\n",
    "><br/><br/>\n",
    ">$\\displaystyle C =  \\frac{1}{n\\!-\\!1}\\widetilde{X}\\widetilde{X}^T =  \\frac{1}{n\\!-\\!1}\\ (X- \\overline{X})(X- \\overline{X})^T = \\frac{1}{n\\!-\\!1}\\  \\sum_{i=1}^n (x_i- \\overline{x}) (x_i- \\overline{x})^T$\n",
    "\n",
    "We observe that there is a close relationship between the\n",
    "\n",
    "- Singular Value Decomposition (SVD)\n",
    "\n",
    ">$\\displaystyle \\widetilde{X} = U W V^T$\n",
    "><br/><br/>\n",
    "> where $U^TU=I$, $W$ is diagonal, and $V^TV=I$,\n",
    ">> Columns of $U$ are **left singular vectors** of $\\widetilde{X}$, <br>\n",
    ">> Columns of $V$ are **right singular vectors** of $\\widetilde{X}$.\n",
    "\n",
    "and the \n",
    "- Eigendecomposition\n",
    "\n",
    ">$\\displaystyle C=E\\Lambda E^T$ <br><br>\n",
    "> of the sample covariance matrix $C$, since\n",
    "\n",
    ">$\\displaystyle C = \\frac{1}{n\\!-\\!1}\\  UWV^T\\ VWU^T = \\frac{1}{n\\!-\\!1}\\ U W^2 U^T$.\n",
    "><br/><br/>\n",
    "> So, if $C=E\\Lambda E^T$ then $E = U$ and $\\displaystyle \\Lambda = \\frac{1}{n\\!-\\!1}\\  W^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract sample mean\n",
    "avg = mean(X, axis=1).reshape(X[:,1].size,1)\n",
    "Xtilde = X- avg\n",
    "Xtilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample covariance matrix\n",
    "C = Xtilde.dot(Xtilde.T) / (X[0,:].size-1) \n",
    "print (\"Average\\n\", avg)\n",
    "print (\"Covariance\\n\", C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L, E = np.linalg.eig(C)\n",
    "E, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E, L, E_same = np.linalg.svd(C)\n",
    "E, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E.dot(E.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose( E.T, np.linalg.inv(E) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, W, V = np.linalg.svd(X)\n",
    "U, W**2 / (X[0,:].size-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively\n",
    "U, W**2 / (X.shape[1]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ np.allclose( U.dot(U.T), np.eye(U.shape[0]) ), \n",
    "  np.allclose( V.dot(V.T), np.eye(V.shape[0]) )  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
