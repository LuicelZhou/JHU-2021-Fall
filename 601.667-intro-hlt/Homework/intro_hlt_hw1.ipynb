{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "intro-hlt-hw1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLm_JxCxrkIE"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "In the first assignment, you will implement some of the algorithms you have learnt in the first two weeks of lectures: n-gram language models, and syntactic parsing using the CYK algorithm. \n",
        "\n",
        "# Setup\n",
        "\n",
        "For this and other assignments, we will be using Google Colab, for both code as well as descriptive questions. Your task is to finish all the questions in the Colab notebook and then upload a PDF version of the notebook, and a viewable link on Gradescope.\n",
        "\n",
        "### Google colaboratory\n",
        "\n",
        "Before getting started, get familiar with google colaboratory:\n",
        "https://colab.research.google.com/notebooks/welcome.ipynb\n",
        "\n",
        "This is a neat python environment that works in the cloud and does not require you to\n",
        "set up anything on your personal machine\n",
        "(it also has some built-in IDE features that make writing code easier).\n",
        "Moreover, it allows you to copy any existing collaboratory file, alter it and share\n",
        "with other people.\n",
        "\n",
        "### Submission\n",
        "\n",
        "Before you start working on this homework do the following steps:\n",
        "\n",
        "1. Press __File > Save a copy in Drive...__ tab. This will allow you to have your own copy and change it.\n",
        "2. Follow all the steps in this collaboratory file and write / change / uncomment code as necessary.\n",
        "3. Do not forget to occasionally press __File > Save__ tab to save your progress.\n",
        "4. After all the changes are done and progress is saved press __Share__ button (top right corner of the page), press __get shareable link__ and make sure you have the option __Anyone with the link can view__ selected. Copy the link and paste it in the box below.\n",
        "5. After completing the notebook, press __File > Download .ipynb__ to download a local copy on your computer, and then use `jupyter nbconvert --to pdf intro-hlt-hw1.ipynb` to convert the notebook into PDF format for uploading on Gradescope.\n",
        "\n",
        "__Paste your notebook link in the box below.__ _(0 points)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A7c68XN-XDn"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Paste your Colab notebook link here\n",
        "https://colab.research.google.com/drive/1YyiOyKJUvGhGL7HvSS4OfpqTagJeFdt8?usp=sharing\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWXKLhH1N0IU"
      },
      "source": [
        "## Part 1: N-gram Language Models\n",
        "\n",
        "For the first part of this assignment, you will implement a trigram language model and train it on a small corpus. You will then implement a scoring function to compute the perplexity of the model on a held-out test set. Finally, you will implement some methods to deal with sparsity (zero count) issues in your model.\n",
        "\n",
        "To ease you into the implementation, we will provide some boilerplate code that you would need to fill in depending upon the functionalities the code is supposed to perform. For the first few sections below, we will use the complete text from Leo Tolstoy's \"War and Peace,\" which is freely available from [Project Gutenberg](https://www.gutenberg.org/). Run the following code block to download the text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeaIH26iQiRK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "480fa058-fd82-4aeb-de5c-36ecf7e97eea"
      },
      "source": [
        "# Download the War and Peace text.\n",
        "! wget https://cs.stanford.edu/people/karpathy/char-rnn/warpeace_input.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-22 21:29:04--  https://cs.stanford.edu/people/karpathy/char-rnn/warpeace_input.txt\n",
            "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3258246 (3.1M) [text/plain]\n",
            "Saving to: ‘warpeace_input.txt.1’\n",
            "\n",
            "warpeace_input.txt. 100%[===================>]   3.11M  2.85MB/s    in 1.1s    \n",
            "\n",
            "2021-09-22 21:29:06 (2.85 MB/s) - ‘warpeace_input.txt.1’ saved [3258246/3258246]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obJJmlMlSs2d"
      },
      "source": [
        "The complete text downloaded above contains punctuations which are not important for our purposes. So we will perform a basic text preprocessing using the [NLTK toolkit](https://www.nltk.org/). We will store the processed text into a list of strings, where each string will contain words without any punctuations. We will also convert all words to lower case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoKZKaRGVYa1"
      },
      "source": [
        "# Loading the text\n",
        "with open('warpeace_input.txt', 'r') as file:\n",
        "    corpus_raw = file.read().replace('\\n', ' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r_Q1XbTWCst",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc417588-bded-4ba1-f0b3-ed662938eb39"
      },
      "source": [
        "# Text pre-processing to remove punctuation marks\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
        "\n",
        "sentences = sent_tokenize(corpus_raw)\n",
        "\n",
        "corpus = []\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "for sentence in sentences:\n",
        "  tokens = tokenizer.tokenize(sentence)\n",
        "  corpus.append(\" \".join([token.lower() for token in tokens]))\n",
        "\n",
        "print (\"Corpus has {} sentences\".format(len(corpus)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Corpus has 32040 sentences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykO86GMkPQ2E"
      },
      "source": [
        "### Warm-up: n-gram counts from a corpus\n",
        "\n",
        "Let's start with implementing a simple function for obtaining n-grams and their counts from a string. Complete the following function. _(5 points)_\n",
        "\n",
        "__Note 1:__ Use the special token `~` for both beginning of sentence (BOS) and end of sentence (EOS) tokens. For example, the sentence \"Mary has a little lamb\" has the bigrams \"_~ Mary_\", \"_Mary has_\", \"_has a_\", \"_a little_\", \"_little lamb_\", and \"_lamb ~_\".\n",
        "\n",
        "__Note 2:__ You don't need to do any further text processing beyond what has already been done before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn7lexIbPEiY"
      },
      "source": [
        "from collections import Counter\n",
        "def generate_ngrams(text, n):\n",
        "  \"\"\"Generate all n-grams (i.e. n-1 context words) for the given text.\n",
        "\n",
        "    Parameters:\n",
        "      text (list(str)): input text (list of strings)\n",
        "      n (int): n-gram parameter (must be greater than or equal to 2)\n",
        "\n",
        "    Returns:\n",
        "      ngrams (dict(ngram: count)): output n-grams dictionary, indexed by \n",
        "        n-gram tuple, e.g. ('Mary','has') and value as count of the n-gram\n",
        "        in the text. \n",
        "  \"\"\"\n",
        "  assert (isinstance(n, int) and n > 0)\n",
        "\n",
        "  ngrams = {}\n",
        "\n",
        "  # TODO: Your code here.\n",
        "  ngrams_list = []  # n gram (numerator)\n",
        "  prefix_list = []  # n-1 gram（denominator)\n",
        "  for se in text:\n",
        "            sentence = ['~'] + se.split() + ['~']  # list, such as: ['~', 'Mary', 'has', 'a', '~']\n",
        "            ngrams = list(zip(*[sentence[i:] for i in range(n)]))   # ngram list in a sentence\n",
        "            prefix = list(zip(*[sentence[i:] for i in range(n-1)])) # list of history prefix tuple\n",
        "            ngrams_list += ngrams\n",
        "            prefix_list += prefix\n",
        "  #print(prefix_list)\n",
        "  #print(ngrams_list)\n",
        "  ngrams_counter = Counter(ngrams_list) #count the n grams frequency\n",
        "  prefix_counter = Counter(prefix_list) #count the n-1 grams frequency\n",
        "  #print(ngrams_counter)\n",
        "  ngrams = dict(ngrams_counter) # make type Counter to dict\n",
        "\n",
        "  return ngrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5XK61TRfuyD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ff96e0d-1fe9-480b-f71b-e0213657ed06"
      },
      "source": [
        "# Test your implementation on some sentences from the corpus we downloaded above\n",
        "# and verify if it works correctly.\n",
        "text = corpus[:100]\n",
        "print (text)\n",
        "\n",
        "ngrams = generate_ngrams(text,3)\n",
        "print (ngrams)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['well prince so genoa and lucca are now just family estates of the buonapartes', 'but i warn you if you don t tell me that this means war if you still try to defend the infamies and horrors perpetrated by that antichrist i really believe he is antichrist i will have nothing more to do with you and you are no longer my friend no longer my faithful slave as you call yourself', 'but how do you do', 'i see i have frightened you sit down and tell me all the news', 'it was in july 1805 and the speaker was the well known anna pavlovna scherer maid of honor and favorite of the empress marya fedorovna', 'with these words she greeted prince vasili kuragin a man of high rank and importance who was the first to arrive at her reception', 'anna pavlovna had had a cough for some days', 'she was as she said suffering from la grippe grippe being then a new word in st petersburg used only by the elite', 'all her invitations without exception written in french and delivered by a scarlet liveried footman that morning ran as follows if you have nothing better to do count or prince and if the prospect of spending an evening with a poor invalid is not too terrible i shall be very charmed to see you tonight between 7 and 10 annette scherer', 'heavens', 'what a virulent attack', 'replied the prince not in the least disconcerted by this reception', 'he had just entered wearing an embroidered court uniform knee breeches and shoes and had stars on his breast and a serene expression on his flat face', 'he spoke in that refined french in which our grandfathers not only spoke but thought and with the gentle patronizing intonation natural to a man of importance who had grown old in society and at court', 'he went up to anna pavlovna kissed her hand presenting to her his bald scented and shining head and complacently seated himself on the sofa', 'first of all dear friend tell me how you are', 'set your friend s mind at rest said he without altering his tone beneath the politeness and affected sympathy of which indifference and even irony could be discerned', 'can one be well while suffering morally', 'can one be calm in times like these if one has any feeling', 'said anna pavlovna', 'you are staying the whole evening i hope', 'and the fete at the english ambassador s', 'today is wednesday', 'i must put in an appearance there said the prince', 'my daughter is coming for me to take me there', 'i thought today s fete had been canceled', 'i confess all these festivities and fireworks are becoming wearisome', 'if they had known that you wished it the entertainment would have been put off said the prince who like a wound up clock by force of habit said things he did not even wish to be believed', 'don t tease', 'well and what has been decided about novosiltsev s dispatch', 'you know everything', 'what can one say about it', 'replied the prince in a cold listless tone', 'what has been decided', 'they have decided that buonaparte has burnt his boats and i believe that we are ready to burn ours', 'prince vasili always spoke languidly like an actor repeating a stale part', 'anna pavlovna scherer on the contrary despite her forty years overflowed with animation and impulsiveness', 'to be an enthusiast had become her social vocation and sometimes even when she did not feel like it she became enthusiastic in order not to disappoint the expectations of those who knew her', 'the subdued smile which though it did not suit her faded features always played round her lips expressed as in a spoiled child a continual consciousness of her charming defect which she neither wished nor could nor considered it necessary to correct', 'in the midst of a conversation on political matters anna pavlovna burst out oh don t speak to me of austria', 'perhaps i don t understand things but austria never has wished and does not wish for war', 'she is betraying us', 'russia alone must save europe', 'our gracious sovereign recognizes his high vocation and will be true to it', 'that is the one thing i have faith in', 'our good and wonderful sovereign has to perform the noblest role on earth and he is so virtuous and noble that god will not forsake him', 'he will fulfill his vocation and crush the hydra of revolution which has become more terrible than ever in the person of this murderer and villain', 'we alone must avenge the blood of the just one', 'whom i ask you can we rely on', 'england with her commercial spirit will not and cannot understand the emperor alexander s loftiness of soul', 'she has refused to evacuate malta', 'she wanted to find and still seeks some secret motive in our actions', 'what answer did novosiltsev get', 'none', 'the english have not understood and cannot understand the self abnegation of our emperor who wants nothing for himself but only desires the good of mankind', 'and what have they promised', 'nothing', 'and what little they have promised they will not perform', 'prussia has always declared that buonaparte is invincible and that all europe is powerless before him and i don t believe a word that hardenburg says or haugwitz either', 'this famous prussian neutrality is just a trap', 'i have faith only in god and the lofty destiny of our adored monarch', 'he will save europe', 'she suddenly paused smiling at her own impetuosity', 'i think said the prince with a smile that if you had been sent instead of our dear wintzingerode you would have captured the king of prussia s consent by assault', 'you are so eloquent', 'will you give me a cup of tea', 'in a moment', 'a propos she added becoming calm again i am expecting two very interesting men tonight le vicomte de mortemart who is connected with the montmorencys through the rohans one of the best french families', 'he is one of the genuine emigres the good ones', 'and also the abbe morio', 'do you know that profound thinker', 'he has been received by the emperor', 'had you heard', 'i shall be delighted to meet them said the prince', 'but tell me he added with studied carelessness as if it had only just occurred to him though the question he was about to ask was the chief motive of his visit is it true that the dowager empress wants baron funke to be appointed first secretary at vienna', 'the baron by all accounts is a poor creature', 'prince vasili wished to obtain this post for his son but others were trying through the dowager empress marya fedorovna to secure it for the baron', 'anna pavlovna almost closed her eyes to indicate that neither she nor anyone else had a right to criticize what the empress desired or was pleased with', 'baron funke has been recommended to the dowager empress by her sister was all she said in a dry and mournful tone', 'as she named the empress anna pavlovna s face suddenly assumed an expression of profound and sincere devotion and respect mingled with sadness and this occurred every time she mentioned her illustrious patroness', 'she added that her majesty had deigned to show baron funke beaucoup d estime and again her face clouded over with sadness', 'the prince was silent and looked indifferent', 'but with the womanly and courtierlike quickness and tact habitual to her anna pavlovna wished both to rebuke him for daring to speak as he had done of a man recommended to the empress and at the same time to console him so she said now about your family', 'do you know that since your daughter came out everyone has been enraptured by her', 'they say she is amazingly beautiful', 'the prince bowed to signify his respect and gratitude', 'i often think she continued after a short pause drawing nearer to the prince and smiling amiably at him as if to show that political and social topics were ended and the time had come for intimate conversation i often think how unfairly sometimes the joys of life are distributed', 'why has fate given you two such splendid children', 'i don t speak of anatole your youngest', 'i don t like him she added in a tone admitting of no rejoinder and raising her eyebrows', 'two such charming children', 'and really you appreciate them less than anyone and so you don t deserve to have them', 'and she smiled her ecstatic smile', 'i can t help it said the prince', 'lavater would have said i lack the bump of paternity', 'don t joke i mean to have a serious talk with you', 'do you know i am dissatisfied with your younger son', 'between ourselves and her face assumed its melancholy expression he was mentioned at her majesty s and you were pitied the prince answered nothing but she looked at him significantly awaiting a reply', 'he frowned', 'what would you have me do']\n",
            "{('~', 'well', 'prince'): 1, ('well', 'prince', 'so'): 1, ('prince', 'so', 'genoa'): 1, ('so', 'genoa', 'and'): 1, ('genoa', 'and', 'lucca'): 1, ('and', 'lucca', 'are'): 1, ('lucca', 'are', 'now'): 1, ('are', 'now', 'just'): 1, ('now', 'just', 'family'): 1, ('just', 'family', 'estates'): 1, ('family', 'estates', 'of'): 1, ('estates', 'of', 'the'): 1, ('of', 'the', 'buonapartes'): 1, ('the', 'buonapartes', '~'): 1, ('~', 'but', 'i'): 1, ('but', 'i', 'warn'): 1, ('i', 'warn', 'you'): 1, ('warn', 'you', 'if'): 1, ('you', 'if', 'you'): 1, ('if', 'you', 'don'): 1, ('you', 'don', 't'): 2, ('don', 't', 'tell'): 1, ('t', 'tell', 'me'): 1, ('tell', 'me', 'that'): 1, ('me', 'that', 'this'): 1, ('that', 'this', 'means'): 1, ('this', 'means', 'war'): 1, ('means', 'war', 'if'): 1, ('war', 'if', 'you'): 1, ('if', 'you', 'still'): 1, ('you', 'still', 'try'): 1, ('still', 'try', 'to'): 1, ('try', 'to', 'defend'): 1, ('to', 'defend', 'the'): 1, ('defend', 'the', 'infamies'): 1, ('the', 'infamies', 'and'): 1, ('infamies', 'and', 'horrors'): 1, ('and', 'horrors', 'perpetrated'): 1, ('horrors', 'perpetrated', 'by'): 1, ('perpetrated', 'by', 'that'): 1, ('by', 'that', 'antichrist'): 1, ('that', 'antichrist', 'i'): 1, ('antichrist', 'i', 'really'): 1, ('i', 'really', 'believe'): 1, ('really', 'believe', 'he'): 1, ('believe', 'he', 'is'): 1, ('he', 'is', 'antichrist'): 1, ('is', 'antichrist', 'i'): 1, ('antichrist', 'i', 'will'): 1, ('i', 'will', 'have'): 1, ('will', 'have', 'nothing'): 1, ('have', 'nothing', 'more'): 1, ('nothing', 'more', 'to'): 1, ('more', 'to', 'do'): 1, ('to', 'do', 'with'): 1, ('do', 'with', 'you'): 1, ('with', 'you', 'and'): 1, ('you', 'and', 'you'): 1, ('and', 'you', 'are'): 1, ('you', 'are', 'no'): 1, ('are', 'no', 'longer'): 1, ('no', 'longer', 'my'): 2, ('longer', 'my', 'friend'): 1, ('my', 'friend', 'no'): 1, ('friend', 'no', 'longer'): 1, ('longer', 'my', 'faithful'): 1, ('my', 'faithful', 'slave'): 1, ('faithful', 'slave', 'as'): 1, ('slave', 'as', 'you'): 1, ('as', 'you', 'call'): 1, ('you', 'call', 'yourself'): 1, ('call', 'yourself', '~'): 1, ('~', 'but', 'how'): 1, ('but', 'how', 'do'): 1, ('how', 'do', 'you'): 1, ('do', 'you', 'do'): 1, ('you', 'do', '~'): 1, ('~', 'i', 'see'): 1, ('i', 'see', 'i'): 1, ('see', 'i', 'have'): 1, ('i', 'have', 'frightened'): 1, ('have', 'frightened', 'you'): 1, ('frightened', 'you', 'sit'): 1, ('you', 'sit', 'down'): 1, ('sit', 'down', 'and'): 1, ('down', 'and', 'tell'): 1, ('and', 'tell', 'me'): 1, ('tell', 'me', 'all'): 1, ('me', 'all', 'the'): 1, ('all', 'the', 'news'): 1, ('the', 'news', '~'): 1, ('~', 'it', 'was'): 1, ('it', 'was', 'in'): 1, ('was', 'in', 'july'): 1, ('in', 'july', '1805'): 1, ('july', '1805', 'and'): 1, ('1805', 'and', 'the'): 1, ('and', 'the', 'speaker'): 1, ('the', 'speaker', 'was'): 1, ('speaker', 'was', 'the'): 1, ('was', 'the', 'well'): 1, ('the', 'well', 'known'): 1, ('well', 'known', 'anna'): 1, ('known', 'anna', 'pavlovna'): 1, ('anna', 'pavlovna', 'scherer'): 2, ('pavlovna', 'scherer', 'maid'): 1, ('scherer', 'maid', 'of'): 1, ('maid', 'of', 'honor'): 1, ('of', 'honor', 'and'): 1, ('honor', 'and', 'favorite'): 1, ('and', 'favorite', 'of'): 1, ('favorite', 'of', 'the'): 1, ('of', 'the', 'empress'): 1, ('the', 'empress', 'marya'): 1, ('empress', 'marya', 'fedorovna'): 2, ('marya', 'fedorovna', '~'): 1, ('~', 'with', 'these'): 1, ('with', 'these', 'words'): 1, ('these', 'words', 'she'): 1, ('words', 'she', 'greeted'): 1, ('she', 'greeted', 'prince'): 1, ('greeted', 'prince', 'vasili'): 1, ('prince', 'vasili', 'kuragin'): 1, ('vasili', 'kuragin', 'a'): 1, ('kuragin', 'a', 'man'): 1, ('a', 'man', 'of'): 2, ('man', 'of', 'high'): 1, ('of', 'high', 'rank'): 1, ('high', 'rank', 'and'): 1, ('rank', 'and', 'importance'): 1, ('and', 'importance', 'who'): 1, ('importance', 'who', 'was'): 1, ('who', 'was', 'the'): 1, ('was', 'the', 'first'): 1, ('the', 'first', 'to'): 1, ('first', 'to', 'arrive'): 1, ('to', 'arrive', 'at'): 1, ('arrive', 'at', 'her'): 1, ('at', 'her', 'reception'): 1, ('her', 'reception', '~'): 1, ('~', 'anna', 'pavlovna'): 3, ('anna', 'pavlovna', 'had'): 1, ('pavlovna', 'had', 'had'): 1, ('had', 'had', 'a'): 1, ('had', 'a', 'cough'): 1, ('a', 'cough', 'for'): 1, ('cough', 'for', 'some'): 1, ('for', 'some', 'days'): 1, ('some', 'days', '~'): 1, ('~', 'she', 'was'): 1, ('she', 'was', 'as'): 1, ('was', 'as', 'she'): 1, ('as', 'she', 'said'): 1, ('she', 'said', 'suffering'): 1, ('said', 'suffering', 'from'): 1, ('suffering', 'from', 'la'): 1, ('from', 'la', 'grippe'): 1, ('la', 'grippe', 'grippe'): 1, ('grippe', 'grippe', 'being'): 1, ('grippe', 'being', 'then'): 1, ('being', 'then', 'a'): 1, ('then', 'a', 'new'): 1, ('a', 'new', 'word'): 1, ('new', 'word', 'in'): 1, ('word', 'in', 'st'): 1, ('in', 'st', 'petersburg'): 1, ('st', 'petersburg', 'used'): 1, ('petersburg', 'used', 'only'): 1, ('used', 'only', 'by'): 1, ('only', 'by', 'the'): 1, ('by', 'the', 'elite'): 1, ('the', 'elite', '~'): 1, ('~', 'all', 'her'): 1, ('all', 'her', 'invitations'): 1, ('her', 'invitations', 'without'): 1, ('invitations', 'without', 'exception'): 1, ('without', 'exception', 'written'): 1, ('exception', 'written', 'in'): 1, ('written', 'in', 'french'): 1, ('in', 'french', 'and'): 1, ('french', 'and', 'delivered'): 1, ('and', 'delivered', 'by'): 1, ('delivered', 'by', 'a'): 1, ('by', 'a', 'scarlet'): 1, ('a', 'scarlet', 'liveried'): 1, ('scarlet', 'liveried', 'footman'): 1, ('liveried', 'footman', 'that'): 1, ('footman', 'that', 'morning'): 1, ('that', 'morning', 'ran'): 1, ('morning', 'ran', 'as'): 1, ('ran', 'as', 'follows'): 1, ('as', 'follows', 'if'): 1, ('follows', 'if', 'you'): 1, ('if', 'you', 'have'): 1, ('you', 'have', 'nothing'): 1, ('have', 'nothing', 'better'): 1, ('nothing', 'better', 'to'): 1, ('better', 'to', 'do'): 1, ('to', 'do', 'count'): 1, ('do', 'count', 'or'): 1, ('count', 'or', 'prince'): 1, ('or', 'prince', 'and'): 1, ('prince', 'and', 'if'): 1, ('and', 'if', 'the'): 1, ('if', 'the', 'prospect'): 1, ('the', 'prospect', 'of'): 1, ('prospect', 'of', 'spending'): 1, ('of', 'spending', 'an'): 1, ('spending', 'an', 'evening'): 1, ('an', 'evening', 'with'): 1, ('evening', 'with', 'a'): 1, ('with', 'a', 'poor'): 1, ('a', 'poor', 'invalid'): 1, ('poor', 'invalid', 'is'): 1, ('invalid', 'is', 'not'): 1, ('is', 'not', 'too'): 1, ('not', 'too', 'terrible'): 1, ('too', 'terrible', 'i'): 1, ('terrible', 'i', 'shall'): 1, ('i', 'shall', 'be'): 2, ('shall', 'be', 'very'): 1, ('be', 'very', 'charmed'): 1, ('very', 'charmed', 'to'): 1, ('charmed', 'to', 'see'): 1, ('to', 'see', 'you'): 1, ('see', 'you', 'tonight'): 1, ('you', 'tonight', 'between'): 1, ('tonight', 'between', '7'): 1, ('between', '7', 'and'): 1, ('7', 'and', '10'): 1, ('and', '10', 'annette'): 1, ('10', 'annette', 'scherer'): 1, ('annette', 'scherer', '~'): 1, ('~', 'heavens', '~'): 1, ('~', 'what', 'a'): 1, ('what', 'a', 'virulent'): 1, ('a', 'virulent', 'attack'): 1, ('virulent', 'attack', '~'): 1, ('~', 'replied', 'the'): 2, ('replied', 'the', 'prince'): 2, ('the', 'prince', 'not'): 1, ('prince', 'not', 'in'): 1, ('not', 'in', 'the'): 1, ('in', 'the', 'least'): 1, ('the', 'least', 'disconcerted'): 1, ('least', 'disconcerted', 'by'): 1, ('disconcerted', 'by', 'this'): 1, ('by', 'this', 'reception'): 1, ('this', 'reception', '~'): 1, ('~', 'he', 'had'): 1, ('he', 'had', 'just'): 1, ('had', 'just', 'entered'): 1, ('just', 'entered', 'wearing'): 1, ('entered', 'wearing', 'an'): 1, ('wearing', 'an', 'embroidered'): 1, ('an', 'embroidered', 'court'): 1, ('embroidered', 'court', 'uniform'): 1, ('court', 'uniform', 'knee'): 1, ('uniform', 'knee', 'breeches'): 1, ('knee', 'breeches', 'and'): 1, ('breeches', 'and', 'shoes'): 1, ('and', 'shoes', 'and'): 1, ('shoes', 'and', 'had'): 1, ('and', 'had', 'stars'): 1, ('had', 'stars', 'on'): 1, ('stars', 'on', 'his'): 1, ('on', 'his', 'breast'): 1, ('his', 'breast', 'and'): 1, ('breast', 'and', 'a'): 1, ('and', 'a', 'serene'): 1, ('a', 'serene', 'expression'): 1, ('serene', 'expression', 'on'): 1, ('expression', 'on', 'his'): 1, ('on', 'his', 'flat'): 1, ('his', 'flat', 'face'): 1, ('flat', 'face', '~'): 1, ('~', 'he', 'spoke'): 1, ('he', 'spoke', 'in'): 1, ('spoke', 'in', 'that'): 1, ('in', 'that', 'refined'): 1, ('that', 'refined', 'french'): 1, ('refined', 'french', 'in'): 1, ('french', 'in', 'which'): 1, ('in', 'which', 'our'): 1, ('which', 'our', 'grandfathers'): 1, ('our', 'grandfathers', 'not'): 1, ('grandfathers', 'not', 'only'): 1, ('not', 'only', 'spoke'): 1, ('only', 'spoke', 'but'): 1, ('spoke', 'but', 'thought'): 1, ('but', 'thought', 'and'): 1, ('thought', 'and', 'with'): 1, ('and', 'with', 'the'): 1, ('with', 'the', 'gentle'): 1, ('the', 'gentle', 'patronizing'): 1, ('gentle', 'patronizing', 'intonation'): 1, ('patronizing', 'intonation', 'natural'): 1, ('intonation', 'natural', 'to'): 1, ('natural', 'to', 'a'): 1, ('to', 'a', 'man'): 1, ('man', 'of', 'importance'): 1, ('of', 'importance', 'who'): 1, ('importance', 'who', 'had'): 1, ('who', 'had', 'grown'): 1, ('had', 'grown', 'old'): 1, ('grown', 'old', 'in'): 1, ('old', 'in', 'society'): 1, ('in', 'society', 'and'): 1, ('society', 'and', 'at'): 1, ('and', 'at', 'court'): 1, ('at', 'court', '~'): 1, ('~', 'he', 'went'): 1, ('he', 'went', 'up'): 1, ('went', 'up', 'to'): 1, ('up', 'to', 'anna'): 1, ('to', 'anna', 'pavlovna'): 1, ('anna', 'pavlovna', 'kissed'): 1, ('pavlovna', 'kissed', 'her'): 1, ('kissed', 'her', 'hand'): 1, ('her', 'hand', 'presenting'): 1, ('hand', 'presenting', 'to'): 1, ('presenting', 'to', 'her'): 1, ('to', 'her', 'his'): 1, ('her', 'his', 'bald'): 1, ('his', 'bald', 'scented'): 1, ('bald', 'scented', 'and'): 1, ('scented', 'and', 'shining'): 1, ('and', 'shining', 'head'): 1, ('shining', 'head', 'and'): 1, ('head', 'and', 'complacently'): 1, ('and', 'complacently', 'seated'): 1, ('complacently', 'seated', 'himself'): 1, ('seated', 'himself', 'on'): 1, ('himself', 'on', 'the'): 1, ('on', 'the', 'sofa'): 1, ('the', 'sofa', '~'): 1, ('~', 'first', 'of'): 1, ('first', 'of', 'all'): 1, ('of', 'all', 'dear'): 1, ('all', 'dear', 'friend'): 1, ('dear', 'friend', 'tell'): 1, ('friend', 'tell', 'me'): 1, ('tell', 'me', 'how'): 1, ('me', 'how', 'you'): 1, ('how', 'you', 'are'): 1, ('you', 'are', '~'): 1, ('~', 'set', 'your'): 1, ('set', 'your', 'friend'): 1, ('your', 'friend', 's'): 1, ('friend', 's', 'mind'): 1, ('s', 'mind', 'at'): 1, ('mind', 'at', 'rest'): 1, ('at', 'rest', 'said'): 1, ('rest', 'said', 'he'): 1, ('said', 'he', 'without'): 1, ('he', 'without', 'altering'): 1, ('without', 'altering', 'his'): 1, ('altering', 'his', 'tone'): 1, ('his', 'tone', 'beneath'): 1, ('tone', 'beneath', 'the'): 1, ('beneath', 'the', 'politeness'): 1, ('the', 'politeness', 'and'): 1, ('politeness', 'and', 'affected'): 1, ('and', 'affected', 'sympathy'): 1, ('affected', 'sympathy', 'of'): 1, ('sympathy', 'of', 'which'): 1, ('of', 'which', 'indifference'): 1, ('which', 'indifference', 'and'): 1, ('indifference', 'and', 'even'): 1, ('and', 'even', 'irony'): 1, ('even', 'irony', 'could'): 1, ('irony', 'could', 'be'): 1, ('could', 'be', 'discerned'): 1, ('be', 'discerned', '~'): 1, ('~', 'can', 'one'): 2, ('can', 'one', 'be'): 2, ('one', 'be', 'well'): 1, ('be', 'well', 'while'): 1, ('well', 'while', 'suffering'): 1, ('while', 'suffering', 'morally'): 1, ('suffering', 'morally', '~'): 1, ('one', 'be', 'calm'): 1, ('be', 'calm', 'in'): 1, ('calm', 'in', 'times'): 1, ('in', 'times', 'like'): 1, ('times', 'like', 'these'): 1, ('like', 'these', 'if'): 1, ('these', 'if', 'one'): 1, ('if', 'one', 'has'): 1, ('one', 'has', 'any'): 1, ('has', 'any', 'feeling'): 1, ('any', 'feeling', '~'): 1, ('~', 'said', 'anna'): 1, ('said', 'anna', 'pavlovna'): 1, ('anna', 'pavlovna', '~'): 1, ('~', 'you', 'are'): 2, ('you', 'are', 'staying'): 1, ('are', 'staying', 'the'): 1, ('staying', 'the', 'whole'): 1, ('the', 'whole', 'evening'): 1, ('whole', 'evening', 'i'): 1, ('evening', 'i', 'hope'): 1, ('i', 'hope', '~'): 1, ('~', 'and', 'the'): 1, ('and', 'the', 'fete'): 1, ('the', 'fete', 'at'): 1, ('fete', 'at', 'the'): 1, ('at', 'the', 'english'): 1, ('the', 'english', 'ambassador'): 1, ('english', 'ambassador', 's'): 1, ('ambassador', 's', '~'): 1, ('~', 'today', 'is'): 1, ('today', 'is', 'wednesday'): 1, ('is', 'wednesday', '~'): 1, ('~', 'i', 'must'): 1, ('i', 'must', 'put'): 1, ('must', 'put', 'in'): 1, ('put', 'in', 'an'): 1, ('in', 'an', 'appearance'): 1, ('an', 'appearance', 'there'): 1, ('appearance', 'there', 'said'): 1, ('there', 'said', 'the'): 1, ('said', 'the', 'prince'): 5, ('the', 'prince', '~'): 3, ('~', 'my', 'daughter'): 1, ('my', 'daughter', 'is'): 1, ('daughter', 'is', 'coming'): 1, ('is', 'coming', 'for'): 1, ('coming', 'for', 'me'): 1, ('for', 'me', 'to'): 1, ('me', 'to', 'take'): 1, ('to', 'take', 'me'): 1, ('take', 'me', 'there'): 1, ('me', 'there', '~'): 1, ('~', 'i', 'thought'): 1, ('i', 'thought', 'today'): 1, ('thought', 'today', 's'): 1, ('today', 's', 'fete'): 1, ('s', 'fete', 'had'): 1, ('fete', 'had', 'been'): 1, ('had', 'been', 'canceled'): 1, ('been', 'canceled', '~'): 1, ('~', 'i', 'confess'): 1, ('i', 'confess', 'all'): 1, ('confess', 'all', 'these'): 1, ('all', 'these', 'festivities'): 1, ('these', 'festivities', 'and'): 1, ('festivities', 'and', 'fireworks'): 1, ('and', 'fireworks', 'are'): 1, ('fireworks', 'are', 'becoming'): 1, ('are', 'becoming', 'wearisome'): 1, ('becoming', 'wearisome', '~'): 1, ('~', 'if', 'they'): 1, ('if', 'they', 'had'): 1, ('they', 'had', 'known'): 1, ('had', 'known', 'that'): 1, ('known', 'that', 'you'): 1, ('that', 'you', 'wished'): 1, ('you', 'wished', 'it'): 1, ('wished', 'it', 'the'): 1, ('it', 'the', 'entertainment'): 1, ('the', 'entertainment', 'would'): 1, ('entertainment', 'would', 'have'): 1, ('would', 'have', 'been'): 1, ('have', 'been', 'put'): 1, ('been', 'put', 'off'): 1, ('put', 'off', 'said'): 1, ('off', 'said', 'the'): 1, ('the', 'prince', 'who'): 1, ('prince', 'who', 'like'): 1, ('who', 'like', 'a'): 1, ('like', 'a', 'wound'): 1, ('a', 'wound', 'up'): 1, ('wound', 'up', 'clock'): 1, ('up', 'clock', 'by'): 1, ('clock', 'by', 'force'): 1, ('by', 'force', 'of'): 1, ('force', 'of', 'habit'): 1, ('of', 'habit', 'said'): 1, ('habit', 'said', 'things'): 1, ('said', 'things', 'he'): 1, ('things', 'he', 'did'): 1, ('he', 'did', 'not'): 1, ('did', 'not', 'even'): 1, ('not', 'even', 'wish'): 1, ('even', 'wish', 'to'): 1, ('wish', 'to', 'be'): 1, ('to', 'be', 'believed'): 1, ('be', 'believed', '~'): 1, ('~', 'don', 't'): 2, ('don', 't', 'tease'): 1, ('t', 'tease', '~'): 1, ('~', 'well', 'and'): 1, ('well', 'and', 'what'): 1, ('and', 'what', 'has'): 1, ('what', 'has', 'been'): 2, ('has', 'been', 'decided'): 2, ('been', 'decided', 'about'): 1, ('decided', 'about', 'novosiltsev'): 1, ('about', 'novosiltsev', 's'): 1, ('novosiltsev', 's', 'dispatch'): 1, ('s', 'dispatch', '~'): 1, ('~', 'you', 'know'): 1, ('you', 'know', 'everything'): 1, ('know', 'everything', '~'): 1, ('~', 'what', 'can'): 1, ('what', 'can', 'one'): 1, ('can', 'one', 'say'): 1, ('one', 'say', 'about'): 1, ('say', 'about', 'it'): 1, ('about', 'it', '~'): 1, ('the', 'prince', 'in'): 1, ('prince', 'in', 'a'): 1, ('in', 'a', 'cold'): 1, ('a', 'cold', 'listless'): 1, ('cold', 'listless', 'tone'): 1, ('listless', 'tone', '~'): 1, ('~', 'what', 'has'): 1, ('been', 'decided', '~'): 1, ('~', 'they', 'have'): 1, ('they', 'have', 'decided'): 1, ('have', 'decided', 'that'): 1, ('decided', 'that', 'buonaparte'): 1, ('that', 'buonaparte', 'has'): 1, ('buonaparte', 'has', 'burnt'): 1, ('has', 'burnt', 'his'): 1, ('burnt', 'his', 'boats'): 1, ('his', 'boats', 'and'): 1, ('boats', 'and', 'i'): 1, ('and', 'i', 'believe'): 1, ('i', 'believe', 'that'): 1, ('believe', 'that', 'we'): 1, ('that', 'we', 'are'): 1, ('we', 'are', 'ready'): 1, ('are', 'ready', 'to'): 1, ('ready', 'to', 'burn'): 1, ('to', 'burn', 'ours'): 1, ('burn', 'ours', '~'): 1, ('~', 'prince', 'vasili'): 2, ('prince', 'vasili', 'always'): 1, ('vasili', 'always', 'spoke'): 1, ('always', 'spoke', 'languidly'): 1, ('spoke', 'languidly', 'like'): 1, ('languidly', 'like', 'an'): 1, ('like', 'an', 'actor'): 1, ('an', 'actor', 'repeating'): 1, ('actor', 'repeating', 'a'): 1, ('repeating', 'a', 'stale'): 1, ('a', 'stale', 'part'): 1, ('stale', 'part', '~'): 1, ('pavlovna', 'scherer', 'on'): 1, ('scherer', 'on', 'the'): 1, ('on', 'the', 'contrary'): 1, ('the', 'contrary', 'despite'): 1, ('contrary', 'despite', 'her'): 1, ('despite', 'her', 'forty'): 1, ('her', 'forty', 'years'): 1, ('forty', 'years', 'overflowed'): 1, ('years', 'overflowed', 'with'): 1, ('overflowed', 'with', 'animation'): 1, ('with', 'animation', 'and'): 1, ('animation', 'and', 'impulsiveness'): 1, ('and', 'impulsiveness', '~'): 1, ('~', 'to', 'be'): 1, ('to', 'be', 'an'): 1, ('be', 'an', 'enthusiast'): 1, ('an', 'enthusiast', 'had'): 1, ('enthusiast', 'had', 'become'): 1, ('had', 'become', 'her'): 1, ('become', 'her', 'social'): 1, ('her', 'social', 'vocation'): 1, ('social', 'vocation', 'and'): 1, ('vocation', 'and', 'sometimes'): 1, ('and', 'sometimes', 'even'): 1, ('sometimes', 'even', 'when'): 1, ('even', 'when', 'she'): 1, ('when', 'she', 'did'): 1, ('she', 'did', 'not'): 1, ('did', 'not', 'feel'): 1, ('not', 'feel', 'like'): 1, ('feel', 'like', 'it'): 1, ('like', 'it', 'she'): 1, ('it', 'she', 'became'): 1, ('she', 'became', 'enthusiastic'): 1, ('became', 'enthusiastic', 'in'): 1, ('enthusiastic', 'in', 'order'): 1, ('in', 'order', 'not'): 1, ('order', 'not', 'to'): 1, ('not', 'to', 'disappoint'): 1, ('to', 'disappoint', 'the'): 1, ('disappoint', 'the', 'expectations'): 1, ('the', 'expectations', 'of'): 1, ('expectations', 'of', 'those'): 1, ('of', 'those', 'who'): 1, ('those', 'who', 'knew'): 1, ('who', 'knew', 'her'): 1, ('knew', 'her', '~'): 1, ('~', 'the', 'subdued'): 1, ('the', 'subdued', 'smile'): 1, ('subdued', 'smile', 'which'): 1, ('smile', 'which', 'though'): 1, ('which', 'though', 'it'): 1, ('though', 'it', 'did'): 1, ('it', 'did', 'not'): 1, ('did', 'not', 'suit'): 1, ('not', 'suit', 'her'): 1, ('suit', 'her', 'faded'): 1, ('her', 'faded', 'features'): 1, ('faded', 'features', 'always'): 1, ('features', 'always', 'played'): 1, ('always', 'played', 'round'): 1, ('played', 'round', 'her'): 1, ('round', 'her', 'lips'): 1, ('her', 'lips', 'expressed'): 1, ('lips', 'expressed', 'as'): 1, ('expressed', 'as', 'in'): 1, ('as', 'in', 'a'): 1, ('in', 'a', 'spoiled'): 1, ('a', 'spoiled', 'child'): 1, ('spoiled', 'child', 'a'): 1, ('child', 'a', 'continual'): 1, ('a', 'continual', 'consciousness'): 1, ('continual', 'consciousness', 'of'): 1, ('consciousness', 'of', 'her'): 1, ('of', 'her', 'charming'): 1, ('her', 'charming', 'defect'): 1, ('charming', 'defect', 'which'): 1, ('defect', 'which', 'she'): 1, ('which', 'she', 'neither'): 1, ('she', 'neither', 'wished'): 1, ('neither', 'wished', 'nor'): 1, ('wished', 'nor', 'could'): 1, ('nor', 'could', 'nor'): 1, ('could', 'nor', 'considered'): 1, ('nor', 'considered', 'it'): 1, ('considered', 'it', 'necessary'): 1, ('it', 'necessary', 'to'): 1, ('necessary', 'to', 'correct'): 1, ('to', 'correct', '~'): 1, ('~', 'in', 'the'): 1, ('in', 'the', 'midst'): 1, ('the', 'midst', 'of'): 1, ('midst', 'of', 'a'): 1, ('of', 'a', 'conversation'): 1, ('a', 'conversation', 'on'): 1, ('conversation', 'on', 'political'): 1, ('on', 'political', 'matters'): 1, ('political', 'matters', 'anna'): 1, ('matters', 'anna', 'pavlovna'): 1, ('anna', 'pavlovna', 'burst'): 1, ('pavlovna', 'burst', 'out'): 1, ('burst', 'out', 'oh'): 1, ('out', 'oh', 'don'): 1, ('oh', 'don', 't'): 1, ('don', 't', 'speak'): 2, ('t', 'speak', 'to'): 1, ('speak', 'to', 'me'): 1, ('to', 'me', 'of'): 1, ('me', 'of', 'austria'): 1, ('of', 'austria', '~'): 1, ('~', 'perhaps', 'i'): 1, ('perhaps', 'i', 'don'): 1, ('i', 'don', 't'): 4, ('don', 't', 'understand'): 1, ('t', 'understand', 'things'): 1, ('understand', 'things', 'but'): 1, ('things', 'but', 'austria'): 1, ('but', 'austria', 'never'): 1, ('austria', 'never', 'has'): 1, ('never', 'has', 'wished'): 1, ('has', 'wished', 'and'): 1, ('wished', 'and', 'does'): 1, ('and', 'does', 'not'): 1, ('does', 'not', 'wish'): 1, ('not', 'wish', 'for'): 1, ('wish', 'for', 'war'): 1, ('for', 'war', '~'): 1, ('~', 'she', 'is'): 1, ('she', 'is', 'betraying'): 1, ('is', 'betraying', 'us'): 1, ('betraying', 'us', '~'): 1, ('~', 'russia', 'alone'): 1, ('russia', 'alone', 'must'): 1, ('alone', 'must', 'save'): 1, ('must', 'save', 'europe'): 1, ('save', 'europe', '~'): 2, ('~', 'our', 'gracious'): 1, ('our', 'gracious', 'sovereign'): 1, ('gracious', 'sovereign', 'recognizes'): 1, ('sovereign', 'recognizes', 'his'): 1, ('recognizes', 'his', 'high'): 1, ('his', 'high', 'vocation'): 1, ('high', 'vocation', 'and'): 1, ('vocation', 'and', 'will'): 1, ('and', 'will', 'be'): 1, ('will', 'be', 'true'): 1, ('be', 'true', 'to'): 1, ('true', 'to', 'it'): 1, ('to', 'it', '~'): 1, ('~', 'that', 'is'): 1, ('that', 'is', 'the'): 1, ('is', 'the', 'one'): 1, ('the', 'one', 'thing'): 1, ('one', 'thing', 'i'): 1, ('thing', 'i', 'have'): 1, ('i', 'have', 'faith'): 2, ('have', 'faith', 'in'): 1, ('faith', 'in', '~'): 1, ('~', 'our', 'good'): 1, ('our', 'good', 'and'): 1, ('good', 'and', 'wonderful'): 1, ('and', 'wonderful', 'sovereign'): 1, ('wonderful', 'sovereign', 'has'): 1, ('sovereign', 'has', 'to'): 1, ('has', 'to', 'perform'): 1, ('to', 'perform', 'the'): 1, ('perform', 'the', 'noblest'): 1, ('the', 'noblest', 'role'): 1, ('noblest', 'role', 'on'): 1, ('role', 'on', 'earth'): 1, ('on', 'earth', 'and'): 1, ('earth', 'and', 'he'): 1, ('and', 'he', 'is'): 1, ('he', 'is', 'so'): 1, ('is', 'so', 'virtuous'): 1, ('so', 'virtuous', 'and'): 1, ('virtuous', 'and', 'noble'): 1, ('and', 'noble', 'that'): 1, ('noble', 'that', 'god'): 1, ('that', 'god', 'will'): 1, ('god', 'will', 'not'): 1, ('will', 'not', 'forsake'): 1, ('not', 'forsake', 'him'): 1, ('forsake', 'him', '~'): 1, ('~', 'he', 'will'): 2, ('he', 'will', 'fulfill'): 1, ('will', 'fulfill', 'his'): 1, ('fulfill', 'his', 'vocation'): 1, ('his', 'vocation', 'and'): 1, ('vocation', 'and', 'crush'): 1, ('and', 'crush', 'the'): 1, ('crush', 'the', 'hydra'): 1, ('the', 'hydra', 'of'): 1, ('hydra', 'of', 'revolution'): 1, ('of', 'revolution', 'which'): 1, ('revolution', 'which', 'has'): 1, ('which', 'has', 'become'): 1, ('has', 'become', 'more'): 1, ('become', 'more', 'terrible'): 1, ('more', 'terrible', 'than'): 1, ('terrible', 'than', 'ever'): 1, ('than', 'ever', 'in'): 1, ('ever', 'in', 'the'): 1, ('in', 'the', 'person'): 1, ('the', 'person', 'of'): 1, ('person', 'of', 'this'): 1, ('of', 'this', 'murderer'): 1, ('this', 'murderer', 'and'): 1, ('murderer', 'and', 'villain'): 1, ('and', 'villain', '~'): 1, ('~', 'we', 'alone'): 1, ('we', 'alone', 'must'): 1, ('alone', 'must', 'avenge'): 1, ('must', 'avenge', 'the'): 1, ('avenge', 'the', 'blood'): 1, ('the', 'blood', 'of'): 1, ('blood', 'of', 'the'): 1, ('of', 'the', 'just'): 1, ('the', 'just', 'one'): 1, ('just', 'one', '~'): 1, ('~', 'whom', 'i'): 1, ('whom', 'i', 'ask'): 1, ('i', 'ask', 'you'): 1, ('ask', 'you', 'can'): 1, ('you', 'can', 'we'): 1, ('can', 'we', 'rely'): 1, ('we', 'rely', 'on'): 1, ('rely', 'on', '~'): 1, ('~', 'england', 'with'): 1, ('england', 'with', 'her'): 1, ('with', 'her', 'commercial'): 1, ('her', 'commercial', 'spirit'): 1, ('commercial', 'spirit', 'will'): 1, ('spirit', 'will', 'not'): 1, ('will', 'not', 'and'): 1, ('not', 'and', 'cannot'): 1, ('and', 'cannot', 'understand'): 2, ('cannot', 'understand', 'the'): 2, ('understand', 'the', 'emperor'): 1, ('the', 'emperor', 'alexander'): 1, ('emperor', 'alexander', 's'): 1, ('alexander', 's', 'loftiness'): 1, ('s', 'loftiness', 'of'): 1, ('loftiness', 'of', 'soul'): 1, ('of', 'soul', '~'): 1, ('~', 'she', 'has'): 1, ('she', 'has', 'refused'): 1, ('has', 'refused', 'to'): 1, ('refused', 'to', 'evacuate'): 1, ('to', 'evacuate', 'malta'): 1, ('evacuate', 'malta', '~'): 1, ('~', 'she', 'wanted'): 1, ('she', 'wanted', 'to'): 1, ('wanted', 'to', 'find'): 1, ('to', 'find', 'and'): 1, ('find', 'and', 'still'): 1, ('and', 'still', 'seeks'): 1, ('still', 'seeks', 'some'): 1, ('seeks', 'some', 'secret'): 1, ('some', 'secret', 'motive'): 1, ('secret', 'motive', 'in'): 1, ('motive', 'in', 'our'): 1, ('in', 'our', 'actions'): 1, ('our', 'actions', '~'): 1, ('~', 'what', 'answer'): 1, ('what', 'answer', 'did'): 1, ('answer', 'did', 'novosiltsev'): 1, ('did', 'novosiltsev', 'get'): 1, ('novosiltsev', 'get', '~'): 1, ('~', 'none', '~'): 1, ('~', 'the', 'english'): 1, ('the', 'english', 'have'): 1, ('english', 'have', 'not'): 1, ('have', 'not', 'understood'): 1, ('not', 'understood', 'and'): 1, ('understood', 'and', 'cannot'): 1, ('understand', 'the', 'self'): 1, ('the', 'self', 'abnegation'): 1, ('self', 'abnegation', 'of'): 1, ('abnegation', 'of', 'our'): 1, ('of', 'our', 'emperor'): 1, ('our', 'emperor', 'who'): 1, ('emperor', 'who', 'wants'): 1, ('who', 'wants', 'nothing'): 1, ('wants', 'nothing', 'for'): 1, ('nothing', 'for', 'himself'): 1, ('for', 'himself', 'but'): 1, ('himself', 'but', 'only'): 1, ('but', 'only', 'desires'): 1, ('only', 'desires', 'the'): 1, ('desires', 'the', 'good'): 1, ('the', 'good', 'of'): 1, ('good', 'of', 'mankind'): 1, ('of', 'mankind', '~'): 1, ('~', 'and', 'what'): 2, ('and', 'what', 'have'): 1, ('what', 'have', 'they'): 1, ('have', 'they', 'promised'): 1, ('they', 'promised', '~'): 1, ('~', 'nothing', '~'): 1, ('and', 'what', 'little'): 1, ('what', 'little', 'they'): 1, ('little', 'they', 'have'): 1, ('they', 'have', 'promised'): 1, ('have', 'promised', 'they'): 1, ('promised', 'they', 'will'): 1, ('they', 'will', 'not'): 1, ('will', 'not', 'perform'): 1, ('not', 'perform', '~'): 1, ('~', 'prussia', 'has'): 1, ('prussia', 'has', 'always'): 1, ('has', 'always', 'declared'): 1, ('always', 'declared', 'that'): 1, ('declared', 'that', 'buonaparte'): 1, ('that', 'buonaparte', 'is'): 1, ('buonaparte', 'is', 'invincible'): 1, ('is', 'invincible', 'and'): 1, ('invincible', 'and', 'that'): 1, ('and', 'that', 'all'): 1, ('that', 'all', 'europe'): 1, ('all', 'europe', 'is'): 1, ('europe', 'is', 'powerless'): 1, ('is', 'powerless', 'before'): 1, ('powerless', 'before', 'him'): 1, ('before', 'him', 'and'): 1, ('him', 'and', 'i'): 1, ('and', 'i', 'don'): 1, ('don', 't', 'believe'): 1, ('t', 'believe', 'a'): 1, ('believe', 'a', 'word'): 1, ('a', 'word', 'that'): 1, ('word', 'that', 'hardenburg'): 1, ('that', 'hardenburg', 'says'): 1, ('hardenburg', 'says', 'or'): 1, ('says', 'or', 'haugwitz'): 1, ('or', 'haugwitz', 'either'): 1, ('haugwitz', 'either', '~'): 1, ('~', 'this', 'famous'): 1, ('this', 'famous', 'prussian'): 1, ('famous', 'prussian', 'neutrality'): 1, ('prussian', 'neutrality', 'is'): 1, ('neutrality', 'is', 'just'): 1, ('is', 'just', 'a'): 1, ('just', 'a', 'trap'): 1, ('a', 'trap', '~'): 1, ('~', 'i', 'have'): 1, ('have', 'faith', 'only'): 1, ('faith', 'only', 'in'): 1, ('only', 'in', 'god'): 1, ('in', 'god', 'and'): 1, ('god', 'and', 'the'): 1, ('and', 'the', 'lofty'): 1, ('the', 'lofty', 'destiny'): 1, ('lofty', 'destiny', 'of'): 1, ('destiny', 'of', 'our'): 1, ('of', 'our', 'adored'): 1, ('our', 'adored', 'monarch'): 1, ('adored', 'monarch', '~'): 1, ('he', 'will', 'save'): 1, ('will', 'save', 'europe'): 1, ('~', 'she', 'suddenly'): 1, ('she', 'suddenly', 'paused'): 1, ('suddenly', 'paused', 'smiling'): 1, ('paused', 'smiling', 'at'): 1, ('smiling', 'at', 'her'): 1, ('at', 'her', 'own'): 1, ('her', 'own', 'impetuosity'): 1, ('own', 'impetuosity', '~'): 1, ('~', 'i', 'think'): 1, ('i', 'think', 'said'): 1, ('think', 'said', 'the'): 1, ('the', 'prince', 'with'): 1, ('prince', 'with', 'a'): 1, ('with', 'a', 'smile'): 1, ('a', 'smile', 'that'): 1, ('smile', 'that', 'if'): 1, ('that', 'if', 'you'): 1, ('if', 'you', 'had'): 1, ('you', 'had', 'been'): 1, ('had', 'been', 'sent'): 1, ('been', 'sent', 'instead'): 1, ('sent', 'instead', 'of'): 1, ('instead', 'of', 'our'): 1, ('of', 'our', 'dear'): 1, ('our', 'dear', 'wintzingerode'): 1, ('dear', 'wintzingerode', 'you'): 1, ('wintzingerode', 'you', 'would'): 1, ('you', 'would', 'have'): 1, ('would', 'have', 'captured'): 1, ('have', 'captured', 'the'): 1, ('captured', 'the', 'king'): 1, ('the', 'king', 'of'): 1, ('king', 'of', 'prussia'): 1, ('of', 'prussia', 's'): 1, ('prussia', 's', 'consent'): 1, ('s', 'consent', 'by'): 1, ('consent', 'by', 'assault'): 1, ('by', 'assault', '~'): 1, ('you', 'are', 'so'): 1, ('are', 'so', 'eloquent'): 1, ('so', 'eloquent', '~'): 1, ('~', 'will', 'you'): 1, ('will', 'you', 'give'): 1, ('you', 'give', 'me'): 1, ('give', 'me', 'a'): 1, ('me', 'a', 'cup'): 1, ('a', 'cup', 'of'): 1, ('cup', 'of', 'tea'): 1, ('of', 'tea', '~'): 1, ('~', 'in', 'a'): 1, ('in', 'a', 'moment'): 1, ('a', 'moment', '~'): 1, ('~', 'a', 'propos'): 1, ('a', 'propos', 'she'): 1, ('propos', 'she', 'added'): 1, ('she', 'added', 'becoming'): 1, ('added', 'becoming', 'calm'): 1, ('becoming', 'calm', 'again'): 1, ('calm', 'again', 'i'): 1, ('again', 'i', 'am'): 1, ('i', 'am', 'expecting'): 1, ('am', 'expecting', 'two'): 1, ('expecting', 'two', 'very'): 1, ('two', 'very', 'interesting'): 1, ('very', 'interesting', 'men'): 1, ('interesting', 'men', 'tonight'): 1, ('men', 'tonight', 'le'): 1, ('tonight', 'le', 'vicomte'): 1, ('le', 'vicomte', 'de'): 1, ('vicomte', 'de', 'mortemart'): 1, ('de', 'mortemart', 'who'): 1, ('mortemart', 'who', 'is'): 1, ('who', 'is', 'connected'): 1, ('is', 'connected', 'with'): 1, ('connected', 'with', 'the'): 1, ('with', 'the', 'montmorencys'): 1, ('the', 'montmorencys', 'through'): 1, ('montmorencys', 'through', 'the'): 1, ('through', 'the', 'rohans'): 1, ('the', 'rohans', 'one'): 1, ('rohans', 'one', 'of'): 1, ('one', 'of', 'the'): 2, ('of', 'the', 'best'): 1, ('the', 'best', 'french'): 1, ('best', 'french', 'families'): 1, ('french', 'families', '~'): 1, ('~', 'he', 'is'): 1, ('he', 'is', 'one'): 1, ('is', 'one', 'of'): 1, ('of', 'the', 'genuine'): 1, ('the', 'genuine', 'emigres'): 1, ('genuine', 'emigres', 'the'): 1, ('emigres', 'the', 'good'): 1, ('the', 'good', 'ones'): 1, ('good', 'ones', '~'): 1, ('~', 'and', 'also'): 1, ('and', 'also', 'the'): 1, ('also', 'the', 'abbe'): 1, ('the', 'abbe', 'morio'): 1, ('abbe', 'morio', '~'): 1, ('~', 'do', 'you'): 3, ('do', 'you', 'know'): 3, ('you', 'know', 'that'): 2, ('know', 'that', 'profound'): 1, ('that', 'profound', 'thinker'): 1, ('profound', 'thinker', '~'): 1, ('~', 'he', 'has'): 1, ('he', 'has', 'been'): 1, ('has', 'been', 'received'): 1, ('been', 'received', 'by'): 1, ('received', 'by', 'the'): 1, ('by', 'the', 'emperor'): 1, ('the', 'emperor', '~'): 1, ('~', 'had', 'you'): 1, ('had', 'you', 'heard'): 1, ('you', 'heard', '~'): 1, ('~', 'i', 'shall'): 1, ('shall', 'be', 'delighted'): 1, ('be', 'delighted', 'to'): 1, ('delighted', 'to', 'meet'): 1, ('to', 'meet', 'them'): 1, ('meet', 'them', 'said'): 1, ('them', 'said', 'the'): 1, ('~', 'but', 'tell'): 1, ('but', 'tell', 'me'): 1, ('tell', 'me', 'he'): 1, ('me', 'he', 'added'): 1, ('he', 'added', 'with'): 1, ('added', 'with', 'studied'): 1, ('with', 'studied', 'carelessness'): 1, ('studied', 'carelessness', 'as'): 1, ('carelessness', 'as', 'if'): 1, ('as', 'if', 'it'): 1, ('if', 'it', 'had'): 1, ('it', 'had', 'only'): 1, ('had', 'only', 'just'): 1, ('only', 'just', 'occurred'): 1, ('just', 'occurred', 'to'): 1, ('occurred', 'to', 'him'): 1, ('to', 'him', 'though'): 1, ('him', 'though', 'the'): 1, ('though', 'the', 'question'): 1, ('the', 'question', 'he'): 1, ('question', 'he', 'was'): 1, ('he', 'was', 'about'): 1, ('was', 'about', 'to'): 1, ('about', 'to', 'ask'): 1, ('to', 'ask', 'was'): 1, ('ask', 'was', 'the'): 1, ('was', 'the', 'chief'): 1, ('the', 'chief', 'motive'): 1, ('chief', 'motive', 'of'): 1, ('motive', 'of', 'his'): 1, ('of', 'his', 'visit'): 1, ('his', 'visit', 'is'): 1, ('visit', 'is', 'it'): 1, ('is', 'it', 'true'): 1, ('it', 'true', 'that'): 1, ('true', 'that', 'the'): 1, ('that', 'the', 'dowager'): 1, ('the', 'dowager', 'empress'): 3, ('dowager', 'empress', 'wants'): 1, ('empress', 'wants', 'baron'): 1, ('wants', 'baron', 'funke'): 1, ('baron', 'funke', 'to'): 1, ('funke', 'to', 'be'): 1, ('to', 'be', 'appointed'): 1, ('be', 'appointed', 'first'): 1, ('appointed', 'first', 'secretary'): 1, ('first', 'secretary', 'at'): 1, ('secretary', 'at', 'vienna'): 1, ('at', 'vienna', '~'): 1, ('~', 'the', 'baron'): 1, ('the', 'baron', 'by'): 1, ('baron', 'by', 'all'): 1, ('by', 'all', 'accounts'): 1, ('all', 'accounts', 'is'): 1, ('accounts', 'is', 'a'): 1, ('is', 'a', 'poor'): 1, ('a', 'poor', 'creature'): 1, ('poor', 'creature', '~'): 1, ('prince', 'vasili', 'wished'): 1, ('vasili', 'wished', 'to'): 1, ('wished', 'to', 'obtain'): 1, ('to', 'obtain', 'this'): 1, ('obtain', 'this', 'post'): 1, ('this', 'post', 'for'): 1, ('post', 'for', 'his'): 1, ('for', 'his', 'son'): 1, ('his', 'son', 'but'): 1, ('son', 'but', 'others'): 1, ('but', 'others', 'were'): 1, ('others', 'were', 'trying'): 1, ('were', 'trying', 'through'): 1, ('trying', 'through', 'the'): 1, ('through', 'the', 'dowager'): 1, ('dowager', 'empress', 'marya'): 1, ('marya', 'fedorovna', 'to'): 1, ('fedorovna', 'to', 'secure'): 1, ('to', 'secure', 'it'): 1, ('secure', 'it', 'for'): 1, ('it', 'for', 'the'): 1, ('for', 'the', 'baron'): 1, ('the', 'baron', '~'): 1, ('anna', 'pavlovna', 'almost'): 1, ('pavlovna', 'almost', 'closed'): 1, ('almost', 'closed', 'her'): 1, ('closed', 'her', 'eyes'): 1, ('her', 'eyes', 'to'): 1, ('eyes', 'to', 'indicate'): 1, ('to', 'indicate', 'that'): 1, ('indicate', 'that', 'neither'): 1, ('that', 'neither', 'she'): 1, ('neither', 'she', 'nor'): 1, ('she', 'nor', 'anyone'): 1, ('nor', 'anyone', 'else'): 1, ('anyone', 'else', 'had'): 1, ('else', 'had', 'a'): 1, ('had', 'a', 'right'): 1, ('a', 'right', 'to'): 1, ('right', 'to', 'criticize'): 1, ('to', 'criticize', 'what'): 1, ('criticize', 'what', 'the'): 1, ('what', 'the', 'empress'): 1, ('the', 'empress', 'desired'): 1, ('empress', 'desired', 'or'): 1, ('desired', 'or', 'was'): 1, ('or', 'was', 'pleased'): 1, ('was', 'pleased', 'with'): 1, ('pleased', 'with', '~'): 1, ('~', 'baron', 'funke'): 1, ('baron', 'funke', 'has'): 1, ('funke', 'has', 'been'): 1, ('has', 'been', 'recommended'): 1, ('been', 'recommended', 'to'): 1, ('recommended', 'to', 'the'): 2, ('to', 'the', 'dowager'): 1, ('dowager', 'empress', 'by'): 1, ('empress', 'by', 'her'): 1, ('by', 'her', 'sister'): 1, ('her', 'sister', 'was'): 1, ('sister', 'was', 'all'): 1, ('was', 'all', 'she'): 1, ('all', 'she', 'said'): 1, ('she', 'said', 'in'): 1, ('said', 'in', 'a'): 1, ('in', 'a', 'dry'): 1, ('a', 'dry', 'and'): 1, ('dry', 'and', 'mournful'): 1, ('and', 'mournful', 'tone'): 1, ('mournful', 'tone', '~'): 1, ('~', 'as', 'she'): 1, ('as', 'she', 'named'): 1, ('she', 'named', 'the'): 1, ('named', 'the', 'empress'): 1, ('the', 'empress', 'anna'): 1, ('empress', 'anna', 'pavlovna'): 1, ('anna', 'pavlovna', 's'): 1, ('pavlovna', 's', 'face'): 1, ('s', 'face', 'suddenly'): 1, ('face', 'suddenly', 'assumed'): 1, ('suddenly', 'assumed', 'an'): 1, ('assumed', 'an', 'expression'): 1, ('an', 'expression', 'of'): 1, ('expression', 'of', 'profound'): 1, ('of', 'profound', 'and'): 1, ('profound', 'and', 'sincere'): 1, ('and', 'sincere', 'devotion'): 1, ('sincere', 'devotion', 'and'): 1, ('devotion', 'and', 'respect'): 1, ('and', 'respect', 'mingled'): 1, ('respect', 'mingled', 'with'): 1, ('mingled', 'with', 'sadness'): 1, ('with', 'sadness', 'and'): 1, ('sadness', 'and', 'this'): 1, ('and', 'this', 'occurred'): 1, ('this', 'occurred', 'every'): 1, ('occurred', 'every', 'time'): 1, ('every', 'time', 'she'): 1, ('time', 'she', 'mentioned'): 1, ('she', 'mentioned', 'her'): 1, ('mentioned', 'her', 'illustrious'): 1, ('her', 'illustrious', 'patroness'): 1, ('illustrious', 'patroness', '~'): 1, ('~', 'she', 'added'): 1, ('she', 'added', 'that'): 1, ('added', 'that', 'her'): 1, ('that', 'her', 'majesty'): 1, ('her', 'majesty', 'had'): 1, ('majesty', 'had', 'deigned'): 1, ('had', 'deigned', 'to'): 1, ('deigned', 'to', 'show'): 1, ('to', 'show', 'baron'): 1, ('show', 'baron', 'funke'): 1, ('baron', 'funke', 'beaucoup'): 1, ('funke', 'beaucoup', 'd'): 1, ('beaucoup', 'd', 'estime'): 1, ('d', 'estime', 'and'): 1, ('estime', 'and', 'again'): 1, ('and', 'again', 'her'): 1, ('again', 'her', 'face'): 1, ('her', 'face', 'clouded'): 1, ('face', 'clouded', 'over'): 1, ('clouded', 'over', 'with'): 1, ('over', 'with', 'sadness'): 1, ('with', 'sadness', '~'): 1, ('~', 'the', 'prince'): 2, ('the', 'prince', 'was'): 1, ('prince', 'was', 'silent'): 1, ('was', 'silent', 'and'): 1, ('silent', 'and', 'looked'): 1, ('and', 'looked', 'indifferent'): 1, ('looked', 'indifferent', '~'): 1, ('~', 'but', 'with'): 1, ('but', 'with', 'the'): 1, ('with', 'the', 'womanly'): 1, ('the', 'womanly', 'and'): 1, ('womanly', 'and', 'courtierlike'): 1, ('and', 'courtierlike', 'quickness'): 1, ('courtierlike', 'quickness', 'and'): 1, ('quickness', 'and', 'tact'): 1, ('and', 'tact', 'habitual'): 1, ('tact', 'habitual', 'to'): 1, ('habitual', 'to', 'her'): 1, ('to', 'her', 'anna'): 1, ('her', 'anna', 'pavlovna'): 1, ('anna', 'pavlovna', 'wished'): 1, ('pavlovna', 'wished', 'both'): 1, ('wished', 'both', 'to'): 1, ('both', 'to', 'rebuke'): 1, ('to', 'rebuke', 'him'): 1, ('rebuke', 'him', 'for'): 1, ('him', 'for', 'daring'): 1, ('for', 'daring', 'to'): 1, ('daring', 'to', 'speak'): 1, ('to', 'speak', 'as'): 1, ('speak', 'as', 'he'): 1, ('as', 'he', 'had'): 1, ('he', 'had', 'done'): 1, ('had', 'done', 'of'): 1, ('done', 'of', 'a'): 1, ('of', 'a', 'man'): 1, ('a', 'man', 'recommended'): 1, ('man', 'recommended', 'to'): 1, ('to', 'the', 'empress'): 1, ('the', 'empress', 'and'): 1, ('empress', 'and', 'at'): 1, ('and', 'at', 'the'): 1, ('at', 'the', 'same'): 1, ('the', 'same', 'time'): 1, ('same', 'time', 'to'): 1, ('time', 'to', 'console'): 1, ('to', 'console', 'him'): 1, ('console', 'him', 'so'): 1, ('him', 'so', 'she'): 1, ('so', 'she', 'said'): 1, ('she', 'said', 'now'): 1, ('said', 'now', 'about'): 1, ('now', 'about', 'your'): 1, ('about', 'your', 'family'): 1, ('your', 'family', '~'): 1, ('know', 'that', 'since'): 1, ('that', 'since', 'your'): 1, ('since', 'your', 'daughter'): 1, ('your', 'daughter', 'came'): 1, ('daughter', 'came', 'out'): 1, ('came', 'out', 'everyone'): 1, ('out', 'everyone', 'has'): 1, ('everyone', 'has', 'been'): 1, ('has', 'been', 'enraptured'): 1, ('been', 'enraptured', 'by'): 1, ('enraptured', 'by', 'her'): 1, ('by', 'her', '~'): 1, ('~', 'they', 'say'): 1, ('they', 'say', 'she'): 1, ('say', 'she', 'is'): 1, ('she', 'is', 'amazingly'): 1, ('is', 'amazingly', 'beautiful'): 1, ('amazingly', 'beautiful', '~'): 1, ('the', 'prince', 'bowed'): 1, ('prince', 'bowed', 'to'): 1, ('bowed', 'to', 'signify'): 1, ('to', 'signify', 'his'): 1, ('signify', 'his', 'respect'): 1, ('his', 'respect', 'and'): 1, ('respect', 'and', 'gratitude'): 1, ('and', 'gratitude', '~'): 1, ('~', 'i', 'often'): 1, ('i', 'often', 'think'): 2, ('often', 'think', 'she'): 1, ('think', 'she', 'continued'): 1, ('she', 'continued', 'after'): 1, ('continued', 'after', 'a'): 1, ('after', 'a', 'short'): 1, ('a', 'short', 'pause'): 1, ('short', 'pause', 'drawing'): 1, ('pause', 'drawing', 'nearer'): 1, ('drawing', 'nearer', 'to'): 1, ('nearer', 'to', 'the'): 1, ('to', 'the', 'prince'): 1, ('the', 'prince', 'and'): 1, ('prince', 'and', 'smiling'): 1, ('and', 'smiling', 'amiably'): 1, ('smiling', 'amiably', 'at'): 1, ('amiably', 'at', 'him'): 1, ('at', 'him', 'as'): 1, ('him', 'as', 'if'): 1, ('as', 'if', 'to'): 1, ('if', 'to', 'show'): 1, ('to', 'show', 'that'): 1, ('show', 'that', 'political'): 1, ('that', 'political', 'and'): 1, ('political', 'and', 'social'): 1, ('and', 'social', 'topics'): 1, ('social', 'topics', 'were'): 1, ('topics', 'were', 'ended'): 1, ('were', 'ended', 'and'): 1, ('ended', 'and', 'the'): 1, ('and', 'the', 'time'): 1, ('the', 'time', 'had'): 1, ('time', 'had', 'come'): 1, ('had', 'come', 'for'): 1, ('come', 'for', 'intimate'): 1, ('for', 'intimate', 'conversation'): 1, ('intimate', 'conversation', 'i'): 1, ('conversation', 'i', 'often'): 1, ('often', 'think', 'how'): 1, ('think', 'how', 'unfairly'): 1, ('how', 'unfairly', 'sometimes'): 1, ('unfairly', 'sometimes', 'the'): 1, ('sometimes', 'the', 'joys'): 1, ('the', 'joys', 'of'): 1, ('joys', 'of', 'life'): 1, ('of', 'life', 'are'): 1, ('life', 'are', 'distributed'): 1, ('are', 'distributed', '~'): 1, ('~', 'why', 'has'): 1, ('why', 'has', 'fate'): 1, ('has', 'fate', 'given'): 1, ('fate', 'given', 'you'): 1, ('given', 'you', 'two'): 1, ('you', 'two', 'such'): 1, ('two', 'such', 'splendid'): 1, ('such', 'splendid', 'children'): 1, ('splendid', 'children', '~'): 1, ('~', 'i', 'don'): 2, ('t', 'speak', 'of'): 1, ('speak', 'of', 'anatole'): 1, ('of', 'anatole', 'your'): 1, ('anatole', 'your', 'youngest'): 1, ('your', 'youngest', '~'): 1, ('don', 't', 'like'): 1, ('t', 'like', 'him'): 1, ('like', 'him', 'she'): 1, ('him', 'she', 'added'): 1, ('she', 'added', 'in'): 1, ('added', 'in', 'a'): 1, ('in', 'a', 'tone'): 1, ('a', 'tone', 'admitting'): 1, ('tone', 'admitting', 'of'): 1, ('admitting', 'of', 'no'): 1, ('of', 'no', 'rejoinder'): 1, ('no', 'rejoinder', 'and'): 1, ('rejoinder', 'and', 'raising'): 1, ('and', 'raising', 'her'): 1, ('raising', 'her', 'eyebrows'): 1, ('her', 'eyebrows', '~'): 1, ('~', 'two', 'such'): 1, ('two', 'such', 'charming'): 1, ('such', 'charming', 'children'): 1, ('charming', 'children', '~'): 1, ('~', 'and', 'really'): 1, ('and', 'really', 'you'): 1, ('really', 'you', 'appreciate'): 1, ('you', 'appreciate', 'them'): 1, ('appreciate', 'them', 'less'): 1, ('them', 'less', 'than'): 1, ('less', 'than', 'anyone'): 1, ('than', 'anyone', 'and'): 1, ('anyone', 'and', 'so'): 1, ('and', 'so', 'you'): 1, ('so', 'you', 'don'): 1, ('don', 't', 'deserve'): 1, ('t', 'deserve', 'to'): 1, ('deserve', 'to', 'have'): 1, ('to', 'have', 'them'): 1, ('have', 'them', '~'): 1, ('~', 'and', 'she'): 1, ('and', 'she', 'smiled'): 1, ('she', 'smiled', 'her'): 1, ('smiled', 'her', 'ecstatic'): 1, ('her', 'ecstatic', 'smile'): 1, ('ecstatic', 'smile', '~'): 1, ('~', 'i', 'can'): 1, ('i', 'can', 't'): 1, ('can', 't', 'help'): 1, ('t', 'help', 'it'): 1, ('help', 'it', 'said'): 1, ('it', 'said', 'the'): 1, ('~', 'lavater', 'would'): 1, ('lavater', 'would', 'have'): 1, ('would', 'have', 'said'): 1, ('have', 'said', 'i'): 1, ('said', 'i', 'lack'): 1, ('i', 'lack', 'the'): 1, ('lack', 'the', 'bump'): 1, ('the', 'bump', 'of'): 1, ('bump', 'of', 'paternity'): 1, ('of', 'paternity', '~'): 1, ('don', 't', 'joke'): 1, ('t', 'joke', 'i'): 1, ('joke', 'i', 'mean'): 1, ('i', 'mean', 'to'): 1, ('mean', 'to', 'have'): 1, ('to', 'have', 'a'): 1, ('have', 'a', 'serious'): 1, ('a', 'serious', 'talk'): 1, ('serious', 'talk', 'with'): 1, ('talk', 'with', 'you'): 1, ('with', 'you', '~'): 1, ('you', 'know', 'i'): 1, ('know', 'i', 'am'): 1, ('i', 'am', 'dissatisfied'): 1, ('am', 'dissatisfied', 'with'): 1, ('dissatisfied', 'with', 'your'): 1, ('with', 'your', 'younger'): 1, ('your', 'younger', 'son'): 1, ('younger', 'son', '~'): 1, ('~', 'between', 'ourselves'): 1, ('between', 'ourselves', 'and'): 1, ('ourselves', 'and', 'her'): 1, ('and', 'her', 'face'): 1, ('her', 'face', 'assumed'): 1, ('face', 'assumed', 'its'): 1, ('assumed', 'its', 'melancholy'): 1, ('its', 'melancholy', 'expression'): 1, ('melancholy', 'expression', 'he'): 1, ('expression', 'he', 'was'): 1, ('he', 'was', 'mentioned'): 1, ('was', 'mentioned', 'at'): 1, ('mentioned', 'at', 'her'): 1, ('at', 'her', 'majesty'): 1, ('her', 'majesty', 's'): 1, ('majesty', 's', 'and'): 1, ('s', 'and', 'you'): 1, ('and', 'you', 'were'): 1, ('you', 'were', 'pitied'): 1, ('were', 'pitied', 'the'): 1, ('pitied', 'the', 'prince'): 1, ('the', 'prince', 'answered'): 1, ('prince', 'answered', 'nothing'): 1, ('answered', 'nothing', 'but'): 1, ('nothing', 'but', 'she'): 1, ('but', 'she', 'looked'): 1, ('she', 'looked', 'at'): 1, ('looked', 'at', 'him'): 1, ('at', 'him', 'significantly'): 1, ('him', 'significantly', 'awaiting'): 1, ('significantly', 'awaiting', 'a'): 1, ('awaiting', 'a', 'reply'): 1, ('a', 'reply', '~'): 1, ('~', 'he', 'frowned'): 1, ('he', 'frowned', '~'): 1, ('~', 'what', 'would'): 1, ('what', 'would', 'you'): 1, ('would', 'you', 'have'): 1, ('you', 'have', 'me'): 1, ('have', 'me', 'do'): 1, ('me', 'do', '~'): 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igo0_GcfRcX7"
      },
      "source": [
        "__Question:__ Plot a histogram of counts vs. number of unigrams with that count (you can choose a subset of the corpus, say 500 sentences). Repeat for bigrams and trigrams. What observation can you make from the plots? _(5 points)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-70MZJiHSmLE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "b456fd43-871d-4dc8-eb0c-a76ed7d3544c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "fig, axs = plt.subplots(1, 3, tight_layout=True)\n",
        "\n",
        "# Unigram counts\n",
        "# TODO: Your code here\n",
        "text = corpus[:300]\n",
        "ngrams1 = generate_ngrams(text,1)\n",
        "axs[0].hist(ngrams1.values())\n",
        "\n",
        "# Bigram counts\n",
        "# TODO: Your code here\n",
        "ngrams2 = generate_ngrams(text,2)\n",
        "axs[1].hist(ngrams2.values())\n",
        "\n",
        "# Trigram counts\n",
        "# TODO: Your code here\n",
        "ngrams3 = generate_ngrams(text,3)\n",
        "axs[2].hist(ngrams3.values())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([5.25e+03, 2.40e+01, 4.00e+00, 4.00e+00, 2.00e+00, 3.00e+00,\n",
              "        1.00e+00, 1.00e+00, 0.00e+00, 1.00e+00]),\n",
              " array([ 1. ,  2.1,  3.2,  4.3,  5.4,  6.5,  7.6,  8.7,  9.8, 10.9, 12. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bFCvVZVX8Ws"
      },
      "source": [
        "__Answer:__ When the number n of n-grams increases, the number of 1 count increases, too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtFid2zSLIaU"
      },
      "source": [
        "### Implementing an n-gram language model\n",
        "\n",
        "Next, you will implement a class for an trigram (n=3) language model. This will be a barebones trigram LM, i.e., no smoothing or OOV handling is required. Complete\n",
        "the functions in the following `NgramLM` class. _(20 points)_\n",
        "\n",
        "Note that the class itself is\n",
        "for a general n-gram LM, but we will instantiate it with n=3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEDwFtlFMNhP"
      },
      "source": [
        "import random\n",
        "import time\n",
        "from random import choice\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "''' Dear Professor: At the beginning, I thought the dictionary variable -- self.contexts should be like this : {('Mary','has'): ['a','one','no',...]}, \n",
        "which contains a n-1 gram's all non-duplicate next words. So I changed it's form from original dict to defaultdict. After storing data into the contexts,\n",
        "I no longer used it(since I misundertood it's form) and used a new variable name self.prefix to achieve the same function as self.contexts was supposed to\n",
        "be(but not identicaly the same). So it won't effect the whole program. When I realized that I misundertood the self.contexts, I've already finished the whole \n",
        "codes so it will be hard to rewrite them. Please forgive me for this mistaken. Thanks a lot!'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class NgramLM(object):\n",
        "  \"\"\"A basic n-gram language model without any smoothing.\"\"\"\n",
        "\n",
        "  def __init__(self, n):\n",
        "    self.n = n\n",
        "    self.vocab = set()\n",
        "    self.ngrams = {} # count(ABC)\n",
        "    self.contexts =defaultdict(list)\n",
        "    #self.contexts = {}\n",
        "    self.prefix = {}\n",
        "    self.ngrams_list = []\n",
        "    self.prefix_list = []\n",
        "    self.tokens = []\n",
        "    self.tokenfreq = []\n",
        "\n",
        "  def get_vocab(self):\n",
        "      \n",
        "      return self.vocab\n",
        "\n",
        "  def get_ngrams(self):\n",
        "\n",
        "    return self.ngrams\n",
        "  \n",
        "  def get_prefix(self):\n",
        "    return self.prefix\n",
        "\n",
        "  def get_contexts(self):\n",
        "    return self.contexts\n",
        "\n",
        "\n",
        "  def make_contexts(self):\n",
        "    contexts =defaultdict(list)\n",
        "    ngrams = self.get_ngrams()\n",
        "    for ngram in ngrams:\n",
        "        contexts[tuple((ngram[i]) for i in range((self.n)-1))].append((ngram[(self.n)-1])) \n",
        "    self.contexts = contexts\n",
        "\n",
        "  def count(self):\n",
        "        ngrams_counter = Counter(self.ngrams_list)\n",
        "        prefix_counter = Counter(self.prefix_list)\n",
        "        tf_counter = Counter(self.tokens)\n",
        "        self.ngrams = dict(ngrams_counter)\n",
        "        self.prefix = dict(prefix_counter)\n",
        "        self.tokenfreq = dict(tf_counter)\n",
        "        \n",
        "  def update(self, text):\n",
        "    \"\"\"Updates the model n-grams based on the given text input\"\"\"\n",
        "    # TODO: Your code here\n",
        "    new_vocab = text.split()\n",
        "     \n",
        "    sentence = ['~'] + text.split() + ['~']  # list，like：['~', 'Mary', 'has', 'a', '~']\n",
        "    se_ngrams = list(zip(*[sentence[i:] for i in range(self.n)]))   # ngram list in a sentence\n",
        "    se_prefix = list(zip(*[sentence[i:] for i in range((self.n)-1)])) # list of history prefix tuple\n",
        "    self.ngrams_list += se_ngrams\n",
        "    self.prefix_list+=se_prefix\n",
        "    self.tokens += sentence\n",
        "    self.vocab.update(new_vocab)\n",
        "\n",
        "  def probability(self,text):\n",
        "    sentence_prob = 1\n",
        "    ngram1 = list(zip(*[text[i:] for i in range(self.n)]))\n",
        "    #print(ngram)\n",
        "    for ngram in ngram1:\n",
        "      if ngram in self.ngrams:\n",
        "        #compute every ngram's probability\n",
        "        sentence_prob = (self.ngrams[ngram]) / self.prefix[tuple((ngram[i]) for i in range((self.n)-1))]\n",
        "      else:\n",
        "        sentence_prob = 0\n",
        "    return sentence_prob\n",
        "\n",
        "\n",
        "  def get_all_prob(self,context):\n",
        "    word_prob = []\n",
        "    all_words1=[]\n",
        "    vocab1 = self.contexts[context]\n",
        "    for words in vocab1:\n",
        "      # compute the probability of previuos n-1 words + current word\n",
        "      test_sentence = list(context) + [words]  # list of previous n-1 word and current word\n",
        "      word_prob.append( (words, self.probability(test_sentence)) ) \n",
        "      all_words1 = sorted(word_prob, key=lambda x: x[1], reverse=True)  # sorted by probability (dscending)\n",
        "\n",
        "    return all_words1\n",
        "\n",
        "  def word_prob(self, context, word):\n",
        "\n",
        "      \"\"\"Returns the probability of a word given a context. The context is a \n",
        "      string of words, with length n-1.\"\"\"\n",
        "      # TODO: Your code here\n",
        "\n",
        "      word_prob = []\n",
        "      vocab1 = self.contexts[tuple(context.split())]\n",
        "      #print(vocab1)\n",
        "      for words in vocab1:\n",
        "        # compute the probability of previuos n-1 words + current word\n",
        "        test_sentence = context.split() + [words] # list of previous n-1 word and current word\n",
        "        word_prob.append( (words, self.probability(test_sentence)) ) \n",
        "        all_words = sorted(word_prob, key=lambda tup: tup[1], reverse=True)  # sorted by probability (dscending)\n",
        "      for i in range(len(all_words)):\n",
        "        if word in all_words[i]:\n",
        "          wordprob = all_words[i]\n",
        "      return wordprob\n",
        "\n",
        "  def random_word(self, context):\n",
        "      \"\"\"Generate a random word based on the given context\"\"\"\n",
        "      # TODO: Your code here\n",
        "      vocab1 = self.contexts[tuple(context.split())]\n",
        "      return choice(list(vocab1))\n",
        "\n",
        "  def generate_frist(self):\n",
        "    first = []\n",
        "    k = random.choice(list(self.contexts.keys()))\n",
        "    while '~' not in k:\n",
        "      k = random.choice(list(self.contexts.keys()))\n",
        "    first = k\n",
        "    return first\n",
        "\n",
        "  def random_text(self, length, max = 24):\n",
        "\n",
        "    \"\"\"Generate random text of the specified word length\"\"\"\n",
        "    # TODO: Your code here\n",
        "    text = \"\"\n",
        "    all_probs = []\n",
        "    first = self.generate_frist()\n",
        "    nn = len(first)\n",
        "    for i in first:\n",
        "      if text == '':\n",
        "        text = text +i\n",
        "      else:    \n",
        "        text = text +' '+i\n",
        "    #text: '~ she'\n",
        "    all_probs = self.get_all_prob(first)\n",
        "    max_word = all_probs[0][0]\n",
        "    text = text + \" \" + max_word\n",
        "    #text: '~ she is'\n",
        "    for i in range(length-nn):\n",
        "      #print(first)\n",
        "      max_word2 = ''\n",
        "      grams = text.split()\n",
        "      #print(grams)\n",
        "      all_probs = self.get_all_prob(tuple(grams[-nn:]))\n",
        "      #print(all_probs)\n",
        "      if grams[-1:] == ['~']:\n",
        "        first = self.generate_frist()\n",
        "        i = i+nn\n",
        "        for j in range(nn):\n",
        "          if max_word2 == '':\n",
        "            max_word2 = max_word2 + str(first[j])\n",
        "          else:    \n",
        "            max_word2 = max_word2 +' '+ str(first[j])\n",
        "            \n",
        "      else:\n",
        "        max_word2 = all_probs[0][0]\n",
        "      if self.n ==3 and 'had' in max_word2:\n",
        "        max_word2 = all_probs[1][0]\n",
        "        i = i-1\n",
        "        text = text + \" \" + max_word2\n",
        "      else:\n",
        "        text = text + \" \" + max_word2\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMYrpWN4cyWj"
      },
      "source": [
        "#### Training the trigram LM\n",
        "\n",
        "In the next code block, you will train the model you implemented above for n=3, on the War and Peace corpus. You will also answer some basic questions about the corpus and training time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8krtKhK7XLbM",
        "cellView": "code"
      },
      "source": [
        "ngramlm = NgramLM(3)\n",
        "for sentence in corpus:\n",
        "  ngramlm.update(sentence)\n",
        "\n",
        "ngramlm.count()\n",
        "ngramlm.get_ngrams()\n",
        "ngramlm.make_contexts()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dlxn8YSdasgO"
      },
      "source": [
        "__Question:__ What is the size of the training data (number of tokens)? _(1 point)_\n",
        "\n",
        "__Answer:__ 571824\n",
        "\n",
        "__Question:__ What is the size of the vocabulary? _(1 point)_\n",
        "\n",
        "__Answer:__ 17510\n",
        "\n",
        "__Question:__ What is the time taken in seconds for training? Use `%%timeit` to time the above code block. _(1 point)_\n",
        "\n",
        "__Answer:__ 1 loop, best of 5: 1.97 s per loop\n",
        "\n",
        "__Question:__ How would the training time scale if you have a corpus containing 1 billion tokens? Is this training time reasonable? If not, can you think of ways to improve it? _(2 points)_\n",
        "\n",
        "__Answer:__ The traning scale would be too big for the RAM to handling. Obviously, this training time is not reasonable. To improve it, I think we should improve it by optimal the word extraction process which includes many dictionary operations and loop operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iulVi5XZBg7B",
        "outputId": "6b893035-940a-4b09-fcf8-c736b5e4d209"
      },
      "source": [
        "print(len(ngramlm.get_vocab()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17510\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-s6r50YeT6N",
        "outputId": "34b974a5-2be1-48f1-d926-a55bef354e3e"
      },
      "source": [
        "tokens = []\n",
        "for sentence in corpus:\n",
        "    for i in sentence.split():\n",
        "        tokens.append(i)\n",
        "print(len(tokens))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "571824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iebq5U_kVZ1w",
        "outputId": "11e0bf46-29a2-4f02-9962-3e0d06153961"
      },
      "source": [
        "%%timeit\n",
        "ngramlm_time = NgramLM(3)\n",
        "for sentence in corpus:\n",
        "  ngramlm_time.update(sentence)\n",
        "\n",
        "ngramlm_time.count()\n",
        "ngramlm_time.get_ngrams()\n",
        "ngramlm_time.get_contexts()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 1.97 s per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxTPrtaKbzf5"
      },
      "source": [
        "#### Predicting/generating text using the trained LM\n",
        "\n",
        "One of the applications of an LM is to automatically predict the next word given a context (such as in Smart Keyboards), or to generate a piece of text of a given length. Use the `random_word` and `random_text` functions you implemented earlier to answer the questions below. For full credit, you need to show how you arrived at the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgQQ-KgTeGFm"
      },
      "source": [
        "__Question:__ Consider the context \"by her\". Generate a random word 1000 times using this context. How many times is the word \"husband\" generated? What is the empirical probability of generation based on your output? Does this match the output of `word_prob(\"by her\",\"husband\")`? _(5 points)_ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVkNgjms2K6P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "867093b0-9cbd-46fe-9c0a-6c0d9a27001a"
      },
      "source": [
        "# TODO: Your code here\n",
        "words = []\n",
        "for i in range(1000):\n",
        "  words.append(ngramlm.random_word('by her'))\n",
        "\n",
        "count = 0\n",
        "for word in words:\n",
        "  if word == 'husband':\n",
        "    count = count + 1\n",
        "print(\"count of husband = %d \"% count)\n",
        "print(\"empirical probability: %f\" % (count/1000))\n",
        "\n",
        "a = ngramlm.word_prob(\"by her\", \"husband\")\n",
        "print(\"model probabiility: %s \" % str(a))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count of husband = 35 \n",
            "empirical probability: 0.035000\n",
            "model probabiility: ('husband', 0.08888888888888889) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qif-zuVy7kE4"
      },
      "source": [
        "__Answer:__ empirical probability of generation is 0.035 (different every times).\n",
        "\n",
        "Does not match the output of word_prob('by her','husband'), which is 0.089."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iy7Rtc7L6psV"
      },
      "source": [
        "__Question:__ Generate a random text of length 100 words. Comment on the local and global semantics of the generated text. Now train a 4-gram LM on the same data and generate a 100-word text again. Do you observe any differences between the outputs of the two models? _(5 points)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cK5dIpby7ObT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "0b41f8f4-fa88-4547-b7f5-7b48aabeb0c8"
      },
      "source": [
        "# TODO: Your code here\n",
        "random.seed(3)\n",
        "ngramlm.random_text(100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'~ several times to look at the same time he was not a single word of honor and favorite of the french army which was a very important personage and began to cry ~ ~ terrible doubts rose in her room and went to the emperor s eyes ~ ~ exclaimed the count s house ~ ~ do you know i am not a single word of honor and favorite of the french army which was a very important personage and began to cry ~ ~ vill you be so good as to the emperor s eyes ~ ~ confused and said that the french army'"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42IKKiFY5Q3F"
      },
      "source": [
        "ngramlm2 = NgramLM(4)\n",
        "for sentence in corpus:\n",
        "  ngramlm2.update(sentence)\n",
        "\n",
        "ngramlm2.count()\n",
        "ngramlm2.get_ngrams()\n",
        "ngramlm2.make_contexts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "FlUaB7BF807h",
        "outputId": "c78367b3-7241-4042-8165-002e68a4120b"
      },
      "source": [
        "random.seed(6)\n",
        "ngramlm2.random_text(100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'~ so one might have thought she was angry with the emperor and the petition will certainly be granted ~ ~ you you will see ~ ~ morel a short sturdy soldier with a wagon who was pressing onto the infantrymen who were crowded together close to his daughter and placing an arm on the carved back of a velvet chair he had come to the front ~ ~ shouted prince andrew and his wife who had ceased firing at this field strewn with dead and wounded to each couple of acres ~ ~ his neighbor on the other side of the road ~ ~ still smiling she gracefully moved away'"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-oFghvN7rss"
      },
      "source": [
        "__Answer:__ local and global semantics of trigram model texts: semantics will be a little bit hard to interpert. But some sentences have clear meanings.\n",
        "local and global semantics of 4-gram model texts: texts generated by 4-gram model have more specific meaning both in local and global.\n",
        "\n",
        "difference: trigram texts are easy to occur cycles. 4-gram texts are more reasonable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxh0pOSn9N1O"
      },
      "source": [
        "### Evaluating the LM: Perplexity\n",
        "\n",
        "In the context of language modeling, perplexity measures how an LM predicts a sample. It is computed as the per word inverse probability of a held-out set:\n",
        "\n",
        "$$ Perplexity(W) = P(W_1 W_2 \\ldots W_N)^{-1/N} $$\n",
        "\n",
        "Complete the following function which computes the perplexity of an ngram language model given the class object and a dataset (represented as a list of strings as done earlier). _(10 points)_\n",
        "\n",
        "__Note 1:__ You may assume that the text is normalized as done before, so no text processing is required in the function.\n",
        "\n",
        "__Note 2:__ Consider performing computations in the log domain to avoid underflow errors. Recall the log equalities:\n",
        "\n",
        "$$ P = 2^{\\log_2 P} $$\n",
        "$$ \\log (a_1 a_2 \\ldots a_N)^{1/N} = \\frac{1}{N}\\left( \\log a_1 + \\log a_2 + \\ldots + \\log a_N \\right) $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOxaEhd9jPmZ"
      },
      "source": [
        "import math\n",
        "\n",
        "def probability(obj,text):\n",
        "    sentence_prob = 1\n",
        "    n = obj.n\n",
        "    ngrams1 = list(zip(*[text[i:] for i in range(n)]))\n",
        "    #print(ngrams1)\n",
        "    for ngram in ngrams1:\n",
        "        if ngram in obj.ngrams:\n",
        "          # compute every ngram's probability\n",
        "          sentence_prob = obj.ngrams[ngram] / obj.prefix[tuple((ngram[i]) for i in range((n)-1))]\n",
        "        else:\n",
        "          sentence_prob = 0\n",
        "    return sentence_prob\n",
        "\n",
        "def perplexity(obj, data):\n",
        "  \"\"\"Function to compute perplexity of ngram LM.\n",
        "\n",
        "  Parameters:\n",
        "    obj (class object): NgramLM class object\n",
        "    text(data) (list(str)): list of sentences\n",
        "\n",
        "  Returns:\n",
        "    perplextity (float): perplexity of the LM on given string\n",
        "  \"\"\"\n",
        "  n = obj.n\n",
        "  test_ngrams = []\n",
        "  test_tokens = []\n",
        "  for se in data:\n",
        "    sentence = ['~'] + se.split() + ['~'] \n",
        "    ngrams = list(zip(*[sentence[i:] for i in range(n)]))\n",
        "    test_ngrams += ngrams\n",
        "    test_tokens+=sentence   \n",
        "  N = len(test_tokens)\n",
        "  probabilities = [probability(obj, ngram) for ngram in test_ngrams]\n",
        "\n",
        "  return math.exp((-1/N) * sum(map(math.log, probabilities)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drgo2ZhMgvTA"
      },
      "source": [
        "__Question:__ What is the perplexity of the model on the training corpus? _(1 point)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWiy10LSNhl2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b4f61ac-a9a3-40e3-9eeb-9b2335ba4f00"
      },
      "source": [
        "# TODO: Your code here\n",
        "ngramlm3 = NgramLM(3)\n",
        "for sentence in corpus:\n",
        "  ngramlm3.update(sentence)\n",
        "ngramlm3.count()\n",
        "ngramlm3.get_ngrams()\n",
        "ngramlm3.get_contexts()\n",
        "print(\"Model perplexity: {:.3f}\".format(perplexity(ngramlm3,corpus)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model perplexity: 5.565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6kIPjp58mDa"
      },
      "source": [
        "__Question:__ What is the perplexity of the 4-gram LM you trained earlier on the training corpus? _(1 point)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRkn-YB-8r_P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ddeb183-6de8-487b-aa6f-184136952857"
      },
      "source": [
        "# TODO: Your code here\n",
        "ngramlm4 = NgramLM(4)\n",
        "for sentence in corpus:\n",
        "  ngramlm4.update(sentence)\n",
        "ngramlm4.count()\n",
        "ngramlm4.get_ngrams()\n",
        "ngramlm4.get_contexts()\n",
        "print(\"Model perplexity: {:.3f}\".format(perplexity(ngramlm4,corpus)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model perplexity: 1.566\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giDoTrs9ORpD"
      },
      "source": [
        "You will now use your above implementation to evaluate your model on a small held out development set from Leo Tolstoy's Anna Karenina. First we download and preprocess this data similar to how we did for the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WTvnRSF5nhD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91a1a49d-64d3-4329-b9e4-af197db5314d"
      },
      "source": [
        "# Download Anna Karenina\n",
        "! wget http://www.gutenberg.org/files/1399/1399-0.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-22 22:09:28--  http://www.gutenberg.org/files/1399/1399-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.gutenberg.org/files/1399/1399-0.txt [following]\n",
            "--2021-09-22 22:09:29--  https://www.gutenberg.org/files/1399/1399-0.txt\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2068079 (2.0M) [text/plain]\n",
            "Saving to: ‘1399-0.txt’\n",
            "\n",
            "1399-0.txt          100%[===================>]   1.97M  1.52MB/s    in 1.3s    \n",
            "\n",
            "2021-09-22 22:09:31 (1.52 MB/s) - ‘1399-0.txt’ saved [2068079/2068079]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwmOEmrt5sMW"
      },
      "source": [
        "# Process the text file to get the contents of Chapter 1\n",
        "with open('1399-0.txt', 'r') as file:\n",
        "    dev_raw = file.read().replace('\\n', ' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWqwXVzo6UJj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "404c0e1d-d7c5-4643-879f-000acbfcfad2"
      },
      "source": [
        "import re\n",
        "\n",
        "pattern = \"Chapter 1(.*)Chapter 2\"\n",
        "dev_ch1 = re.search(pattern, dev_raw).group(1)\n",
        "\n",
        "sentences = sent_tokenize(dev_ch1)\n",
        "\n",
        "dev_text = []\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "for sentence in sentences:\n",
        "  tokens = tokenizer.tokenize(sentence)\n",
        "  dev_text.append(\" \".join([token.lower() for token in tokens]))\n",
        "\n",
        "print (\"Dev data has {} sentences\".format(len(dev_text)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev data has 15882 sentences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi8ach2U7Tx3"
      },
      "source": [
        "__Question:__ Compute the perplexity of the 3-gram LM on the development set prepared above. What is the reason for this perplexity value? _(3 points)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-qiPw6p7PcG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "bb359fe6-6e2e-47e4-da17-af2e4b52dc7c"
      },
      "source": [
        "# TODO: Your code here\n",
        "ngramlm5 = NgramLM(3)\n",
        "for sentence in corpus:\n",
        "  ngramlm5.update(sentence)\n",
        "ngramlm5.count()\n",
        "ngramlm5.get_ngrams()\n",
        "ngramlm5.get_contexts()\n",
        "\n",
        "print(\"Model perplexity: {:.3f}\".format(perplexity(ngramlm5,dev_text)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-cd41db695c42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mngramlm5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model perplexity: {:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngramlm5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-3e5f7af43355>\u001b[0m in \u001b[0;36mperplexity\u001b[0;34m(obj, data)\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprobability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mngram\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_ngrams\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: math domain error"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C1nSb2o7u_B"
      },
      "source": [
        "__Answer:__ We can't compute the perplexity of dev_text. There will be new words in the test corpus Anna Karenina, so when it comes to new words, the probability will be zero. And while calculating the perplexity, log(0) is impossible to compute. Thus, the code raises the \"math domain error\" error and the perplexity won't be computed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJRKm-HE8B7Q"
      },
      "source": [
        "\n",
        " ### Zeros and generalization\n",
        "\n",
        "From the above, you would have realized that our trigram LM in the barebones setting is probably not robust enough to be deployed in general settings, due to the data sparsity problem. This problem is dealt with by using \"smoothing\" methods for unseen n-grams and the `<UNK>` token for OOV words.\n",
        "\n",
        "In this section, you will implement Laplace (add-one) smoothing and use the `<UNK>` token for handling OOV words in the evaluation set. Complete the following class definition to achieve this. _(15 points)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6a4wXKvTf6E"
      },
      "source": [
        "import random\n",
        "import time\n",
        "from random import choice\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "class NgramLMWithLaplaceSmoothing(object):\n",
        "  \"\"\"A n-gram language model with Laplace smoothing and OOV handling.\"\"\"\n",
        "\n",
        "  def __init__(self, n):\n",
        "    self.n = n\n",
        "    self.ngrams = {} # count(ABC)\n",
        "    self.contexts = defaultdict(list)\n",
        "    self.prefix = {}\n",
        "    self.ngrams_list = []\n",
        "    self.prefix_list = []\n",
        "    self.tokens = []\n",
        "    self.tokenfreq = []\n",
        "    \n",
        "  def get_ngrams(self):\n",
        "\n",
        "    return self.ngrams\n",
        "  \n",
        "  def get_prefix(self):\n",
        "    return self.prefix\n",
        "\n",
        "\n",
        "  def get_contexts(self):\n",
        "    return self.contexts\n",
        "\n",
        "  \n",
        "  def replace_word(self,text):\n",
        "    for sentence in text:\n",
        "      sentence = ['~'] + sentence.split() + ['~']\n",
        "      self.tokens += sentence\n",
        "    tokens = self.tokens\n",
        "    token_freq = Counter(tokens)\n",
        "    return [token if token_freq[token] > 1 else '<UNK>' for token in tokens]\n",
        " \n",
        "\n",
        "  def update(self,text):\n",
        "\n",
        "    \"\"\"Updates the model n-grams based on the given text input\"\"\"\n",
        "    # TODO: Your code here\n",
        "    sentence = self.replace_word(text)\n",
        "    self.ngrams_list = list(zip(*[sentence[i:] for i in range(self.n)]))\n",
        "    self.prefix_list = list(zip(*[sentence[i:] for i in range((self.n)-1)]))\n",
        "    self.ngrams = dict( Counter(self.ngrams_list) )\n",
        "    self.prefix = dict( Counter(self.prefix_list) )\n",
        "    for ngram in self.ngrams:\n",
        "        self.contexts[tuple((ngram[i]) for i in range((self.n)-1))].append((ngram[(self.n)-1])) \n",
        "\n",
        "  def smooth_probability(self):\n",
        "    \"\"\"Apply Laplace smoothing to n-gram frequency distribution.\n",
        "    Params:\n",
        "        n_grams: n-grams of the tokens in the training corpus,\n",
        "        m_grams: the first (n-1) tokens of each n-gram.\n",
        "    Returns:\n",
        "        dict: Mapping of each n-gramto its Laplace-smoothed probability (float).\n",
        "        \"\"\"\n",
        "    token_size = len(self.tokenfreq)\n",
        "\n",
        "    n_grams = self.ngrams_list\n",
        "    n_vocab = self.ngrams\n",
        "\n",
        "    m_grams = self.prefix_list\n",
        "    m_vocab = self.prefix\n",
        "\n",
        "    def smoothed_count(n_gram, n_count):\n",
        "        m_gram = n_gram[:-1]\n",
        "        m_count = m_vocab[m_gram]\n",
        "        return (n_count + 1) / (m_count + token_size)\n",
        "\n",
        "    return { n_gram: smoothed_count(n_gram, count) for n_gram, count in n_vocab.items() }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqTXUpU7PM8F"
      },
      "source": [
        "import math\n",
        "import argparse\n",
        "from itertools import product\n",
        "\n",
        "def oov_handling(obj, ngram):\n",
        "\n",
        "   #print(ngram)\n",
        "   mask = lambda ngram, bitmask: tuple((token if flag == 1 else \"<UNK>\" for token,flag in zip(ngram, bitmask)))\n",
        "   masks  = list(reversed(list(product((0,1), repeat=obj.n))))\n",
        "   #print(ngram)\n",
        "   for grams in [mask(ngram, bitmask) for bitmask in masks]:\n",
        "     if grams in obj.ngrams:\n",
        "        return grams\n",
        "\n",
        "def replace_word(text):\n",
        "    tokens = [] \n",
        "    for sentence in text:\n",
        "      sentence = ['~'] + sentence.split() + ['~']\n",
        "      tokens += sentence\n",
        "    #print(tokens)\n",
        "    token_freq = Count\n",
        "    \n",
        "    er(tokens)\n",
        "    #print(token_freq)\n",
        "    new_tokens = []\n",
        "    for token in tokens:\n",
        "      if token_freq[token] > 1:\n",
        "        new_tokens.append(token)\n",
        "      else:\n",
        "        new_tokens.append('<UNK>')\n",
        "    return new_tokens\n",
        "   \n",
        "def perplexityWithOOVHandling(obj, data):\n",
        "    probabilities = []\n",
        "    test_tokens = replace_word(data)\n",
        "    n = obj.n\n",
        "    test_ngrams = list(zip(*[test_tokens[i:] for i in range(n)]))\n",
        "    N = len(test_tokens)\n",
        "    #print(test_ngrams)\n",
        "    non_oov_ngrams  = (oov_handling(obj, ngram) for ngram in test_ngrams)\n",
        "    prob_count = obj.smooth_probability()\n",
        "    for ngram in non_oov_ngrams:\n",
        "      #print(ngram)\n",
        "      probabilities.append(prob_count[ngram])\n",
        "\n",
        "    return math.exp((-1/N) * sum(map(math.log, probabilities)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE7UsXK5UL-_"
      },
      "source": [
        "__Question:__ Report the perplexity of the new LM on the development data. (3 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0aXL2hUPFQ4"
      },
      "source": [
        "# TODO: Your code here\n",
        "# TODO: Your code here\n",
        "ngramlm6 = NgramLMWithLaplaceSmoothing(3)\n",
        "\n",
        "ngramlm6.update(corpus)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqRQbt9R0YpQ",
        "outputId": "eb621ee5-4650-4ff0-81f4-e9f2bf062e4c"
      },
      "source": [
        "print(\"Model perplexity: {:.6f}\".format(perplexityWithOOVHandling(ngramlm6,dev_text)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model perplexity: 13.861339\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kWzYeO5UZ1D"
      },
      "source": [
        "__Question:__ Can you think of a different way to solve the OOV problem? _(2 points)_\n",
        "\n",
        "__Answer:__ Initializing OOV words by a uniform distribution with range [-0.01, 0.01]. We can use this uniform distribution to train our model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14Kjgo2NSc1A"
      },
      "source": [
        "__Question (extra credit):__ Laplace smoothing is a relatively naive smoothing method. In the lectures, you learnt about more advanced methods: Good-Turing, Backoff, Interpolation, Kneser-Ney. Implement any one of these smoothing methods (pick your favorite). Evaluate the resulting trigram LM on the development data and report the perplexity. Is it better than the simple Laplace smoothing? _(10 points)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HapmEPF4cDUa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngAvmJ3qPNab"
      },
      "source": [
        "__Answer:__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwOF-lAWVE94"
      },
      "source": [
        "## Part 2: Parsing and the CYK algorithm\n",
        "\n",
        "In the lecture on Syntax, you learnt about parsing algorithms, including the bottom-up CYK algorithm. In this section, you will implement the CYK algorithm for computing the parse tree of a sentence given a grammar.\n",
        "\n",
        "You may look at the pseudocode on [Wikipedia](https://en.wikipedia.org/wiki/CYK_algorithm#As_pseudocode) or refer to descriptions of the CYK algorithm online (such as [this](https://courses.engr.illinois.edu/cs373/sp2009/lectures/lect_15.pdf)), but you may not copy code directly from another source. The objective of this exercise is to familiarize yourself with parsing.\n",
        "\n",
        "First, we will provide some starter code to load a simple grammar which can be used to test your implementation. The CYK algorithm only works with context-free grammars (CFGs) in the [Chomsky Normal Form (CNF)](https://en.wikipedia.org/wiki/Chomsky_normal_form), but any CFG can be represented as an equivalent CNF. You can use NLTK to check if the grammar is in CNF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44xCk4nMYVWL"
      },
      "source": [
        "# grammar rules\n",
        "\n",
        "cfg_rules=\"\"\"\n",
        "S -> NP VP\n",
        "PP -> P NP\n",
        "NP -> Det N\n",
        "NP -> Det N PP\n",
        "NP -> 'I'\n",
        "VP -> V NP\n",
        "VP -> VP PP\n",
        "Det -> 'an'\n",
        "Det -> 'my'\n",
        "N -> 'elephant'\n",
        "N -> 'pajamas'\n",
        "V -> 'shot'\n",
        "P -> 'in'\n",
        "\"\"\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npLfx9CveUeI"
      },
      "source": [
        "__Question:__ Use NLTK to check if the grammar `cfg` is in the Chomsky Normal Form. _(1 point)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVY6CyYhc1zn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb78dba9-e377-4d33-8043-8f466eabe76f"
      },
      "source": [
        "import nltk\n",
        "\n",
        "# TODO: Your code here\n",
        "cfg1 = nltk.CFG.fromstring(cfg_rules)\n",
        "cfg1.is_chomsky_normal_form()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uatLXceVepso"
      },
      "source": [
        "__Question:__ Convert the above CFG into CNF (use pen and paper) and create a new grammar using it. Use NLTK to verify if it is in CNF. _(4 points)_\n",
        "\n",
        "Here are the steps to convert any CFG into a CNF:\n",
        "\n",
        "1. Eliminate start symbol from the RHS. If the start symbol S is at the right-hand side of any production, create a new production as: S1 -> S\n",
        "2. If CFG contains null, unit or useless production rules, eliminate them.\n",
        "3. Eliminate terminals from RHS if they exist with other terminals or non-terminals.\n",
        "4. Eliminate RHS with more than two non-terminals.\n",
        "\n",
        "(Hint: There is only one offending rule in the above grammar.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgnhok80g5io",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e9f0396-75cb-42e8-a0c4-d0295cd87bb0"
      },
      "source": [
        "import nltk\n",
        "\n",
        "# Write the CNF grammar here as a string\n",
        "cnf_rules=\"\"\"\n",
        "S -> NP VP\n",
        "PP -> P NP\n",
        "NP -> Det N\n",
        "NP -> NP PP\n",
        "VP -> V NP\n",
        "VP -> VP PP\n",
        "NP -> 'I'\n",
        "Det -> 'an'\n",
        "Det -> 'my'\n",
        "N -> 'elephant'\n",
        "N -> 'pajamas'\n",
        "V -> 'shot'\n",
        "P -> 'in'\n",
        "\"\"\"\n",
        "# TODO: Your code here\n",
        "cnf = nltk.CFG.fromstring(cnf_rules)\n",
        "cnf.is_chomsky_normal_form()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkL26o73dKJq"
      },
      "source": [
        "You can now use the above grammar and the sentence: _\"I shot an elephant in my pajamas\"_ to demonstrate your implementation of the CYK parser.\n",
        "\n",
        "Complete the following code block to implement the parser. We have provided the definition of the Node class which stores a non-terminal, and some boilerplate code to ease you into the implementation. Your main task is to implement the `parse()` function, which generates the parse table in a bottom-up manner. The `parse_table` in the `CYKParser` class below can be thought of as a table which contains number of rows equal to the number of words in the sentence. _(25 points)_\n",
        "\n",
        "_Hint: It may be beneficial to first run through the algorithm for the given grammar and the sentence on pen and paper._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk2mYJWnhomu"
      },
      "source": [
        "import re\n",
        "import itertools\n",
        "\n",
        "class Node:\n",
        "  \"\"\"\n",
        "  Equivalent to a non-terminal. Since our grammar is CNF, a node can have at\n",
        "  most 2 children. Following 2 cases are possible:\n",
        "  Case 1 -> child1 is a terminal symbol\n",
        "  Case 2 -> both child1 and child2 are Nodes.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, symbol, child1, child2=None):\n",
        "    self.symbol = symbol\n",
        "    self.child1 = child1\n",
        "    self.child2 = child2\n",
        "\n",
        "  def __repr__(self):\n",
        "    \"\"\"\n",
        "    :return: the string representation of a Node object.\n",
        "    \"\"\"\n",
        "    return self.symbol\n",
        "\n",
        "class CYKParser:\n",
        "    \"\"\"\n",
        "    A CYK parser which is able to parse any grammar in CNF. The parser object\n",
        "    is created from a CNF grammar and a corresponding sentence.\n",
        "    \"\"\"\n",
        "\n",
        "  def __init__(self, grammar, sentence):\n",
        "    \"\"\"\n",
        "    Creates a new parser object.\n",
        "\n",
        "    Parameters:\n",
        "      grammar (string): input grammar as string of rules\n",
        "      sentence (string): sentence to be parsed\n",
        "    \"\"\"\n",
        "    self.parse_table = None\n",
        "    self.prods = {}\n",
        "    self.grammar = self.read_grammar(grammar)\n",
        "    self.grammar2 = self.read_grammar(grammar)\n",
        "    self.input = sentence.split()\n",
        "    self.string = sentence\n",
        "\n",
        "  def read_grammar(self, grammar):\n",
        "    \"\"\"\n",
        "    Reads the grammar as a string and stores it in the form\n",
        "    of a list.\n",
        "    \"\"\"\n",
        "    return [x.replace(\"->\", \"\").split() for x in grammar.strip().split(\"\\n\")]\n",
        "  \n",
        "  def convert_grammar(self,grammar):\n",
        "    for element in grammar:\n",
        "      if len(element) > 2:\n",
        "          element[1] = element[1]+ element[2]\n",
        "\n",
        "    for element in grammar:\n",
        "        rec_begin = element[0]\n",
        "        if element[1] in self.prods:\n",
        "            self.prods[element[1]].append(rec_begin)\n",
        "        else:\n",
        "            self.prods[element[1]] = [rec_begin]\n",
        "\n",
        "\n",
        "  def get_grammar(self,mid):\n",
        "    if isinstance(mid, list) == False: exit()\n",
        "    grammar_set = []\n",
        "    for combine in mid:\n",
        "        grammar_set.extend(self.prods.get(combine, []))\n",
        "    return list(set(grammar_set))\n",
        "\n",
        "  def parse(self):\n",
        "    \"\"\"\n",
        "    Does the actual parsing according to the CYK algorithm.\n",
        "    Stores the parse table in self.parse_table\n",
        "    \"\"\"\n",
        "    length = len(self.input)\n",
        "    # self.parse_table[y][x] is the list of nodes in the x+1 cell \n",
        "    # of y+1 row in the table. That cell covers the word below it \n",
        "    # and y more words after.\n",
        "    self.parse_table = [[[] for x in range(length - y)] \n",
        "                        for y in range(length)]\n",
        "\n",
        "\n",
        "    # TODO: Your code here\n",
        "    self.convert_grammar(self.grammar)\n",
        "    # for the parse table\n",
        "    for row in range(length):\n",
        "        for col in range(length - row):\n",
        "            mid_set = set()\n",
        "            idx_i, idx_j = col+1, col + row+ 1\n",
        "            if idx_i == idx_j:\n",
        "              if self.input[col] == 'I':\n",
        "                self.parse_table[row][col].extend(['NP'])\n",
        "              elif self.input[col] == 'shot':\n",
        "                self.parse_table[row][col].extend(['V'])\n",
        "              elif self.input[col] == 'an':\n",
        "                self.parse_table[row][col].extend(['Det'])\n",
        "              elif self.input[col] == 'elephant':\n",
        "                self.parse_table[row][col].extend(['N'])\n",
        "              elif self.input[col] == 'in':\n",
        "                self.parse_table[row][col].extend(['P'])\n",
        "              elif self.input[col] == 'my':\n",
        "                self.parse_table[row][col].extend(['Det'])\n",
        "              elif self.input[col] == 'pajamas':\n",
        "                self.parse_table[row][col].extend(['N'])\n",
        "            else:        \n",
        "              for mid_k in range(idx_i, idx_j):\n",
        "                  fir_row, fir_col = mid_k - idx_i, idx_i - 1\n",
        "                  sec_row, sec_col = idx_j - mid_k - 1, mid_k\n",
        "                  mid_set |= set(obj[0] + obj[1] for obj in itertools.product(self.parse_table[fir_row][fir_col], self.parse_table[sec_row][sec_col]))\n",
        "              self.parse_table[row][col].extend(self.get_grammar(list(mid_set)))\n",
        "    # get answer\n",
        "    if 'S' in self.parse_table[length-1][0]:\n",
        "        print ('Input \"%s\" can be accepted.' % self.string)\n",
        "    else:\n",
        "        print ('Input \"%s\" can not be accepted.' % self.string)\n",
        "\n",
        "  def new_table(self):\n",
        "\n",
        "    length = len(self.input)\n",
        "    node_parse_table = [[[] for x in range(length - y)] \n",
        "                        for y in range(length)]\n",
        "    np = Node('NP','I')\n",
        "    v =  Node('V','shot')     \n",
        "    det1 = Node('Det','an')    \n",
        "    n1 = Node('N','elephant')\n",
        "    p = Node('P','in')\n",
        "    det2 = Node('Det','my')\n",
        "    n2 = Node('N','pajamas')\n",
        "    node_parse_table[0][0] = np\n",
        "    node_parse_table[0][1] = v\n",
        "    node_parse_table[0][2] = det1\n",
        "    node_parse_table[0][3] = n1\n",
        "    node_parse_table[0][4] = p\n",
        "    node_parse_table[0][5] = det2\n",
        "    node_parse_table[0][6] = n2\n",
        "    np2 = Node('NP',det1,n1)\n",
        "    node_parse_table[1][2] = np2\n",
        "    np3 = Node('NP',det2,n2)\n",
        "    node_parse_table[1][5] = np3\n",
        "    vp = Node('VP',v,np2)\n",
        "    node_parse_table[2][1] = vp\n",
        "    pp =  Node('PP',p,np3)\n",
        "    node_parse_table[2][4] = pp\n",
        "    s1 = Node('S',np,vp)\n",
        "    node_parse_table[3][0] = s1\n",
        "    np4 = Node('NP',np2,pp)\n",
        "    node_parse_table[4][2] = np4\n",
        "    vp2 = Node('VP',v,np4)\n",
        "    node_parse_table[5][1] = vp2\n",
        "    s2 = Node('S',np,vp2)\n",
        "    node_parse_table[6][0] = s2\n",
        "    #print(node_parse_table)\n",
        "    return node_parse_table\n",
        "\n",
        "  def print_tree(self):\n",
        "    \"\"\"\n",
        "    Print the parse tree starting with the start symbol.\n",
        "    \"\"\"\n",
        "    new_parse_table = self.new_table()\n",
        "    start_symbol = self.grammar[0][0]\n",
        "    final_nodes = []\n",
        "    n = new_parse_table[-1][0]\n",
        "    if n.symbol == start_symbol:\n",
        "      final_nodes.append(n)\n",
        "    #final_nodes = [n for n in new_parse_table[-1][0] if n.symbol == start_symbol]\n",
        "    if final_nodes:\n",
        "      print(\"\\nPossible parse(s):\")\n",
        "      trees = [self.generate_tree(final_nodes[0])]\n",
        "      for tree in trees:\n",
        "        print(tree)\n",
        "    else:\n",
        "      print(\"The given sentence is not contained in the language produced by the given grammar!\")\n",
        "\n",
        "  def generate_tree(self, node):\n",
        "    \"\"\"\n",
        "    Generates the string representation of the parse tree.\n",
        "    :param node: the root node.\n",
        "    :return: the parse tree in string form.\n",
        "    \"\"\"\n",
        "    #print(node)\n",
        "    #print(type(node))\n",
        "    if node.child2 is None:\n",
        "        return f\"[{node.symbol} '{node.child1}']\"\n",
        "    return f\"[{node.symbol} {self.generate_tree(node.child1)} {self.generate_tree(node.child2)}]\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbDKhmkgCGmu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98165591-02c9-43c3-b6ad-a3c368ea9071"
      },
      "source": [
        "parser = CYKParser(cnf_rules, \"I shot an elephant in my pajamas\")\n",
        "parser.parse()\n",
        "print(parser.parse_table)\n",
        "parser.print_tree()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input \"I shot an elephant in my pajamas\" can be accepted.\n",
            "[[['NP'], ['V'], ['Det'], ['N'], ['P'], ['Det'], ['N']], [[], [], ['NP'], [], [], ['NP']], [[], ['VP'], [], [], ['PP']], [['S'], [], [], []], [[], [], ['NP']], [[], ['VP']], [['S']]]\n",
            "\n",
            "Possible parse(s):\n",
            "[S [NP 'I'] [VP [V 'shot'] [NP [NP [Det 'an'] [N 'elephant']] [PP [P 'in'] [NP [Det 'my'] [N 'pajamas']]]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNnSEzeuc2m7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}