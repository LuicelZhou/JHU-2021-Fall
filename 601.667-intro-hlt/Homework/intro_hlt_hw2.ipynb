{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "intro-hlt-hw2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLm_JxCxrkIE"
      },
      "source": [
        "# Assignment 2\n",
        "\n",
        "In the last few lectures, you have looked at two work-horses of natural language processing: recurrent neural networks, and word embeddings (distributional semantics). In this assignment, you will put the learning into practice by implementing some of these yourself, and doing some exploratory analysis.\n",
        "\n",
        "# Setup\n",
        "\n",
        "For this assignment, as in the previous one, we will be using Google Colab, for both code as well as descriptive questions. Your task is to finish all the questions in the Colab notebook and then upload a PDF version of the notebook, and a viewable link on Gradescope. \n",
        "\n",
        "### Google colaboratory\n",
        "\n",
        "Before getting started, get familiar with google colaboratory:\n",
        "https://colab.research.google.com/notebooks/welcome.ipynb\n",
        "\n",
        "This is a neat python environment that works in the cloud and does not require you to\n",
        "set up anything on your personal machine\n",
        "(it also has some built-in IDE features that make writing code easier).\n",
        "Moreover, it allows you to copy any existing collaboratory file, alter it and share\n",
        "with other people.\n",
        "\n",
        "### Submission\n",
        "\n",
        "Before you start working on this homework do the following steps:\n",
        "\n",
        "1. Press __File > Save a copy in Drive...__ tab. This will allow you to have your own copy and change it.\n",
        "2. Follow all the steps in this collaboratory file and write / change / uncomment code as necessary.\n",
        "3. Do not forget to occasionally press __File > Save__ tab to save your progress.\n",
        "4. After all the changes are done and progress is saved press __Share__ button (top right corner of the page), press __get shareable link__ and make sure you have the option __Anyone with the link can view__ selected.\n",
        "5. After completing the notebook, press __File > Download .ipynb__ to download a local copy on your computer, and then use `jupyter nbconvert --to pdf intro-hlt-hw2.ipynb` to convert the notebook into PDF format for uploading on Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2kR0zyxKUh2"
      },
      "source": [
        "```\n",
        "# Paste your Colab notebook link here\n",
        "https://colab.research.google.com/drive/1JJoZYwO8ExF0PGV8dtzkF3JK6dsNZUTk?usp=sharing\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj0UWhFRmNcm"
      },
      "source": [
        "### Part 1: Language modeling with RNNs\n",
        "\n",
        "For the first part of this assignment, you will train a character-level language model on a lyrics dataset.\n",
        "\n",
        "More specifically, you will:\n",
        "1. Implement a character-level language model based on recurrent neural networks.\n",
        "2. Train it on a lyrics dataset.\n",
        "3. Sample previously unseen lyrics from our model.\n",
        "4. Augment your model with an artist information, so that you can generate songs conditioned on some artist.\n",
        "\n",
        "Let's get started!\n",
        "\n",
        "#### Character-Level language model\n",
        "\n",
        "![alt text](http://warmspringwinds.github.io/assets/img/character_level_model.jpg \"Logo Title Text 1\")\n",
        "\n",
        "Before choosing a model, let’s have a closer look at our task. Given current letter and all previous letters, we will try to predict the next character. During training we will just take a sequence, and use all its characters except the last one as an input and the same sequence starting from the second character as groundtruth (see the picture above).\n",
        "\n",
        "Our language model is defined on a character level. We will create a dictionary which will contain all English characters plus some special symbols, such as period, comma, and end-of-line symbol. Each charecter will be represented as one-hot-encoded tensor. For more information about character-level models and examples, refer to the [the following resource](https://github.com/spro/practical-pytorch).\n",
        "\n",
        "\n",
        "Our objective is to model is _p_(current letter | all previous letters). At first, the task seems intractable as the number of previous letters is variable and it might become really large in case of long sequences. \n",
        "\n",
        "__Question:__ Based on what you learned in Assignment 1, describe how you would deal with this problem? _(2 points)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IDdwQ-aubA2"
      },
      "source": [
        "__Answer:__ We can use the n-gram model to specify the number of previous letters. So large scale sequences could be the same as tiny senquences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7b0RJuauGLd"
      },
      "source": [
        "#### Recurrent Neural Networks (RNNs)\n",
        "\n",
        "![alt text](http://warmspringwinds.github.io/assets/img/rnn_unfold.jpg \"Logo Title Text 1\")\n",
        "\n",
        "RNNs are a family of neural networks for processing sequential data. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Because of arbitrary size input sequences, they are concisely depicted as a graph with a cycle (see the picture). But they can be “unfolded” if the size of input sequence is known. They define a non-linear mapping from a current input $x_t$ and previous hidden state $s_{t−1}$ to the output $o_t$ and current hidden state $s_t$. Hidden state size has a predefined size and stores features which are updated on each step and affect the result of mapping.\n",
        "\n",
        "Now align the previous picture of the character-level language model and the unfolded RNN picture to see how we are using the RNN model to learn a character level language model.\n",
        "\n",
        "While the picture depicts the Vanilla RNN, LSTMs or GRUs are more commonly used in practice.\n",
        "\n",
        "For a more elaborate introduction to RNNs, we refer reader to [the following resource](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/).\n",
        "\n",
        "__Question:__ Why are LSTMs/GRUs preferred over RNNs? _(3 points)_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvoUw3D7yG0r"
      },
      "source": [
        "__Answer:__ Because RNNs have the vanishing gradient problem. LSTMs and GRUs were created as a solution to the vanishing gradient problem. They have internal mechanisms called gates that can regulate the flow of information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxyGXlYGnkuK"
      },
      "source": [
        "#### Downloading the data\n",
        "\n",
        "For your experiments, you will use a subset of [Song Lyrics Kaggle dataset](https://www.kaggle.com/mousehead/songlyrics) which contains a good variety of recent artists and more older ones. It is stored as a Pandas file originally, but you we will provide you with a Python wrapper to use the data for training your models easily."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCGNCWWLoixC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa745359-e1a0-4204-d3e0-98ec2d332ca7"
      },
      "source": [
        "# Downloading dataset and installing dependencies\n",
        "!wget https://www.dropbox.com/s/ge1bhvik5jya9hr/songdata.csv?dl=0\n",
        "!mv songdata.csv\\?dl\\=0 songdata.csv\n",
        "!pip install livelossplot==0.3.4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-13 20:10:18--  https://www.dropbox.com/s/ge1bhvik5jya9hr/songdata.csv?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6016:18::a27d:112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/ge1bhvik5jya9hr/songdata.csv [following]\n",
            "--2021-10-13 20:10:18--  https://www.dropbox.com/s/raw/ge1bhvik5jya9hr/songdata.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc5e82bc97ce5ce047eeadaba377.dl.dropboxusercontent.com/cd/0/inline/BX-P5qe0ZxJ67dWoMsUS78Tf78HIPVVw2x3NgyD4bA1pnBQtUB3yFJJhavUjmDK31ChjTmODvogTysk24zKdOcsiFSgDq1rddourHCyZF6sQRRBd3GFkzWznbP3Dlne2QT1e9W0UTzcsWHqDyKi_LQg9/file# [following]\n",
            "--2021-10-13 20:10:18--  https://uc5e82bc97ce5ce047eeadaba377.dl.dropboxusercontent.com/cd/0/inline/BX-P5qe0ZxJ67dWoMsUS78Tf78HIPVVw2x3NgyD4bA1pnBQtUB3yFJJhavUjmDK31ChjTmODvogTysk24zKdOcsiFSgDq1rddourHCyZF6sQRRBd3GFkzWznbP3Dlne2QT1e9W0UTzcsWHqDyKi_LQg9/file\n",
            "Resolving uc5e82bc97ce5ce047eeadaba377.dl.dropboxusercontent.com (uc5e82bc97ce5ce047eeadaba377.dl.dropboxusercontent.com)... 162.125.80.15, 2620:100:6016:15::a27d:10f\n",
            "Connecting to uc5e82bc97ce5ce047eeadaba377.dl.dropboxusercontent.com (uc5e82bc97ce5ce047eeadaba377.dl.dropboxusercontent.com)|162.125.80.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 72436445 (69M) [text/plain]\n",
            "Saving to: ‘songdata.csv?dl=0’\n",
            "\n",
            "songdata.csv?dl=0   100%[===================>]  69.08M  18.9MB/s    in 4.5s    \n",
            "\n",
            "2021-10-13 20:10:24 (15.4 MB/s) - ‘songdata.csv?dl=0’ saved [72436445/72436445]\n",
            "\n",
            "Collecting livelossplot==0.3.4\n",
            "  Downloading livelossplot-0.3.4-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from livelossplot==0.3.4) (5.3.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from livelossplot==0.3.4) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot==0.3.4) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot==0.3.4) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot==0.3.4) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot==0.3.4) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot==0.3.4) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->livelossplot==0.3.4) (1.15.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->livelossplot==0.3.4) (0.12.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->livelossplot==0.3.4) (1.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->livelossplot==0.3.4) (2.11.3)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook->livelossplot==0.3.4) (5.1.3)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook->livelossplot==0.3.4) (4.8.1)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.7/dist-packages (from notebook->livelossplot==0.3.4) (5.1.1)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.7/dist-packages (from notebook->livelossplot==0.3.4) (5.1.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from notebook->livelossplot==0.3.4) (4.10.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from notebook->livelossplot==0.3.4) (0.2.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook->livelossplot==0.3.4) (5.6.1)\n",
            "Requirement already satisfied: jupyter-client>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from notebook->livelossplot==0.3.4) (5.3.5)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=5.2.0->notebook->livelossplot==0.3.4) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->livelossplot==0.3.4) (0.7.0)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->notebook->livelossplot==0.3.4) (5.5.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.3.4) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.3.4) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.3.4) (57.4.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.3.4) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.3.4) (1.0.18)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.3.4) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.3.4) (4.4.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->notebook->livelossplot==0.3.4) (0.2.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->livelossplot==0.3.4) (2.0.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook->livelossplot==0.3.4) (0.5.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook->livelossplot==0.3.4) (0.3)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook->livelossplot==0.3.4) (1.5.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook->livelossplot==0.3.4) (4.1.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook->livelossplot==0.3.4) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook->livelossplot==0.3.4) (0.7.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->livelossplot==0.3.4) (2.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook->livelossplot==0.3.4) (21.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook->livelossplot==0.3.4) (0.5.1)\n",
            "Installing collected packages: livelossplot\n",
            "Successfully installed livelossplot-0.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q4EfBmmox5J"
      },
      "source": [
        "#### Exploring the downloaded data\n",
        "\n",
        "First, let us create a dictionary, we will use 100 characters and some special symbols including $\\n$, which will allow our generator to decide when to move to a new line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yp6UPpBRpCt1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3929390-dd6a-40e7-da5b-ef3780024178"
      },
      "source": [
        "import torch\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "\n",
        "\n",
        "all_characters = string.printable\n",
        "number_of_characters = len(all_characters)\n",
        "\n",
        "print(all_characters)\n",
        "#print(number_of_characters)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
            "\r\u000b\f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8mlo_01pHs8"
      },
      "source": [
        "Below we will define some helper functions that will help us to convert a character to corresponding\n",
        "labels that we will use for actual training. Read the implementations and figure out what each function does. \n",
        "\n",
        "__Question:__ Why would a `pad_sequence` function be required? _(2 points)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFnPzBRtpMYM"
      },
      "source": [
        "def character_to_label(character):\n",
        "    character_label = all_characters.find(character)\n",
        "    #find(sub(,start,end))\n",
        "    #Returns the lowest index of the substring if it is found in a given string. If it’s not found then it returns -1.\n",
        "    return character_label\n",
        "\n",
        "def string_to_labels(character_string):    \n",
        "    return list(map(lambda character: character_to_label(character), character_string))\n",
        "    #returns the list of every character's label.\n",
        "\n",
        "def pad_sequence(seq, max_length, pad_label=100):\n",
        "    seq += [pad_label for i in range(max_length - len(seq))]\n",
        "    #返回长度为maxlength的序列 （在seq的基础上填充padlabel）\n",
        "    #如输入[1,2,3,4],10 输出为[1,2,3,4,100,100,100,100,100,100]\n",
        "    return seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bTpXYNKpmwR"
      },
      "source": [
        "__Answer:__ Inputs of our model is a tensor, so it can only accept sequence inputs of the same length. So if the current sequence is of variable length, then pad_sequences() is used. This function converts the sequence into a new sequence of the same length after padding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJx5FZsvp9lj"
      },
      "source": [
        "Now we will define a dataset class that will take care of loading data from the dataset. Complete the `__getitem__` function below. Specifically, your job is to obtain padded input-output pairs of sequences from the data. _(3 points)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxryOy3AqPDL"
      },
      "source": [
        "# The class works in two modes: train and validation, which we will\n",
        "# use during training. It also pads sequences to be of the same length.\n",
        "\n",
        "class LyricsGenerationDataset(data.Dataset):\n",
        "    \n",
        "    def __init__(self, csv_file_path,\n",
        "                 minimum_song_count=None,\n",
        "                 artists=None,\n",
        "                 train=True):\n",
        "        \n",
        "        self.lyrics_dataframe = pd.read_csv(csv_file_path)\n",
        "        \n",
        "        if artists:    \n",
        "            self.lyrics_dataframe = self.lyrics_dataframe[self.lyrics_dataframe.artist.isin(artists)]\n",
        "            self.lyrics_dataframe = self.lyrics_dataframe.reset_index()\n",
        "        \n",
        "        if minimum_song_count:\n",
        "            self.lyrics_dataframe = self.lyrics_dataframe.groupby('artist').filter(lambda x: len(x) > minimum_song_count)\n",
        "            self.lyrics_dataframe = self.lyrics_dataframe.reset_index()\n",
        "        \n",
        "        # Get the length of the biggest lyric text\n",
        "        # We will need that for padding\n",
        "        self.max_text_len = self.lyrics_dataframe.text.str.len().max()\n",
        "        whole_dataset_len = len(self.lyrics_dataframe)\n",
        "        self.indexes = range(whole_dataset_len)\n",
        "        \n",
        "        # We will use the first 500 samples for validation set\n",
        "        if train:\n",
        "          self.indexes = self.indexes[500:]\n",
        "        else:\n",
        "          self.indexes = self.indexes[:500]\n",
        "        \n",
        "        self.artists_list = list(self.lyrics_dataframe.artist.unique())\n",
        "        self.number_of_artists = len(self.artists_list)\n",
        "    \n",
        "    def __len__(self):    \n",
        "        return len(self.indexes)\n",
        "    \n",
        "    def __getitem__(self, index):    \n",
        "        \n",
        "        index = self.indexes[index]\n",
        "        sequence_raw_string = self.lyrics_dataframe.loc[index].text\n",
        "        sequence_string_labels = string_to_labels(sequence_raw_string)\n",
        "        sequence_length = len(sequence_string_labels) - 1\n",
        "\n",
        "        # Generate input and output sequence (shifted by 1 character)\n",
        "        # from the sequence_string_labels\n",
        "        # YOUR CODE\n",
        "\n",
        "        input_sequence = sequence_string_labels[:-1]\n",
        "        output_sequence = sequence_string_labels[1:]\n",
        "\n",
        "        # Pad sequences so that all of them have the same length. For\n",
        "        # the output sequence, use pad_label=-100 so that it is omitted\n",
        "        # in cross-entropy loss. \n",
        "        # YOUR CODE   \n",
        "        \n",
        "        input_string_labels_padded = pad_sequence(input_sequence,self.max_text_len) \n",
        "        output_string_labels_padded = pad_sequence(output_sequence,self.max_text_len,pad_label=-100)\n",
        "\n",
        "        return (torch.LongTensor(input_string_labels_padded),\n",
        "                torch.LongTensor(output_string_labels_padded),\n",
        "                torch.LongTensor([sequence_length]) )\n",
        "\n",
        "# Here are the artists\n",
        "artists = [\n",
        "'ABBA',\n",
        "'Ace Of Base',\n",
        "'Backstreet Boys',\n",
        "'Bob Marley',\n",
        "'Bon Jovi',\n",
        "'Britney Spears',\n",
        "'Bruno Mars',\n",
        "'Coldplay',\n",
        "'Ed Sheeran',\n",
        "'Elton John',\n",
        "'Elvis Presley',\n",
        "'Eminem',\n",
        "'Evanescence',\n",
        "'Fall Out Boy',\n",
        "'Foo Fighters',\n",
        "'Green Day',\n",
        "'HIM',\n",
        "'Imagine Dragons',\n",
        "'Justin Bieber',\n",
        "'Justin Timberlake',\n",
        "'Katy Perry',\n",
        "'Lady Gaga',\n",
        "'Lana Del Rey',\n",
        "'Linkin Park',\n",
        "'Madonna',\n",
        "'Marilyn Manson',\n",
        "'Maroon 5',\n",
        "'Metallica',\n",
        "'Michael Jackson',\n",
        "'Nickelback',\n",
        "'Oasis',\n",
        "'One Direction',\n",
        "'P!nk',\n",
        "'Queen',\n",
        "'Red Hot Chili Peppers',\n",
        "'Rihanna',\n",
        "'Robbie Williams',\n",
        "'Sting',\n",
        "'The Script',\n",
        "'Weezer',\n",
        "'Yellowcard']\n",
        "\n",
        "trainset = LyricsGenerationDataset(csv_file_path='songdata.csv', artists=artists)\n",
        "valset = LyricsGenerationDataset(csv_file_path='songdata.csv', artists=artists, train=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQQ5wQjasaf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "66e4aed2-6170-4eb3-a333-754dee174b20"
      },
      "source": [
        "# Let us inspect the dataset quickly\n",
        "trainset.lyrics_dataframe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>artist</th>\n",
              "      <th>song</th>\n",
              "      <th>link</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>ABBA</td>\n",
              "      <td>Ahe's My Kind Of Girl</td>\n",
              "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
              "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>ABBA</td>\n",
              "      <td>Andante, Andante</td>\n",
              "      <td>/a/abba/andante+andante_20002708.html</td>\n",
              "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>ABBA</td>\n",
              "      <td>As Good As New</td>\n",
              "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
              "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>ABBA</td>\n",
              "      <td>Bang</td>\n",
              "      <td>/a/abba/bang_20598415.html</td>\n",
              "      <td>Making somebody happy is a question of give an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>ABBA</td>\n",
              "      <td>Bang-A-Boomerang</td>\n",
              "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
              "      <td>Making somebody happy is a question of give an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4793</th>\n",
              "      <td>57175</td>\n",
              "      <td>Yellowcard</td>\n",
              "      <td>The Finish Line</td>\n",
              "      <td>/y/yellowcard/the+finish+line_10195563.html</td>\n",
              "      <td>Hello friend, it's been too long and every tow...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4794</th>\n",
              "      <td>57176</td>\n",
              "      <td>Yellowcard</td>\n",
              "      <td>Twenty Three</td>\n",
              "      <td>/y/yellowcard/twenty+three_10195543.html</td>\n",
              "      <td>I got to tell you that he waited all his life ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4795</th>\n",
              "      <td>57177</td>\n",
              "      <td>Yellowcard</td>\n",
              "      <td>Waiting Game</td>\n",
              "      <td>/y/yellowcard/waiting+game_20423993.html</td>\n",
              "      <td>You and me  \\nA little different  \\nThough we ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4796</th>\n",
              "      <td>57178</td>\n",
              "      <td>Yellowcard</td>\n",
              "      <td>Way Away</td>\n",
              "      <td>/y/yellowcard/way+away_10195536.html</td>\n",
              "      <td>I think I'm breaking out  \\nI'm gonna leave yo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4797</th>\n",
              "      <td>57179</td>\n",
              "      <td>Yellowcard</td>\n",
              "      <td>Words, Hands, Hearts</td>\n",
              "      <td>/y/yellowcard/words+hands+hearts_20424033.html</td>\n",
              "      <td>The whole world was sleeping  \\nAnd I was ther...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4798 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index  ...                                               text\n",
              "0         0  ...  Look at her face, it's a wonderful face  \\nAnd...\n",
              "1         1  ...  Take it easy with me, please  \\nTouch me gentl...\n",
              "2         2  ...  I'll never know why I had to go  \\nWhy I had t...\n",
              "3         3  ...  Making somebody happy is a question of give an...\n",
              "4         4  ...  Making somebody happy is a question of give an...\n",
              "...     ...  ...                                                ...\n",
              "4793  57175  ...  Hello friend, it's been too long and every tow...\n",
              "4794  57176  ...  I got to tell you that he waited all his life ...\n",
              "4795  57177  ...  You and me  \\nA little different  \\nThough we ...\n",
              "4796  57178  ...  I think I'm breaking out  \\nI'm gonna leave yo...\n",
              "4797  57179  ...  The whole world was sleeping  \\nAnd I was ther...\n",
              "\n",
              "[4798 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CVjAz47sqLJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f18fb6c-fabc-4291-cb8f-2fbdd65c2cc9"
      },
      "source": [
        "print (len(trainset))\n",
        "print (len(valset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4298\n",
            "500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4krJ8S-jsub7"
      },
      "source": [
        "#### Implementing an RNN class\n",
        "\n",
        "First, we will implement a __vanilla RNN__ using PyTorch. Fill in the code block below to complete the implementation. _(10 points)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3oH_06ms5sF"
      },
      "source": [
        "class VanillaRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 input_size=101,\n",
        "                 hidden_size=512,\n",
        "                 num_classes=100,\n",
        "                 n_layers=2): #两层rnn网络\n",
        "        \n",
        "        # input_size = 101 -- 100 characters + background character\n",
        "        # num_classes = 100 -- we predict what character goes next\n",
        "        \n",
        "        super(VanillaRNN, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_classes = num_classes\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        # input_size -- size of the dictionary + 1 (accounts for padding constant)\n",
        "        \n",
        "        # Below use nn.Embedding to map from input_size to hidden_size\n",
        "        # nn.Embedding converts labels into one-hot encoding and runs a linear\n",
        "        # layer on each of the converted one-hot encoded elements\n",
        "        # YOUR CODE\n",
        "        \n",
        "        self.embedder = nn.Embedding(self.input_size,self.hidden_size,100)\n",
        "\n",
        "        # Below use nn.RNN that accepts hidden_size as input size,\n",
        "        # and has hidden size equal to hidden_size argument and\n",
        "        # n_layers\n",
        "        # YOUR CODE\n",
        "\n",
        "        self.rnn = nn.RNN(self.hidden_size,self.hidden_size,self.n_layers)\n",
        "\n",
        "        # Below use nn.Linear to make representation of hidden_size\n",
        "        # to the number of classes that will be fed to softmax\n",
        "        # to decide which character goes next\n",
        "        # YOUR CODE\n",
        "\n",
        "        self.logits_fc = nn.Linear(self.hidden_size,self.num_classes)\n",
        "\n",
        "    \n",
        "    def forward(self,\n",
        "                input_sequences,\n",
        "                input_sequences_lengths,\n",
        "                hidden=None):\n",
        "        \n",
        "        batch_size = input_sequences.shape[1]\n",
        "\n",
        "        embedded = self.embedder(input_sequences)\n",
        "        \n",
        "        # This is needed for efficient processing of sequences of\n",
        "        # variable lengths. Feel free to skip\n",
        "        # Here we run rnns only on non-padded regions of the batch\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_sequences_lengths)\n",
        "        \n",
        "        outputs, hidden = self.rnn(packed, hidden)\n",
        "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n",
        "        \n",
        "        \n",
        "        logits = self.logits_fc(outputs)\n",
        "        # This is needed for cross entropy loss\n",
        "        logits = logits.transpose(0, 1).contiguous()\n",
        "        logits_flatten = logits.view(-1, self.num_classes)\n",
        "        \n",
        "        return logits_flatten, hidden\n",
        "\n",
        "\n",
        "# Helper function for efficient processing of sequences of variable lengths\n",
        "# (You may skip this part)\n",
        "def post_process_sequence_batch(batch_tuple):\n",
        "  \n",
        "  input_sequences, output_sequences, lengths = batch_tuple\n",
        "\n",
        "  splitted_input_sequence_batch = input_sequences.split(split_size=1)\n",
        "  splitted_output_sequence_batch = output_sequences.split(split_size=1)\n",
        "  splitted_lengths_batch = lengths.split(split_size=1)\n",
        "\n",
        "  training_data_tuples = zip(splitted_input_sequence_batch,\n",
        "                             splitted_output_sequence_batch,\n",
        "                             splitted_lengths_batch)\n",
        "\n",
        "  training_data_tuples_sorted = sorted(training_data_tuples,\n",
        "                                       key=lambda p: int(p[2]),\n",
        "                                       reverse=True)\n",
        "\n",
        "  splitted_input_sequence_batch, splitted_output_sequence_batch, splitted_lengths_batch = zip(*training_data_tuples_sorted)\n",
        "\n",
        "  input_sequence_batch_sorted = torch.cat(splitted_input_sequence_batch)\n",
        "  output_sequence_batch_sorted = torch.cat(splitted_output_sequence_batch)\n",
        "  lengths_batch_sorted = torch.cat(splitted_lengths_batch)\n",
        "\n",
        "  input_sequence_batch_sorted = input_sequence_batch_sorted[:, :lengths_batch_sorted[0, 0]]\n",
        "  output_sequence_batch_sorted = output_sequence_batch_sorted[:, :lengths_batch_sorted[0, 0]]\n",
        "\n",
        "  input_sequence_batch_transposed = input_sequence_batch_sorted.transpose(0, 1)\n",
        "\n",
        "  # pytorch's api for rnns wants lenghts to be list of ints\n",
        "  lengths_batch_sorted_list = list(lengths_batch_sorted)\n",
        "  lengths_batch_sorted_list = list(map(lambda x: int(x), lengths_batch_sorted_list))\n",
        "\n",
        "\n",
        "  return input_sequence_batch_transposed, output_sequence_batch_sorted, lengths_batch_sorted_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K_LSKqK6Eog"
      },
      "source": [
        "#### Training the vanilla RNN\n",
        "\n",
        "__Note:__ \n",
        "1. You may need to change your Runtime setting to GPU in order to run the following code blocks.\n",
        "2. On changing the Runtime setting, you would be required to run the previous code-blocks again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BSZbQ065lrc"
      },
      "source": [
        "trainset_loader = torch.utils.data.DataLoader(trainset,\n",
        "                                              batch_size=50,\n",
        "                                              shuffle=True,\n",
        "                                              num_workers=4,\n",
        "                                              drop_last=True)\n",
        "\n",
        "\n",
        "valset_loader = torch.utils.data.DataLoader(valset,\n",
        "                                            batch_size=50,\n",
        "                                            shuffle=True,\n",
        "                                            num_workers=1,\n",
        "                                            drop_last=True)\n",
        "\n",
        "\n",
        "rnn = VanillaRNN(input_size=len(all_characters) + 1, hidden_size=512, num_classes=len(all_characters))\n",
        "rnn.cuda()\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
        "criterion = torch.nn.CrossEntropyLoss().cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9Ayee598MLH"
      },
      "source": [
        "from livelossplot import PlotLosses\n",
        "\n",
        "liveloss = PlotLosses()\n",
        "\n",
        "# We will get back to this function later on,\n",
        "# Just ignore it so far -- we will use it to generate\n",
        "# samples during our training\n",
        "def sample_from_rnn(starting_string=\"I\", sample_length=300, temperature=0.5):\n",
        "    assert temperature >= 0.0\n",
        "    sampled_string = starting_string\n",
        "    hidden = None\n",
        "\n",
        "    first_input = torch.LongTensor( string_to_labels(starting_string) ).cuda()\n",
        "    first_input = first_input.unsqueeze(1)\n",
        "    current_input = first_input\n",
        "\n",
        "    output, hidden = rnn(current_input, [len(sampled_string)], hidden=hidden)\n",
        "    output = output[-1, :].unsqueeze(0)\n",
        "\n",
        "    for i in range(sample_length):\n",
        "        output_dist = nn.functional.softmax( output.view(-1).div(temperature + 1e-4) ).data\n",
        "        predicted_label = torch.multinomial(output_dist, 1)\n",
        "        sampled_string += all_characters[int(predicted_label[0])]\n",
        "        current_input = predicted_label.unsqueeze(1)\n",
        "        output, hidden = rnn(current_input, [1], hidden=hidden)\n",
        "    \n",
        "    return sampled_string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDUy9ne58YKL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "outputId": "6ff7a759-325e-479f-e384-ef8cd9dd854a"
      },
      "source": [
        "# We now run the actual training\n",
        "epochs_number = 8\n",
        "\n",
        "\n",
        "for epoch_number in range(epochs_number):\n",
        "\n",
        "    for batch in trainset_loader:\n",
        "        post_processed_batch_tuple = post_process_sequence_batch(batch)\n",
        "        input_sequences_batch, output_sequences_batch, sequences_lengths = post_processed_batch_tuple\n",
        "        output_sequences_batch_var =  output_sequences_batch.contiguous().view(-1).cuda()\n",
        "        input_sequences_batch_var = input_sequences_batch.cuda()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        logits, _ = rnn(input_sequences_batch_var, sequences_lengths)\n",
        "        \n",
        "        train_loss = criterion(logits, output_sequences_batch_var)\n",
        "        train_loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "    \n",
        "    val_loss_list = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      \n",
        "      for batch in valset_loader:\n",
        "\n",
        "        post_processed_batch_tuple = post_process_sequence_batch(batch)\n",
        "        input_sequences_batch, output_sequences_batch, sequences_lengths = post_processed_batch_tuple\n",
        "        output_sequences_batch_var =  output_sequences_batch.contiguous().view(-1).cuda()\n",
        "        input_sequences_batch_var = input_sequences_batch.cuda()\n",
        "        \n",
        "        logits, _ = rnn(input_sequences_batch_var, sequences_lengths)\n",
        "        loss = criterion(logits, output_sequences_batch_var)\n",
        "        \n",
        "        val_loss_list.append(loss.item())\n",
        "      \n",
        "    liveloss.update({'Validation loss': sum(val_loss_list) / len(val_loss_list),\n",
        "                     'Training loss': train_loss.item()})\n",
        "    liveloss.draw()\n",
        "    \n",
        "    print('Example of a text generated by current model:')\n",
        "    print(sample_from_rnn(starting_string='I', temperature=0.5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss:\n",
            "training   (min:    1.246, max:    1.636, cur:    1.246)\n",
            "\n",
            "Training loss:\n",
            "training   (min:    1.235, max:    1.653, cur:    1.256)\n",
            "Example of a text generated by current model:\n",
            "I said you don't know  \n",
            "Baby don't you go  \n",
            "  \n",
            "Oh the coldier  \n",
            "The way you said to be  \n",
            "I won't let it love is to me  \n",
            "  \n",
            "I got me so feeling  \n",
            "  \n",
            "I will be mine  \n",
            "And I would be an end  \n",
            "The rain  \n",
            "I'm so sorry  \n",
            "  \n",
            "I'm looking for you  \n",
            "  \n",
            "Don't wanna live it all to the truth  \n",
            "  \n",
            "So tell me that \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klRB_YxJbtTd"
      },
      "source": [
        "#### Text Generation\n",
        "\n",
        "During the text generation process we can define a starting prefix of characters that our model will condition on and generate the rest of the sequence. \n",
        "\n",
        "The model generates each new token (characters in our case) by sampling from the output probability distribution. When sampling, we can set a `temperature` parameter that controls the randomness of the sampling process. When this parameter approaches zero, the sampling is equivalent to argmax and when it is close to infinity the sampling is equivalent to sampling from a uniform distribution.\n",
        "\n",
        "Complete the following `sample_from_rnn` function, using the hints provided in the comments. _(10 points)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ii0czcn2b71A"
      },
      "source": [
        "def sample_from_rnn(starting_string=\"Why\", sample_length=300, temperature=1):\n",
        "    assert temperature >= 0.0\n",
        "    sampled_string = starting_string\n",
        "    hidden = None\n",
        "\n",
        "    # Prepare the first input to the RNN\n",
        "    first_input = torch.LongTensor( string_to_labels(starting_string) ).cuda()\n",
        "    first_input = first_input.unsqueeze(1)\n",
        "    current_input = first_input\n",
        "\n",
        "    # Now run the RNN model using current_input as first input. \n",
        "    # Note: You may need to look at the Pytorch RNN documentation to\n",
        "    # understand what it outputs. Hint: It outputs a tuple. \n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    output, hidden = rnn(current_input, [len(sampled_string)], hidden=hidden)\n",
        "    output = output[-1, :].unsqueeze(0)\n",
        "\n",
        "    # The output of the RNN for each time-step are actually logits \n",
        "    # (to be fed into a softmax to get a distribution). To further\n",
        "    # add randomness to the logits, we will use the temperature parameter.\n",
        "    # Scale the logits (output of RNN above) by dividing by the temperature\n",
        "    # (+ 1e-4 so that it is not zero), and then obtain a distribution by\n",
        "    # feeding this into nn.functional.softmax. Then use torch.multinomial\n",
        "    # to sample 1 character from this distribution.\n",
        "    \n",
        "    for i in range(sample_length):\n",
        "\n",
        "        output_dist = nn.functional.softmax( output.view(-1).div(temperature + 1e-4) ).data\n",
        "        predicted_label = torch.multinomial(output_dist, 1) #you code here\n",
        "        sampled_string += all_characters[int(predicted_label[0])] #you code here\n",
        "        current_input = predicted_label.unsqueeze(1)\n",
        "        output, hidden = rnn(current_input, [1], hidden=hidden)\n",
        "    \n",
        "    return sampled_string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZFZq9fscOCH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88631f65-16bf-46c1-acb1-ea2a0452439a"
      },
      "source": [
        "print(sample_from_rnn(starting_string='I', temperature=0.5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I, I'm alright  \n",
            "I wanna be will never let me play off the story  \n",
            "And we wanna live to help me  \n",
            "Cause I can't take you the sky  \n",
            "This is the same man  \n",
            "  \n",
            "I'm the song  \n",
            "  \n",
            "I'm so say  \n",
            "It's wearing my love is a sinner  \n",
            "And we touch me and some real to be so good  \n",
            "When he said to be  \n",
            "But sometim\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XKMJfkf5j0R"
      },
      "source": [
        "__Question:__ How do you obtain probability distribution over the possible characters? _(2 points)_\n",
        "\n",
        "__Answer:__ We can obtain the probability distribution by dividing logit by temperature, and then use the softmax function.\n",
        "\n",
        "__Question:__ How does the `temperature` affect the generated text? Describe using some values of temperature. _(3 points)_\n",
        "\n",
        "__Answer:__ Low temperature values(such as 0.2) result in extremely repetitive and predictable text, but with a very realistic local structure. As temperature rises, the generated text becomes richer, and it sometimes invents new words that sound somewhat reasonable. At high temperatures(suach as 2.0), the local structure starts to break down and most words look like semi-random strings.\n",
        "\n",
        "__Question:__ Explain why the sequential nature of recurrent neural networks makes them less efficient compared to methods based on convolutions, for example. _(2 points)_\n",
        "\n",
        "__Answer:__ The sequence-dependent structure of RNN always occurs the problem of vanishing gradient. RNN training applies the same BP algorithm of loss computation backpropagation as CNN training. The difference is that the parameters U, V, and W are shared in RNN, and in the stochastic gradient descent algorithm, the output of each step depends not only on the network of the current step, but also on the network of the time step before the current step. In CNN, the parameters W1 , W2, and W3 of each layer are independent of each other, and W1 , W2W3 perform concatenated multiplication. On the one hand, they are all sparse matrices, and on the other hand, W1 , W2 , and W3 are different from each other, which largely cancel out the effect of gradient explosion. So CNN is more efficient than RNN.\n",
        "\n",
        "\n",
        "__Question:__ In the homework exercise we have loaded whole text sequences into the GPU memory. Imagine that we will work with sequences of much greater length, how that will this affect GPU memory consumption? How can you solve this problem? _(3 points)_\n",
        "\n",
        "__Answer:__ The computational complexity of forward propagation of Recurrent neural network is O(|Sequence Length|). The longer the Sequence is, the more computation is required. And each time step in a long Sequence includes a memory I/O operation. This is very slow and is bounded by the maximum threads and maximum memory bandwidth of the GPU.\n",
        "How to solve: 1.truncated backpropagation through time 2.Use CPU memory to cache activation vectors 3. Parallelize RNN models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTQ8E4Nk0NIs"
      },
      "source": [
        "__Extra credit:__ In the above class, we used a vanilla RNN. Replace the RNN layers with LSTM or GRU and perform training on the same data. Does it converge faster/slower than the RNN? Try sampling from this model with the same starting string and temperature. Do you observe any difference? _(10 points)_\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ7QWmW20WOC"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA4sBKikfbPs"
      },
      "source": [
        "#### Implementing a conditional RNNLM\n",
        "\n",
        "So far our RNN has been trained on lyrics from all the songs without knowledge of artists. Next, we will condition our training by providing the LM with artist information as well.\n",
        "\n",
        "Below we provide updated functions and dataset class that can be used to train language model that is conditioned on the artist. Complete the functions to implement this conditioned LM. _(10 points)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8jFZRMMgZr7"
      },
      "source": [
        "class LyricsGenerationDatasetConditional(data.Dataset):\n",
        "    \n",
        "    def __init__(self, csv_file_path, minimum_song_count=None, artists=None):\n",
        "        \n",
        "        self.lyrics_dataframe = pd.read_csv(csv_file_path)\n",
        "        \n",
        "        if artists:\n",
        "            self.lyrics_dataframe = self.lyrics_dataframe[self.lyrics_dataframe.artist.isin(artists)]\n",
        "            self.lyrics_dataframe = self.lyrics_dataframe.reset_index()\n",
        "        \n",
        "        if minimum_song_count:\n",
        "            # Getting artists that have 70+ songs\n",
        "            self.lyrics_dataframe = self.lyrics_dataframe.groupby('artist').filter(lambda x: len(x) > minimum_song_count)\n",
        "            # Reindex .loc after we fetched random songs\n",
        "            self.lyrics_dataframe = self.lyrics_dataframe.reset_index()\n",
        "        \n",
        "        # Get the length of the biggest lyric text\n",
        "        # We will need that for padding\n",
        "        self.max_text_len = self.lyrics_dataframe.text.str.len().max()\n",
        "        whole_dataset_len = len(self.lyrics_dataframe)\n",
        "        self.indexes = range(whole_dataset_len)\n",
        "        \n",
        "        # Let's get unique artists and form a list\n",
        "        self.artists_list = list(self.lyrics_dataframe.artist.unique())\n",
        "\n",
        "        # We will need the overall number of artists for \n",
        "        self.number_of_artists = len(self.artists_list)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.indexes)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        index = self.indexes[index]\n",
        "        sequence_raw_string = self.lyrics_dataframe.loc[index].text\n",
        "        sequence_string_labels = string_to_labels(sequence_raw_string)\n",
        "        sequence_length = len(sequence_string_labels) - 1\n",
        "        \n",
        "        # Generate input and output sequence (shifted by 1 character)\n",
        "        # from the sequence_string_labels\n",
        "        \n",
        "        # YOUR CODE (SAME AS EARLIER)\n",
        "        input_sequence = sequence_string_labels[:-1]\n",
        "        output_sequence = sequence_string_labels[1:]        \n",
        "\n",
        "        # Pad sequences so that all of them have the same length. For\n",
        "        # the output sequence, use pad_label=-100 so that it is omitted\n",
        "        # in cross-entropy loss. \n",
        "\n",
        "        # YOUR CODE (SAME AS EARLIER)\n",
        "        input_string_labels_padded = pad_sequence(input_sequence,self.max_text_len) \n",
        "        output_string_labels_padded = pad_sequence(output_sequence,self.max_text_len,pad_label=-100)\n",
        "\n",
        "        # For the given index argument, obtain the artist name from self.lyrics_dataframe\n",
        "        # and then get the corresponding index from self.artists_list. Store this\n",
        "        # value as sequence_artist_label\n",
        "        \n",
        "        # YOUR CODE HERE\n",
        "        artist_name = self.lyrics_dataframe.loc[index].artist\n",
        "        sequence_artist_label = self.artists_list.index(artist_name)\n",
        "\n",
        "        return (torch.LongTensor(input_string_labels_padded),\n",
        "                torch.LongTensor(output_string_labels_padded),\n",
        "                torch.LongTensor([sequence_artist_label]),\n",
        "                torch.LongTensor([sequence_length]) )\n",
        "\n",
        "\n",
        "# This function can be skipped\n",
        "def post_process_sequence_batch_conditional(batch_tuple):\n",
        "    \n",
        "    input_sequences, output_sequences, artists, lengths = batch_tuple\n",
        "    \n",
        "    splitted_input_sequence_batch = input_sequences.split(split_size=1)\n",
        "    splitted_output_sequence_batch = output_sequences.split(split_size=1)\n",
        "    splitted_artists_batch = artists.split(split_size=1)\n",
        "    splitted_lengths_batch = lengths.split(split_size=1)\n",
        "\n",
        "    training_data_tuples = zip(splitted_input_sequence_batch,\n",
        "                               splitted_output_sequence_batch,\n",
        "                               splitted_artists_batch,\n",
        "                               splitted_lengths_batch)\n",
        "\n",
        "    training_data_tuples_sorted = sorted(training_data_tuples,\n",
        "                                         key=lambda p: int(p[3]),\n",
        "                                         reverse=True)\n",
        "\n",
        "    splitted_input_sequence_batch, splitted_output_sequence_batch, splitted_artists_batch, splitted_lengths_batch = zip(*training_data_tuples_sorted)\n",
        "\n",
        "    input_sequence_batch_sorted = torch.cat(splitted_input_sequence_batch)\n",
        "    output_sequence_batch_sorted = torch.cat(splitted_output_sequence_batch)\n",
        "    artists_batch_sorted = torch.cat(splitted_artists_batch)\n",
        "    lengths_batch_sorted = torch.cat(splitted_lengths_batch)\n",
        "    \n",
        "    \n",
        "    # Here we trim overall data matrix using the size of the longest sequence\n",
        "    input_sequence_batch_sorted = input_sequence_batch_sorted[:, :lengths_batch_sorted[0, 0]]\n",
        "    output_sequence_batch_sorted = output_sequence_batch_sorted[:, :lengths_batch_sorted[0, 0]]\n",
        "    \n",
        "    # We should probably repeat this over the whole input sequence\n",
        "    artists_batch_sorted = artists_batch_sorted.expand_as(input_sequence_batch_sorted)\n",
        "\n",
        "    input_sequence_batch_transposed = input_sequence_batch_sorted.transpose(0, 1)\n",
        "    artists_batch_sorted_transposed = artists_batch_sorted.transpose(0, 1)\n",
        "    \n",
        "    # pytorch's api for rnns wants lenghts to be list of ints\n",
        "    lengths_batch_sorted_list = list(lengths_batch_sorted)\n",
        "    lengths_batch_sorted_list = list(map(lambda x: int(x), lengths_batch_sorted_list))\n",
        "    \n",
        "    return input_sequence_batch_transposed, output_sequence_batch_sorted, artists_batch_sorted_transposed, lengths_batch_sorted_list\n",
        "\n",
        "\n",
        "class RNN_Conditional(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, num_classes, num_conditions, n_layers=2):\n",
        "        \n",
        "        super(RNN_Conditional, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_classes = num_classes\n",
        "        self.n_layers = n_layers\n",
        "        self.num_conditions = num_conditions\n",
        "        \n",
        "        # Converts labels into one-hot encoding and runs a linear\n",
        "        # layer on each of the converted one-hot encoded elements\n",
        "        \n",
        "        # input_size -- size of the dictionary + 1 (accounts for padding constant)\n",
        "        `\n",
        "        \n",
        "        \n",
        "        `\n",
        "        \n",
        "        self.characters_embedder = nn.Embedding(self.input_size,self.hidden_size,100) # YOUR CODE HERE\n",
        "        self.artist_embedder = nn.Embedding(self.num_conditions,self.hidden_size) # YOUR CODE HERE\n",
        "        self.rnn = nn.RNN(self.hidden_size*2,self.hidden_size,self.n_layers) # YOUR CODE HERE\n",
        "        self.logits_fc = nn.Linear(self.hidden_size,self.num_classes) # YOUR CODE HERE\n",
        "    \n",
        "    \n",
        "    def forward(self, input_sequences, input_sequences_conditions, input_sequences_lengths, hidden=None):\n",
        "        \n",
        "        batch_size = input_sequences.shape[1]\n",
        "\n",
        "        characters_embedded = self.characters_embedder(input_sequences)\n",
        "        conditions_embedded = self.artist_embedder(input_sequences_conditions)\n",
        "        \n",
        "        embedded_combined = torch.cat((characters_embedded, conditions_embedded), dim=2)\n",
        "\n",
        "        # Here we run rnns only on non-padded regions of the batch\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded_combined, input_sequences_lengths)\n",
        "        outputs, hidden = self.rnn(packed, hidden)\n",
        "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n",
        "        \n",
        "        logits = self.logits_fc(outputs)\n",
        "        logits = logits.transpose(0, 1).contiguous()\n",
        "        logits_flatten = logits.view(-1, self.num_classes)\n",
        "        \n",
        "        return logits_flatten, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyaSbqCPiM7R"
      },
      "source": [
        "#### Training the conditional LM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5DxocaNiOst"
      },
      "source": [
        "trainset = LyricsGenerationDatasetConditional(csv_file_path='songdata.csv', artists=artists)\n",
        "\n",
        "trainset_loader = torch.utils.data.DataLoader(trainset,\n",
        "                                              batch_size=50,\n",
        "                                              shuffle=True, num_workers=4, drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1m6YjkJGiQHD"
      },
      "source": [
        "rnn = RNN_Conditional(input_size=len(all_characters) + 1,\n",
        "          hidden_size=512,\n",
        "          num_classes=len(all_characters),\n",
        "          num_conditions=trainset.number_of_artists)\n",
        "\n",
        "rnn.cuda()\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
        "criterion = torch.nn.CrossEntropyLoss().cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXnz-XC1iSRl"
      },
      "source": [
        "from livelossplot import PlotLosses\n",
        "\n",
        "liveloss_train = PlotLosses()\n",
        "liveloss_val = PlotLosses()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWJQZ14hihNa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "acc18340-be63-4554-8083-d8f60ba2ed87"
      },
      "source": [
        "# Conditional model takes more to converge,\n",
        "# Try training for more epochs for better results\n",
        "epochs_number = 500\n",
        "\n",
        "for epoch_number in range(epochs_number):\n",
        "\n",
        "    for batch in trainset_loader:\n",
        "\n",
        "        post_processed_batch_tuple = post_process_sequence_batch_conditional(batch)\n",
        "        input_sequences_batch, output_sequences_batch, artists_batch, sequences_lengths = post_processed_batch_tuple\n",
        "        output_sequences_batch_var = output_sequences_batch.contiguous().view(-1).cuda()\n",
        "        \n",
        "        input_sequences_batch_var = input_sequences_batch.cuda()\n",
        "        artists_batch_var = artists_batch.cuda()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits, _ = rnn(input_sequences_batch_var,  `` artists_batch_var, sequences_lengths)\n",
        "        \n",
        "        loss = criterion(logits, output_sequences_batch_var)\n",
        "        loss.backward()\n",
        "        \n",
        "    liveloss_train.update({'Training loss': loss.item()})\n",
        "    liveloss_train.draw()\n",
        "\n",
        "    #torch.nn.utils.clip_grad_norm(rnn.parameters(), clip)\n",
        "\n",
        "    optimizer.step()\n",
        "        \n",
        "    torch.save(rnn.state_dict(), 'conditional_rnn.pth')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss:\n",
            "training   (min:    1.254, max:    4.635, cur:    1.312)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-184967d341d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_sequences_batch_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mliveloss_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Training loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0uSmNpLdBSI"
      },
      "source": [
        "__Note: After 5 hours' exceuting, the colab said \"Busy\" and cannot proceed to more epoches, so I interrupted at this time.__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFr9jXf7iqcY"
      },
      "source": [
        "#### Generating from the conditional LM\n",
        "\n",
        "__Question:__ Implement a function `sample_from_rnn_conditionally(artist_label, starting_string, temperature)` similar to the `sample_from_rnn()` function shown earlier. Use this function to generate an Eminem song starting with \"Why\". _(10 points)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwyGLqBziuop"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "def sample_from_rnn_conditionally(artist_label,starting_string=\"Why\", temperature=1):\n",
        "    assert temperature >= 0.0\n",
        "    sampled_string = starting_string\n",
        "    hidden = None\n",
        "\n",
        "    # Prepare the first input to the RNN\n",
        "    first_input = torch.LongTensor(string_to_labels(starting_string)).cuda()\n",
        "    first_input = first_input.unsqueeze(1)\n",
        "\n",
        "    # Prepare the second input to the RNN\n",
        "    second_input = torch.LongTensor(string_to_labels(artist_label)).cuda()\n",
        "    second_input = first_input.unsqueeze(1)\n",
        "\n",
        "    output, hidden = rnn(first_input, second_input, [len(sampled_string)],hidden = hidden)\n",
        "    output = output[-1, :].unsqueeze(0)\n",
        "    \n",
        "    for i in range(sample_length):\n",
        "\n",
        "        output_dist = nn.functional.softmax( output.view(-1).div(temperature + 1e-4) ).data\n",
        "        predicted_label = torch.multinomial(output_dist, 1) #you code here\n",
        "        sampled_string += all_characters[int(predicted_label[0])] #you code here\n",
        "        current_input = predicted_label.unsqueeze(1)\n",
        "        output, hidden = rnn(current_input, [1], hidden=hidden)\n",
        "    \n",
        "    return sampled_string\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGF4idtvsp2F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f18737a6-43e4-4aeb-fa50-44530ffe4749"
      },
      "source": [
        "sample_from_rnn_conditionally('Eminem','Why',temperature = 0.5 )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why do you say I have something but you can see you  \n",
            "I won't let you all we all go  \n",
            "  \n",
            "Cause you're our life with me  \n",
            "  \n",
            "I wanna stand what you know you're gonna rain to be  \n",
            "  \n",
            "I'm a little more  \n",
            "I want to call me to spend it and me  \n",
            "  \n",
            "I'm so slipped to be with you  \n",
            "And say what you can't be th\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nnPlCIpjAZf"
      },
      "source": [
        "__Answer:__ lyrics as above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZw9_-l8jLCU"
      },
      "source": [
        "### Part 2: Distributional Semantics: The Word2Vec model\n",
        "\n",
        "In Part 2 of this assignment, you will use Word2Vec to analyze some of interesting phenomena that arises from distributional semantics. For this, we will use a pretrained model from the [Gensim](https://radimrehurek.com/gensim/) package.\n",
        "\n",
        "\n",
        "Before we dive into this analysis, first read a little bit of theory about the Word2Vec model and answer a few questions below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDaseIPkANWm"
      },
      "source": [
        "#### Understanding word2vec\n",
        "\n",
        "The basic architecture of the skip-gram model is shown below (taken from [this post](https://israelg99.github.io/2017-03-23-Word2Vec-Explained/)):\n",
        "\n",
        "![](https://israelg99.github.io/images/2017-03-23-Word2Vec-Explained/skip_gram_net_arch.png)\n",
        "\n",
        "In this model, word embeddings are trained using a classification task, where the task is that given a word, you have to predict its context words (i.e. words which occur before and after it in a context of size, say, 2). The objective is to learn the probability of any word $O$ given a center word $C$, i.e. $P(O=o \\mid C=c)$.\n",
        "\n",
        "In this model, this probability is computed by taking the vector dot product of the embeddings for $o$ and $c$, and then applying a softmax (to convert it into a probability distribution):\n",
        "\n",
        "$$\n",
        "P(O=o \\mid C=c)=\\frac{\\exp \\left(\\boldsymbol{v}_{o}^{\\top} \\boldsymbol{v}_{c}\\right)}{\\sum_{w \\in \\operatorname{Vocab}} \\exp \\left(\\boldsymbol{v}_{w}^{\\top} \\boldsymbol{v}_{c}\\right)}\n",
        "$$\n",
        "\n",
        "The vectors $\\boldsymbol{v}_{w}$ are initialized randomly for all the words in the vocabulary and then updated during training. The objective, in fact, tries to maximize the above probability of words in context occuring together. Stating it different, it miniimized the negative log-probability, i.e.,\n",
        "\n",
        "$$ J = -\\log P(O=o \\mid C=c). $$\n",
        "\n",
        "Here is another interpretation of this objective function. Suppose ${y}$ and $\\hat{y}$ are vectors of length $|V|$ (size of vocabulary), and the index $k$ in each of these denotes the conditional probability of word $k$ being in the context of given word $c$. $y$ is the empirical distribution (ground truth), i.e., it is a one-hot vector, whereas $\\hat{y}$ is the predicted distribution. In our case, we would have $P(O=o \\mid C=c) = \\hat{y}_o$. Then the loss function $J$ above is equivalent to the binary cross-entropy loss between $y$ and $\\hat{y}$.\n",
        "\n",
        "__Question:__ The cross-entropy loss between 2 distributions is given as $-\\sum_i p_i \\log q_i$. With this information, show that the cross-entropy between $y$ and $\\hat{y}$ is equal to $-\\log \\hat{y}_o$. _(5 points)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n2TDbWcJkTP"
      },
      "source": [
        "__Answer:__ Because the true distribution $y$ is a one-hot vector, i.e., the value is 1 only at the current position of the true contextual word $O$. Therefore:\n",
        "\n",
        "$$-\\sum_{w\\in Vocab} y_w \\log \\hat y_w = - 1 \\times \\log \\hat y_o = - \\log P(O = o | C = c)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh6D5LUmJnyk"
      },
      "source": [
        "Note that in the softmax computation, the denominator computes a sum over the entire vocabulary. This is very computationally expensive. In the actual word2vec implementation, we use something called Negative Sampling instead of computing the softmax this way:\n",
        "\n",
        "$$J^{\\prime} = -\\log \\sigma(v_o v_c) - \\sum_{k=1}^K \\log \\sigma(-v_k v_c)$$\n",
        "\n",
        "Here, $v_k$ is the embedding for a word which is not present in the context. Intuitively, this loss function pushes $v_c$ closer to its context word $v_o$ while pulling it away from randomly sampled $K$ non-context (or negative) words, hence the name negative sampling.\n",
        "\n",
        "An important component of the above objective is the sigmoid function $\\sigma(x) = \\frac{e^x}{e^x + 1}$.\n",
        "\n",
        "__Question:__ Compute the derivative of $\\sigma(x)$ w.r.t $x$. Write the derivative in terms of $\\sigma(x)$. (Show the derivation steps.) _(5 points)_ \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBCyjU7LCGZ5"
      },
      "source": [
        "__Answer:__  First, we get the the derivative of sigmoid function $\\sigma(x)$,  which is : $$\\frac{\\partial \\sigma(x)}{\\partial x} = \\frac{e^{-x}}{(1+e^{-x})} = \\sigma (x) (1-\\sigma (x))$$.\n",
        "\n",
        "Then, we compute the derivative of $v_c$ in terms of $\\sigma(x)$:\n",
        "\n",
        "$$\\frac {\\partial J'}{\\partial v_c} = -(1-\\sigma(u_{o}^Tv_c))u_o + \\sum_{k=1}^K(1-\\sigma(-u_{k}^Tv_c))u_k$$\n",
        "\n",
        "the derivative of $u_k$ in terms of $\\sigma(x)$:\n",
        "\n",
        "$$\\frac {\\partial J'}{\\partial u_k} = -(1-\\sigma(u_{o}^Tv_c))v_c$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXAvtTl5COQC"
      },
      "source": [
        "$\\sigma(x)$ has the nice property that its derivative can be represented using it directly. This makes it useful for gradient-based optimization. In the following, we will use the `gensim` implementation of Word2Vec, which means you don't need to implement any of the training yourself, but it is important to understand what goes on under the hood."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GRWFeCRkcFP"
      },
      "source": [
        "#### Using word2vec\n",
        "\n",
        "In this section, we will use the GenSim implementation of Word2Vec for some exploration. First, let us train a Word2Vec model on our corpus.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsEmWXPAhBDL"
      },
      "source": [
        "def load_embedding_model():\n",
        "  \"\"\" Load GloVe Vectors\n",
        "      Return:\n",
        "          wv_from_bin: All 400000 embeddings, each lengh 200\n",
        "  \"\"\"\n",
        "  import gensim.downloader as api\n",
        "  wv_from_bin = api.load(\"glove-wiki-gigaword-200\")\n",
        "  print(\"Loaded vocab size %i\" % len(wv_from_bin.vocab.keys()))\n",
        "  return wv_from_bin"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eInUSXOOhIcP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "bf32e3cf-2ad6-4de9-aa0f-63c1cf0b6ec9"
      },
      "source": [
        "# -----------------------------------\n",
        "# Run Cell to Load Word Vectors\n",
        "# Note: This will take several minutes\n",
        "# -----------------------------------\n",
        "wv_from_bin = load_embedding_model()\n",
        "print(wv_from_bin)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 252.1/252.1MB downloaded\n",
            "Loaded vocab size 400000\n",
            "<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f87f9044190>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVFXG06UbN5P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6261525e-5cad-4da4-caa6-19ec81fdf66d"
      },
      "source": [
        "# access vector for one word\n",
        "print(wv_from_bin['man'])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.0627e-01 -5.8248e-01 -2.7217e-01 -2.6772e-01 -1.1844e-01 -1.3373e-03\n",
            " -1.8707e-01  1.5110e-01  2.0336e-01 -5.0561e-01 -5.6029e-02 -1.4278e-01\n",
            "  2.7876e-01  5.6570e-01  1.0539e-01  2.0012e-01 -1.0245e-01  3.8694e-01\n",
            "  4.9881e-03 -2.0792e-01  4.3314e-01  2.7799e+00 -3.7724e-03 -1.5376e-01\n",
            "  3.5440e-03 -3.2788e-01 -2.2937e-01 -3.2938e-01  1.0435e-01  1.8370e-01\n",
            " -2.0869e-01 -1.2898e-01  1.9762e-01  5.7719e-01  4.8807e-01 -2.2012e-01\n",
            " -7.8150e-01 -2.6464e-01 -4.6928e-01  4.8230e-01 -7.2708e-01  1.7301e-02\n",
            " -3.5347e-01  1.6986e-01 -4.0509e-01  3.1622e-02 -8.5332e-02  1.1077e-01\n",
            "  2.3554e-01 -1.8391e-01  2.3047e-01  5.0146e-01  1.8733e-01  3.3038e-01\n",
            "  3.8698e-01 -1.6126e-01 -1.2255e-01 -6.4396e-02  5.2114e-01 -1.9056e-01\n",
            " -2.9464e-01 -2.8260e-01 -8.4987e-01  2.0800e-01 -4.2650e-02 -3.0999e-01\n",
            "  4.5363e-01  3.4151e-01 -6.6935e-03  6.2788e-02  4.3688e-02 -3.1161e-01\n",
            " -1.7607e-01  6.5094e-01 -5.8473e-01 -3.6569e-01 -3.4423e-01 -4.3389e-01\n",
            "  6.5343e-02 -1.2442e-01  1.7572e-01 -2.8400e-01  1.9135e-02 -5.3385e-01\n",
            " -4.0021e-01  2.1464e-01  1.4933e-02  1.6345e-02 -3.1991e-01 -6.8469e-01\n",
            " -5.7154e-02  3.1403e-01 -2.8191e-01  6.2003e-01 -5.9969e-01  7.5217e-02\n",
            " -1.7093e-01 -2.1516e-01  3.1219e-01  6.4726e-01  5.7606e-02 -2.3318e-01\n",
            "  1.7409e-01  1.8636e-01  1.2784e-01 -8.5055e-01 -1.2307e-01  1.1455e+00\n",
            " -3.7782e-02  3.2608e-01  3.2714e-01 -4.1493e-01 -3.3780e-02 -2.0761e-02\n",
            " -7.4910e-01  2.6303e-01 -3.4439e-01  3.2692e-01 -7.0086e-01 -4.6634e-02\n",
            " -5.0236e-02  2.5610e-01  4.6046e-02  2.1670e-01  2.7547e-02 -1.9104e-01\n",
            "  2.9514e-01 -2.6253e-01  5.1427e-01 -4.7265e-01 -3.9650e-01 -1.4644e-02\n",
            "  1.1092e-01 -4.2111e-01 -1.7097e-01 -2.7003e-01  2.3945e-01  2.7820e-01\n",
            " -7.5243e-01 -1.2806e-01  4.9752e-01  2.5289e-01  7.3583e-01 -2.5678e-01\n",
            "  1.8087e+00  2.6838e-01  5.8882e-01  4.4866e-02  4.3145e-01  2.7810e-01\n",
            "  3.4408e-01  1.7439e-01 -1.2762e-01  1.1552e-01 -3.9833e-01  1.2771e-01\n",
            " -2.2887e-01 -1.4351e-01 -5.2339e-01 -2.9325e-01  8.8909e-02  8.4056e-01\n",
            "  1.8710e-01 -1.3646e-01  1.9423e-01 -6.6192e-01  8.5687e-05  2.4278e-01\n",
            " -6.8090e-01 -6.2710e-02  2.5439e-01 -3.0117e-01  3.8821e-01  1.3239e-01\n",
            " -3.1887e-01 -2.7511e-01 -4.7149e-01 -3.0114e-01  3.2686e-01 -3.8738e-03\n",
            "  1.0342e+00  1.7300e-01 -7.7236e-01  5.9479e-02 -3.8417e-01 -2.6487e-01\n",
            "  8.1626e-02  1.1445e-01  1.5213e-01  1.2093e-01 -3.4896e-01 -1.3335e-01\n",
            "  2.3203e-01 -2.1832e-02  6.5154e-01  1.1376e-01  1.3703e-01  4.5007e-01\n",
            "  5.1510e-02  7.0033e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxBXR4oOcKDk"
      },
      "source": [
        "__Embedding analysis:__ Choose 20 words at random from the lexicon and obtain their corresponding embeddings. Reduce the embedding dimensionality to 2 (using PCA or T-SNE) and plot them. Do you observe any clusters forming? _(10 points)_\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWtJJKhAc9ua",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "b1f804e4-1137-4714-b995-afebf0c90c95"
      },
      "source": [
        "# Your code here\n",
        "import random\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot as plt\n",
        "random_words = random.sample(list(wv_from_bin.vocab.keys()),20)\n",
        "embedding_list = []\n",
        "for word in random_words:\n",
        "  embedding_list.append(wv_from_bin[word])\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "result = pca.fit_transform(embedding_list)\n",
        "# 可视化展示\n",
        "plt.figure(figsize=(7,6)) \n",
        "plt.scatter(result[:, 0], result[:, 1])\n",
        "words = list(random_words)\n",
        "for i, word in enumerate(words):\n",
        "\tplt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f383hVjoc_aw"
      },
      "source": [
        "__Answer:__ Yes, obvious clusters on the figure can easily be observed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1mn9cK7ef3H"
      },
      "source": [
        "__Synonyms and antonyms:__ Find three words (w1,w2,w3) where w1 and w2 are synonyms and w1 and w3 are antonyms, but Cosine Distance(w1,w3) < Cosine Distance(w1,w2). For example, w1=\"happy\" is closer to w3=\"sad\" than to w2=\"cheerful\".\n",
        "\n",
        "Once you have found your example, please give a possible explanation for why this counter-intuitive result may have happened.\n",
        "\n",
        "You should use the the `wv_from_bin.distance(w1, w2)` function here in order to compute the cosine distance between two words. _(5 points)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSEd4KGPe5mB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f170d272-e02f-4334-c776-39d8001be648"
      },
      "source": [
        "# Your code here\n",
        "import math\n",
        "def find_pairs(compare_word,syn,ant):\n",
        "  for word1,prob1 in syn:\n",
        "    dist1 = math.cos(wv_from_bin.distance(compare_word,word1))\n",
        "    for word2,prob2 in ant:\n",
        "      dist2 =  math.cos(wv_from_bin.distance(compare_word,word2))\n",
        "      if dist1 > dist2:\n",
        "        return word1,word2\n",
        "'''vocab = list(wv_from_bin.vocab)\n",
        "for compare_word in vocab[:100]:\n",
        "  syn = wv_from_bin.most_similar(positive=compare_word,topn=200)\n",
        "  ant = wv_from_bin.most_similar(negative=compare_word,topn=200)\n",
        "  w= find_pairs(compare_word,syn,ant)\n",
        "  if w:\n",
        "    print(compare_word,w[0],w[1])'''\n",
        "compare_word = 'nice'\n",
        "syn = wv_from_bin.most_similar(positive=compare_word,topn=100)\n",
        "ant = wv_from_bin.most_similar(negative=compare_word,topn=100)\n",
        "w= find_pairs(compare_word,syn,ant)\n",
        "print(compare_word,w[0],w[1])\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nice good m.sambucetti\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8MVeoWJdlfi"
      },
      "source": [
        "__Finding analogies:__ Implement a function `analogy()` which solves the analogy problem, e.g., `man:king::woman:x`. Use the `most_similar()` function provided in Gensim for your implementation. _(5 points)_ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkUrl7oFx5Fr"
      },
      "source": [
        "def analogy(x1, x2, y1):\n",
        "  \"\"\"Solves for x1:x2::y1:?\"\"\"\n",
        "  # YOUR CODE HERE\n",
        "  return wv_from_bin.most_similar(positive=[y1, x2], negative=[x1],topn = 1)\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwvZAOEnyclt"
      },
      "source": [
        "Use your `analogy()` function to check if the analogy `man:king::woman:queen` holds for the trained model. Find another such analogy which makes semantic sense. _(5 points)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN9nvq-seHgF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "119a8a5a-876a-4503-fbc8-a5573238b5fc"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "a1 = analogy('man','king','woman')\n",
        "print('man',':','king','::','woman',':',a1[0][0])\n",
        "\n",
        "a2 = analogy('man', 'woman','father')\n",
        "print('man',':','woman','::','father',':',a2[0][0])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "man : king :: woman : queen\n",
            "man : woman :: father : mother\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsdSyUpue6yX"
      },
      "source": [
        "__Answer:__ This model can esaily find the answer of the analogy. Another analogy: man:woman::father:mother"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKkAv9Rze8TQ"
      },
      "source": [
        "__Analyzing bias:__ Complete the following analogies: `man:worker::woman:x` and `woman:worker::man:x` and list the most likely outputs in both cases. Point out the difference between the list of female-associated words and the list of male-associated words, and explain how it is reflecting gender bias. _(5 points)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9ofG0SHfPxW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "526108b3-a10f-41a4-8051-c4c45d26c0eb"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "# YOUR CODE HERE\n",
        "a3 = analogy('man', 'worker','woman')\n",
        "print('man',':','worker','::','woman',':',a3[0][0])\n",
        "\n",
        "a4 = analogy('woman', 'worker','man')\n",
        "print('woman',':','worker','::','man',':',a4[0][0])\n",
        "\n",
        "a5 = analogy('father', 'doctor','mother')\n",
        "print('father',':','doctor','::','monther',':',a5[0][0])\n",
        "\n",
        "a6 = analogy('he', 'guitarist','she')\n",
        "print('he',':','guitarist','::','she',':',a6[0][0])\n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "man : worker :: woman : employee\n",
            "woman : worker :: man : workers\n",
            "father : doctor :: monther : nurse\n",
            "he : guitarist :: she : vocalist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lffjBbCP8XTf"
      },
      "source": [
        "__Answer:__ Due to these two answers for analogies: man:worker::woman:x and woman:worker::man:x, they are obviously different. When it comes to male:female pairs, the answer is like “man is to worker as woman is to x” with x=employee. There are also many different cases such as doctor - nurse and guitarist - vocalist. In additon, words such as \"beautiful\" and \"dance\" are also associated with women, while words such as \"outstanding\" and \"genius\" are more associated with men. \" However, the attributes of these words should have been gender neutral, which reflects the gender bias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ0oXYwSw5af"
      },
      "source": [
        "__Question:__ Suggest some ways to deal with the gender (or racial) bias when training word embedding models, or when using pre-trained models. _(5 points)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gMQZq2zzN8_"
      },
      "source": [
        "__Answer:__ (1) Ensure that gender neutral words such as nurse are equidistant between gender pairs such as he and she.\n",
        "\n",
        "\n",
        "(2) Reduce gender associations that pervade the embedding even among gender neutral words."
      ]
    }
  ]
}